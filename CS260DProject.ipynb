{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b459e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'S2L_Cirriculum' already exists and is not an empty directory.\n",
      "S2L_Cirriculum\tsample_data\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/LiuHenry9800/S2L_Cirriculum.git\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11e51ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'S2L_Cirriculum'\n",
      "/content/S2L_Cirriculum\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.46.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "%cd S2L_Cirriculum\n",
    "!pip install accelerate wandb faiss-cpu transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9698e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"S2L_Cirriculum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a30a7425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-29 01:48:29.688203: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 01:48:29.705163: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764380909.725862    8009 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764380909.732151    8009 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764380909.748113    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764380909.748139    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764380909.748142    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764380909.748146    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 01:48:29.752759: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhenryliu999\u001b[0m (\u001b[33mhenryliu999-other\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-70m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-70m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "ref_model_path: null\n",
      "n_components: -1\n",
      "num_loss_ckpts: -1\n",
      "distance: euclidean\n",
      "seed: 42\n",
      "\n",
      "config.json: 100% 567/567 [00:00<00:00, 4.46MB/s]\n",
      "model.safetensors: 100% 166M/166M [00:02<00:00, 80.4MB/s]\n",
      "*** Model initialized!\n",
      "tokenizer_config.json: 100% 396/396 [00:00<00:00, 2.82MB/s]\n",
      "tokenizer.json: 2.11MB [00:00, 149MB/s]\n",
      "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 950kB/s]\n",
      "*** Tokenizer initialized!\n",
      "*** Smart tokenizer and embedding resize done!\n",
      "README.md: 2.72kB [00:00, 17.0MB/s]\n",
      "MathInstruct.json: 100% 212M/212M [00:04<00:00, 42.8MB/s] \n",
      "Generating train split: 100% 262039/262039 [00:02<00:00, 127352.55 examples/s]\n",
      "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n",
      "*** Schedule built!\n",
      "*** labeled_idx: tensor([   0,    1,    2,  ..., 9997, 9998, 9999])\n",
      "*** jdump(labeled_data_json_format, labeled_data_path) SUCESSFUL to --> res/pythia-70m_cirriculum_full/data/labeled.json\n",
      "*** jdump(unlabeled_data_json_format, unlabeled_data_path) SUCESSFUL to --> res/pythia-70m_cirriculum_full/data/unlabeled.json\n",
      "*** Training-Data-Size = 10000\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "*** SANITY-CHECK: Training-Sample#1. - TEXT.:\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "The distance between two stars is 6.52 √ó 10^5 light years. What is the distance between the two stars in parsecs? (1 parsec = 3.26 light years)\n",
      "Answer Choices: (A) 2 √ó 10^5 (B) 4 √ó 10^6 (C) 5 √ó 10^7 (D) 7 √ó 10^7 (E) 9 √ó 10^8\n",
      "\n",
      "### Response:Let's think about the multi-choice question.\n",
      "6.52 √ó 10^5 ly / (3.26 ly/parsec) = 2 x 10^5 persec\n",
      "The answer is A.</s>\n",
      "\n",
      "\n",
      "/content/S2L_Cirriculum/schedule_base.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=self.model,\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 50278, 'bos_token_id': 50279, 'pad_token_id': 50277}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m setting up run 1gbsfdew (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run 1gbsfdew (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m setting up run 1gbsfdew (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/S2L_Cirriculum/wandb/run-20251129_014918-1gbsfdew\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcelestial-sunset-10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/henryliu999-other/S2L_Cirriculum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/henryliu999-other/S2L_Cirriculum/runs/1gbsfdew\u001b[0m\n",
      "{'loss': 3.7658, 'grad_norm': 422.35272216796875, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'loss': 3.7338, 'grad_norm': 451.20465087890625, 'learning_rate': 6.896551724137931e-07, 'epoch': 0.01}\n",
      "{'loss': 3.5719, 'grad_norm': 419.6539001464844, 'learning_rate': 1.3793103448275862e-06, 'epoch': 0.01}\n",
      "{'loss': 3.4757, 'grad_norm': 413.008544921875, 'learning_rate': 2.0689655172413796e-06, 'epoch': 0.01}\n",
      "{'loss': 3.8353, 'grad_norm': 507.3951721191406, 'learning_rate': 2.7586206896551725e-06, 'epoch': 0.02}\n",
      "{'loss': 4.0288, 'grad_norm': 641.5845336914062, 'learning_rate': 3.448275862068966e-06, 'epoch': 0.02}\n",
      "{'loss': 4.0561, 'grad_norm': 589.9639282226562, 'learning_rate': 4.137931034482759e-06, 'epoch': 0.02}\n",
      "{'loss': 4.1921, 'grad_norm': 689.5428466796875, 'learning_rate': 4.8275862068965525e-06, 'epoch': 0.03}\n",
      "{'loss': 4.1345, 'grad_norm': 783.1348266601562, 'learning_rate': 5.517241379310345e-06, 'epoch': 0.03}\n",
      "{'loss': 4.2642, 'grad_norm': 746.8333740234375, 'learning_rate': 6.206896551724138e-06, 'epoch': 0.03}\n",
      "{'loss': 4.5432, 'grad_norm': 724.808837890625, 'learning_rate': 6.896551724137932e-06, 'epoch': 0.04}\n",
      "{'loss': 4.4541, 'grad_norm': 776.2188110351562, 'learning_rate': 7.586206896551724e-06, 'epoch': 0.04}\n",
      "{'loss': 4.5921, 'grad_norm': 839.0402221679688, 'learning_rate': 8.275862068965518e-06, 'epoch': 0.04}\n",
      "{'loss': 4.4551, 'grad_norm': 876.8865966796875, 'learning_rate': 8.965517241379312e-06, 'epoch': 0.04}\n",
      "{'loss': 4.7138, 'grad_norm': 915.0836181640625, 'learning_rate': 9.655172413793105e-06, 'epoch': 0.05}\n",
      "{'loss': 4.3166, 'grad_norm': 867.3264770507812, 'learning_rate': 1.0344827586206898e-05, 'epoch': 0.05}\n",
      "{'loss': 4.3668, 'grad_norm': 994.2124633789062, 'learning_rate': 1.103448275862069e-05, 'epoch': 0.05}\n",
      "{'loss': 4.3416, 'grad_norm': 1057.8489990234375, 'learning_rate': 1.1724137931034483e-05, 'epoch': 0.06}\n",
      "{'loss': 4.8721, 'grad_norm': 967.5595703125, 'learning_rate': 1.2413793103448277e-05, 'epoch': 0.06}\n",
      "{'loss': 4.4301, 'grad_norm': 1057.0362548828125, 'learning_rate': 1.310344827586207e-05, 'epoch': 0.06}\n",
      "{'loss': 4.735, 'grad_norm': 998.0591430664062, 'learning_rate': 1.3793103448275863e-05, 'epoch': 0.07}\n",
      "{'loss': 4.35, 'grad_norm': 957.4970092773438, 'learning_rate': 1.4482758620689657e-05, 'epoch': 0.07}\n",
      "{'loss': 4.275, 'grad_norm': 1146.5919189453125, 'learning_rate': 1.5172413793103448e-05, 'epoch': 0.07}\n",
      "{'loss': 4.7599, 'grad_norm': 1015.6348266601562, 'learning_rate': 1.586206896551724e-05, 'epoch': 0.08}\n",
      "{'loss': 4.4953, 'grad_norm': 1293.0433349609375, 'learning_rate': 1.6551724137931037e-05, 'epoch': 0.08}\n",
      "{'loss': 4.4784, 'grad_norm': 959.790283203125, 'learning_rate': 1.7241379310344828e-05, 'epoch': 0.08}\n",
      "{'loss': 4.4796, 'grad_norm': 1364.8568115234375, 'learning_rate': 1.7931034482758623e-05, 'epoch': 0.09}\n",
      "{'loss': 4.4238, 'grad_norm': 1302.243408203125, 'learning_rate': 1.8620689655172415e-05, 'epoch': 0.09}\n",
      "{'loss': 4.539, 'grad_norm': 1117.89306640625, 'learning_rate': 1.931034482758621e-05, 'epoch': 0.09}\n",
      "{'loss': 4.4704, 'grad_norm': 1154.024169921875, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 4.5203, 'grad_norm': 1331.9298095703125, 'learning_rate': 1.9999940408195878e-05, 'epoch': 0.1}\n",
      "{'loss': 4.5085, 'grad_norm': 1112.92919921875, 'learning_rate': 1.9999761633493754e-05, 'epoch': 0.1}\n",
      "{'loss': 4.436, 'grad_norm': 1098.4649658203125, 'learning_rate': 1.9999463678024317e-05, 'epoch': 0.11}\n",
      "{'loss': 4.4976, 'grad_norm': 1290.3721923828125, 'learning_rate': 1.999904654533872e-05, 'epoch': 0.11}\n",
      "{'loss': 4.5959, 'grad_norm': 1061.7178955078125, 'learning_rate': 1.9998510240408495e-05, 'epoch': 0.11}\n",
      "{'loss': 4.3478, 'grad_norm': 1028.8590087890625, 'learning_rate': 1.999785476962552e-05, 'epoch': 0.12}\n",
      "{'loss': 4.3721, 'grad_norm': 1032.20947265625, 'learning_rate': 1.9997080140801932e-05, 'epoch': 0.12}\n",
      "{'loss': 4.3482, 'grad_norm': 1106.6773681640625, 'learning_rate': 1.9996186363170037e-05, 'epoch': 0.12}\n",
      "{'loss': 4.1389, 'grad_norm': 898.416015625, 'learning_rate': 1.9995173447382193e-05, 'epoch': 0.12}\n",
      "{'loss': 4.4642, 'grad_norm': 1161.6463623046875, 'learning_rate': 1.9994041405510705e-05, 'epoch': 0.13}\n",
      "{'loss': 4.3778, 'grad_norm': 968.447021484375, 'learning_rate': 1.9992790251047655e-05, 'epoch': 0.13}\n",
      "{'loss': 4.3078, 'grad_norm': 949.67919921875, 'learning_rate': 1.999141999890475e-05, 'epoch': 0.13}\n",
      "{'loss': 4.6665, 'grad_norm': 867.0065307617188, 'learning_rate': 1.9989930665413148e-05, 'epoch': 0.14}\n",
      "{'loss': 4.245, 'grad_norm': 995.5661010742188, 'learning_rate': 1.998832226832327e-05, 'epoch': 0.14}\n",
      "{'loss': 3.825, 'grad_norm': 863.858154296875, 'learning_rate': 1.9986594826804563e-05, 'epoch': 0.14}\n",
      "{'loss': 4.0193, 'grad_norm': 982.7911376953125, 'learning_rate': 1.9984748361445306e-05, 'epoch': 0.15}\n",
      "{'loss': 4.1683, 'grad_norm': 1233.708984375, 'learning_rate': 1.998278289425234e-05, 'epoch': 0.15}\n",
      "{'loss': 4.0453, 'grad_norm': 1162.9830322265625, 'learning_rate': 1.9980698448650805e-05, 'epoch': 0.15}\n",
      "{'loss': 4.1416, 'grad_norm': 1015.9295043945312, 'learning_rate': 1.9978495049483883e-05, 'epoch': 0.16}\n",
      "{'loss': 4.5732, 'grad_norm': 1149.9755859375, 'learning_rate': 1.997617272301248e-05, 'epoch': 0.16}\n",
      "{'loss': 3.5761, 'grad_norm': 485.80743408203125, 'learning_rate': 1.9973731496914914e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4697, 'grad_norm': 426.6372985839844, 'learning_rate': 1.9971171400286602e-05, 'epoch': 0.17}\n",
      "{'loss': 3.584, 'grad_norm': 519.0010375976562, 'learning_rate': 1.9968492463639704e-05, 'epoch': 0.17}\n",
      "{'loss': 3.6141, 'grad_norm': 439.4888916015625, 'learning_rate': 1.9965694718902745e-05, 'epoch': 0.17}\n",
      "{'loss': 3.612, 'grad_norm': 638.9364624023438, 'learning_rate': 1.9962778199420265e-05, 'epoch': 0.18}\n",
      "{'loss': 3.6236, 'grad_norm': 708.377197265625, 'learning_rate': 1.9959742939952393e-05, 'epoch': 0.18}\n",
      "{'loss': 3.5421, 'grad_norm': 555.514892578125, 'learning_rate': 1.9956588976674442e-05, 'epoch': 0.18}\n",
      "{'loss': 3.8792, 'grad_norm': 712.4412841796875, 'learning_rate': 1.995331634717649e-05, 'epoch': 0.19}\n",
      "{'loss': 4.0043, 'grad_norm': 686.3456420898438, 'learning_rate': 1.994992509046291e-05, 'epoch': 0.19}\n",
      "{'loss': 3.8543, 'grad_norm': 1017.60595703125, 'learning_rate': 1.9946415246951928e-05, 'epoch': 0.19}\n",
      "{'loss': 4.3037, 'grad_norm': 982.1740112304688, 'learning_rate': 1.9942786858475126e-05, 'epoch': 0.2}\n",
      "{'loss': 4.1594, 'grad_norm': 815.5997924804688, 'learning_rate': 1.9939039968276942e-05, 'epoch': 0.2}\n",
      "{'loss': 4.1602, 'grad_norm': 769.6072387695312, 'learning_rate': 1.9935174621014173e-05, 'epoch': 0.2}\n",
      "{'loss': 3.9514, 'grad_norm': 888.363525390625, 'learning_rate': 1.9931190862755416e-05, 'epoch': 0.2}\n",
      "{'loss': 3.9517, 'grad_norm': 787.8326416015625, 'learning_rate': 1.992708874098054e-05, 'epoch': 0.21}\n",
      "{'loss': 4.0151, 'grad_norm': 700.5433349609375, 'learning_rate': 1.992286830458012e-05, 'epoch': 0.21}\n",
      "{'loss': 4.1257, 'grad_norm': 735.136962890625, 'learning_rate': 1.9918529603854825e-05, 'epoch': 0.21}\n",
      "{'loss': 3.9699, 'grad_norm': 715.1495971679688, 'learning_rate': 1.991407269051487e-05, 'epoch': 0.22}\n",
      "{'loss': 4.2512, 'grad_norm': 893.5902709960938, 'learning_rate': 1.990949761767935e-05, 'epoch': 0.22}\n",
      "{'loss': 4.1972, 'grad_norm': 738.2159423828125, 'learning_rate': 1.9904804439875635e-05, 'epoch': 0.22}\n",
      "{'loss': 4.2964, 'grad_norm': 770.48046875, 'learning_rate': 1.989999321303871e-05, 'epoch': 0.23}\n",
      "{'loss': 4.1421, 'grad_norm': 791.3756103515625, 'learning_rate': 1.9895063994510512e-05, 'epoch': 0.23}\n",
      "{'loss': 4.1972, 'grad_norm': 821.22119140625, 'learning_rate': 1.989001684303925e-05, 'epoch': 0.23}\n",
      "{'loss': 3.7938, 'grad_norm': 653.3248901367188, 'learning_rate': 1.9884851818778695e-05, 'epoch': 0.24}\n",
      "{'loss': 4.0045, 'grad_norm': 713.2860107421875, 'learning_rate': 1.9879568983287468e-05, 'epoch': 0.24}\n",
      "{'loss': 4.3865, 'grad_norm': 630.612548828125, 'learning_rate': 1.9874168399528307e-05, 'epoch': 0.24}\n",
      "{'loss': 3.6999, 'grad_norm': 711.52392578125, 'learning_rate': 1.986865013186732e-05, 'epoch': 0.25}\n",
      "{'loss': 4.0894, 'grad_norm': 669.0634765625, 'learning_rate': 1.9863014246073216e-05, 'epoch': 0.25}\n",
      "{'loss': 4.1404, 'grad_norm': 664.8681030273438, 'learning_rate': 1.985726080931651e-05, 'epoch': 0.25}\n",
      "{'loss': 4.0165, 'grad_norm': 818.0161743164062, 'learning_rate': 1.9851389890168738e-05, 'epoch': 0.26}\n",
      "{'loss': 4.2078, 'grad_norm': 745.3424682617188, 'learning_rate': 1.9845401558601634e-05, 'epoch': 0.26}\n",
      "{'loss': 4.0232, 'grad_norm': 729.968017578125, 'learning_rate': 1.98392958859863e-05, 'epoch': 0.26}\n",
      "{'loss': 4.2939, 'grad_norm': 771.5748901367188, 'learning_rate': 1.9833072945092334e-05, 'epoch': 0.27}\n",
      "{'loss': 4.1605, 'grad_norm': 897.3772583007812, 'learning_rate': 1.9826732810087e-05, 'epoch': 0.27}\n",
      "{'loss': 3.9693, 'grad_norm': 695.7412719726562, 'learning_rate': 1.9820275556534306e-05, 'epoch': 0.27}\n",
      "{'loss': 3.8213, 'grad_norm': 793.1810302734375, 'learning_rate': 1.9813701261394136e-05, 'epoch': 0.28}\n",
      "{'loss': 4.0736, 'grad_norm': 727.0285034179688, 'learning_rate': 1.980701000302131e-05, 'epoch': 0.28}\n",
      "{'loss': 3.6419, 'grad_norm': 731.1748046875, 'learning_rate': 1.9800201861164665e-05, 'epoch': 0.28}\n",
      "{'loss': 4.174, 'grad_norm': 878.7958984375, 'learning_rate': 1.979327691696608e-05, 'epoch': 0.28}\n",
      "{'loss': 4.2802, 'grad_norm': 679.501953125, 'learning_rate': 1.9786235252959555e-05, 'epoch': 0.29}\n",
      "{'loss': 3.97, 'grad_norm': 820.3197021484375, 'learning_rate': 1.977907695307017e-05, 'epoch': 0.29}\n",
      "{'loss': 3.6461, 'grad_norm': 755.010009765625, 'learning_rate': 1.9771802102613127e-05, 'epoch': 0.29}\n",
      "{'loss': 3.6474, 'grad_norm': 922.7470703125, 'learning_rate': 1.9764410788292724e-05, 'epoch': 0.3}\n",
      "{'loss': 4.1151, 'grad_norm': 1175.52099609375, 'learning_rate': 1.975690309820131e-05, 'epoch': 0.3}\n",
      "{'loss': 3.7153, 'grad_norm': 824.7984619140625, 'learning_rate': 1.9749279121818235e-05, 'epoch': 0.3}\n",
      "{'loss': 3.8259, 'grad_norm': 797.1742553710938, 'learning_rate': 1.9741538950008817e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3699, 'grad_norm': 848.6318969726562, 'learning_rate': 1.9733682675023207e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3918, 'grad_norm': 908.2991943359375, 'learning_rate': 1.972571039049533e-05, 'epoch': 0.31}\n",
      "{'loss': 3.56, 'grad_norm': 690.3239135742188, 'learning_rate': 1.971762219144174e-05, 'epoch': 0.32}\n",
      "{'loss': 3.581, 'grad_norm': 1318.628662109375, 'learning_rate': 1.9709418174260523e-05, 'epoch': 0.32}\n",
      "{'loss': 3.5691, 'grad_norm': 360.3301696777344, 'learning_rate': 1.9701098436730108e-05, 'epoch': 0.32}\n",
      "{'loss': 3.5794, 'grad_norm': 384.8272705078125, 'learning_rate': 1.969266307800813e-05, 'epoch': 0.33}\n",
      "{'loss': 3.7001, 'grad_norm': 414.98516845703125, 'learning_rate': 1.9684112198630246e-05, 'epoch': 0.33}\n",
      "{'loss': 3.4672, 'grad_norm': 480.7335510253906, 'learning_rate': 1.967544590050891e-05, 'epoch': 0.33}\n",
      "{'loss': 3.4846, 'grad_norm': 425.4966125488281, 'learning_rate': 1.9666664286932198e-05, 'epoch': 0.34}\n",
      "{'loss': 3.6409, 'grad_norm': 561.1715087890625, 'learning_rate': 1.9657767462562544e-05, 'epoch': 0.34}\n",
      "{'loss': 3.7004, 'grad_norm': 611.4946899414062, 'learning_rate': 1.9648755533435517e-05, 'epoch': 0.34}\n",
      "{'loss': 3.7913, 'grad_norm': 611.8681030273438, 'learning_rate': 1.9639628606958535e-05, 'epoch': 0.35}\n",
      "{'loss': 3.864, 'grad_norm': 613.6282348632812, 'learning_rate': 1.96303867919096e-05, 'epoch': 0.35}\n",
      "{'loss': 3.8333, 'grad_norm': 716.2864379882812, 'learning_rate': 1.9621030198436007e-05, 'epoch': 0.35}\n",
      "{'loss': 3.8313, 'grad_norm': 635.6500244140625, 'learning_rate': 1.9611558938053003e-05, 'epoch': 0.36}\n",
      "{'loss': 3.9256, 'grad_norm': 734.4190063476562, 'learning_rate': 1.9601973123642493e-05, 'epoch': 0.36}\n",
      "{'loss': 3.7517, 'grad_norm': 607.7977905273438, 'learning_rate': 1.9592272869451672e-05, 'epoch': 0.36}\n",
      "{'loss': 3.7954, 'grad_norm': 570.5303344726562, 'learning_rate': 1.9582458291091664e-05, 'epoch': 0.36}\n",
      "{'loss': 3.9452, 'grad_norm': 653.4329833984375, 'learning_rate': 1.957252950553616e-05, 'epoch': 0.37}\n",
      "{'loss': 4.2676, 'grad_norm': 635.9562377929688, 'learning_rate': 1.9562486631120007e-05, 'epoch': 0.37}\n",
      "{'loss': 4.0707, 'grad_norm': 560.937255859375, 'learning_rate': 1.9552329787537805e-05, 'epoch': 0.37}\n",
      "{'loss': 4.2767, 'grad_norm': 579.6201782226562, 'learning_rate': 1.9542059095842484e-05, 'epoch': 0.38}\n",
      "{'loss': 3.6956, 'grad_norm': 565.275146484375, 'learning_rate': 1.9531674678443853e-05, 'epoch': 0.38}\n",
      "{'loss': 3.785, 'grad_norm': 558.6279907226562, 'learning_rate': 1.952117665910714e-05, 'epoch': 0.38}\n",
      "{'loss': 4.2072, 'grad_norm': 485.4454650878906, 'learning_rate': 1.9510565162951538e-05, 'epoch': 0.39}\n",
      "{'loss': 3.9771, 'grad_norm': 515.3506469726562, 'learning_rate': 1.9499840316448675e-05, 'epoch': 0.39}\n",
      "{'loss': 3.9049, 'grad_norm': 630.528076171875, 'learning_rate': 1.948900224742115e-05, 'epoch': 0.39}\n",
      "{'loss': 3.8167, 'grad_norm': 471.240966796875, 'learning_rate': 1.9478051085040978e-05, 'epoch': 0.4}\n",
      "{'loss': 3.7696, 'grad_norm': 445.46051025390625, 'learning_rate': 1.9466986959828063e-05, 'epoch': 0.4}\n",
      "{'loss': 3.9851, 'grad_norm': 530.3735961914062, 'learning_rate': 1.945581000364864e-05, 'epoch': 0.4}\n",
      "{'loss': 3.4821, 'grad_norm': 513.404541015625, 'learning_rate': 1.9444520349713705e-05, 'epoch': 0.41}\n",
      "{'loss': 3.6048, 'grad_norm': 485.7118225097656, 'learning_rate': 1.9433118132577432e-05, 'epoch': 0.41}\n",
      "{'loss': 3.7038, 'grad_norm': 533.864013671875, 'learning_rate': 1.942160348813556e-05, 'epoch': 0.41}\n",
      "{'loss': 3.9442, 'grad_norm': 591.3815307617188, 'learning_rate': 1.9409976553623767e-05, 'epoch': 0.42}\n",
      "{'loss': 3.627, 'grad_norm': 425.6654052734375, 'learning_rate': 1.9398237467616063e-05, 'epoch': 0.42}\n",
      "{'loss': 3.5278, 'grad_norm': 391.8355407714844, 'learning_rate': 1.9386386370023104e-05, 'epoch': 0.42}\n",
      "{'loss': 3.8114, 'grad_norm': 508.605712890625, 'learning_rate': 1.9374423402090553e-05, 'epoch': 0.43}\n",
      "{'loss': 4.265, 'grad_norm': 393.5132751464844, 'learning_rate': 1.9362348706397374e-05, 'epoch': 0.43}\n",
      "{'loss': 3.8713, 'grad_norm': 348.7862243652344, 'learning_rate': 1.9350162426854152e-05, 'epoch': 0.43}\n",
      "{'loss': 3.8632, 'grad_norm': 385.10595703125, 'learning_rate': 1.933786470870136e-05, 'epoch': 0.44}\n",
      "{'loss': 4.1432, 'grad_norm': 382.6690979003906, 'learning_rate': 1.9325455698507638e-05, 'epoch': 0.44}\n",
      "{'loss': 4.3206, 'grad_norm': 424.7291259765625, 'learning_rate': 1.931293554416805e-05, 'epoch': 0.44}\n",
      "{'loss': 3.628, 'grad_norm': 342.1102600097656, 'learning_rate': 1.9300304394902315e-05, 'epoch': 0.44}\n",
      "{'loss': 3.8524, 'grad_norm': 369.2185363769531, 'learning_rate': 1.9287562401253023e-05, 'epoch': 0.45}\n",
      "{'loss': 3.7849, 'grad_norm': 389.68646240234375, 'learning_rate': 1.927470971508386e-05, 'epoch': 0.45}\n",
      "{'loss': 4.0378, 'grad_norm': 391.1567687988281, 'learning_rate': 1.9261746489577767e-05, 'epoch': 0.45}\n",
      "{'loss': 3.8945, 'grad_norm': 402.6817626953125, 'learning_rate': 1.924867287923515e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2948, 'grad_norm': 403.0817565917969, 'learning_rate': 1.923548903987201e-05, 'epoch': 0.46}\n",
      "{'loss': 3.5839, 'grad_norm': 413.55364990234375, 'learning_rate': 1.9222195128618108e-05, 'epoch': 0.46}\n",
      "{'loss': 3.7777, 'grad_norm': 429.12530517578125, 'learning_rate': 1.9208791303915063e-05, 'epoch': 0.47}\n",
      "{'loss': 3.5295, 'grad_norm': 403.9469299316406, 'learning_rate': 1.919527772551451e-05, 'epoch': 0.47}\n",
      "{'loss': 3.5079, 'grad_norm': 389.1956787109375, 'learning_rate': 1.918165455447614e-05, 'epoch': 0.47}\n",
      "{'loss': 3.6291, 'grad_norm': 515.6707763671875, 'learning_rate': 1.9167921953165827e-05, 'epoch': 0.48}\n",
      "{'loss': 3.8446, 'grad_norm': 1205.201904296875, 'learning_rate': 1.9154080085253665e-05, 'epoch': 0.48}\n",
      "{'loss': 3.7407, 'grad_norm': 254.86614990234375, 'learning_rate': 1.9140129115712035e-05, 'epoch': 0.48}\n",
      "{'loss': 3.5472, 'grad_norm': 292.95013427734375, 'learning_rate': 1.912606921081362e-05, 'epoch': 0.49}\n",
      "{'loss': 3.6844, 'grad_norm': 236.79605102539062, 'learning_rate': 1.9111900538129443e-05, 'epoch': 0.49}\n",
      "{'loss': 3.6248, 'grad_norm': 237.50804138183594, 'learning_rate': 1.909762326652686e-05, 'epoch': 0.49}\n",
      "{'loss': 3.7138, 'grad_norm': 268.8944091796875, 'learning_rate': 1.908323756616754e-05, 'epoch': 0.5}\n",
      "{'loss': 3.704, 'grad_norm': 336.31329345703125, 'learning_rate': 1.9068743608505454e-05, 'epoch': 0.5}\n",
      "{'loss': 3.7503, 'grad_norm': 309.8310852050781, 'learning_rate': 1.9054141566284822e-05, 'epoch': 0.5}\n",
      "{'loss': 3.9026, 'grad_norm': 333.2486267089844, 'learning_rate': 1.9039431613538047e-05, 'epoch': 0.51}\n",
      "{'loss': 4.176, 'grad_norm': 381.112548828125, 'learning_rate': 1.9024613925583652e-05, 'epoch': 0.51}\n",
      "{'loss': 3.8957, 'grad_norm': 327.3789978027344, 'learning_rate': 1.900968867902419e-05, 'epoch': 0.51}\n",
      "{'loss': 3.9879, 'grad_norm': 311.39581298828125, 'learning_rate': 1.899465605174414e-05, 'epoch': 0.52}\n",
      "{'loss': 4.002, 'grad_norm': 419.7293701171875, 'learning_rate': 1.8979516222907776e-05, 'epoch': 0.52}\n",
      "{'loss': 3.7966, 'grad_norm': 328.15252685546875, 'learning_rate': 1.896426937295704e-05, 'epoch': 0.52}\n",
      "{'loss': 4.1663, 'grad_norm': 335.1750793457031, 'learning_rate': 1.8948915683609387e-05, 'epoch': 0.52}\n",
      "{'loss': 3.6649, 'grad_norm': 286.1286315917969, 'learning_rate': 1.8933455337855633e-05, 'epoch': 0.53}\n",
      "{'loss': 4.158, 'grad_norm': 349.30712890625, 'learning_rate': 1.8917888519957756e-05, 'epoch': 0.53}\n",
      "{'loss': 3.6339, 'grad_norm': 283.63909912109375, 'learning_rate': 1.89022154154467e-05, 'epoch': 0.53}\n",
      "{'loss': 3.7626, 'grad_norm': 350.4730529785156, 'learning_rate': 1.8886436211120195e-05, 'epoch': 0.54}\n",
      "{'loss': 4.077, 'grad_norm': 387.3306579589844, 'learning_rate': 1.8870551095040476e-05, 'epoch': 0.54}\n",
      "{'loss': 3.7142, 'grad_norm': 395.45623779296875, 'learning_rate': 1.8854560256532098e-05, 'epoch': 0.54}\n",
      "{'loss': 3.7203, 'grad_norm': 345.4356384277344, 'learning_rate': 1.8838463886179647e-05, 'epoch': 0.55}\n",
      "{'loss': 3.9156, 'grad_norm': 346.55615234375, 'learning_rate': 1.8822262175825463e-05, 'epoch': 0.55}\n",
      "{'loss': 3.9523, 'grad_norm': 357.5606994628906, 'learning_rate': 1.880595531856738e-05, 'epoch': 0.55}\n",
      "{'loss': 4.0147, 'grad_norm': 428.5819396972656, 'learning_rate': 1.878954350875641e-05, 'epoch': 0.56}\n",
      "{'loss': 4.0394, 'grad_norm': 345.8794250488281, 'learning_rate': 1.877302694199442e-05, 'epoch': 0.56}\n",
      "{'loss': 3.8355, 'grad_norm': 407.66241455078125, 'learning_rate': 1.8756405815131815e-05, 'epoch': 0.56}\n",
      "{'loss': 3.7202, 'grad_norm': 395.7967224121094, 'learning_rate': 1.873968032626518e-05, 'epoch': 0.57}\n",
      "{'loss': 3.9053, 'grad_norm': 341.1965637207031, 'learning_rate': 1.872285067473493e-05, 'epoch': 0.57}\n",
      "{'loss': 3.8407, 'grad_norm': 386.27630615234375, 'learning_rate': 1.8705917061122917e-05, 'epoch': 0.57}\n",
      "{'loss': 3.823, 'grad_norm': 411.7342224121094, 'learning_rate': 1.8688879687250067e-05, 'epoch': 0.58}\n",
      "{'loss': 3.88, 'grad_norm': 377.2381896972656, 'learning_rate': 1.8671738756173946e-05, 'epoch': 0.58}\n",
      "{'loss': 3.8312, 'grad_norm': 454.8456726074219, 'learning_rate': 1.8654494472186352e-05, 'epoch': 0.58}\n",
      "{'loss': 3.9315, 'grad_norm': 371.6017761230469, 'learning_rate': 1.8637147040810884e-05, 'epoch': 0.59}\n",
      "{'loss': 3.6284, 'grad_norm': 483.7190246582031, 'learning_rate': 1.8619696668800494e-05, 'epoch': 0.59}\n",
      "{'loss': 3.8684, 'grad_norm': 427.1687927246094, 'learning_rate': 1.860214356413501e-05, 'epoch': 0.59}\n",
      "{'loss': 3.4849, 'grad_norm': 435.3529968261719, 'learning_rate': 1.8584487936018663e-05, 'epoch': 0.6}\n",
      "{'loss': 3.628, 'grad_norm': 353.3283996582031, 'learning_rate': 1.8566729994877604e-05, 'epoch': 0.6}\n",
      "{'loss': 3.6086, 'grad_norm': 478.9867858886719, 'learning_rate': 1.854886995235738e-05, 'epoch': 0.6}\n",
      "{'loss': 3.7628, 'grad_norm': 403.65423583984375, 'learning_rate': 1.8530908021320427e-05, 'epoch': 0.6}\n",
      "{'loss': 3.4015, 'grad_norm': 376.2209777832031, 'learning_rate': 1.8512844415843514e-05, 'epoch': 0.61}\n",
      "{'loss': 3.6969, 'grad_norm': 423.84747314453125, 'learning_rate': 1.8494679351215212e-05, 'epoch': 0.61}\n",
      "{'loss': 3.6371, 'grad_norm': 328.0155944824219, 'learning_rate': 1.8476413043933316e-05, 'epoch': 0.61}\n",
      "{'loss': 3.9675, 'grad_norm': 433.15521240234375, 'learning_rate': 1.8458045711702264e-05, 'epoch': 0.62}\n",
      "{'loss': 3.5686, 'grad_norm': 393.9710998535156, 'learning_rate': 1.8439577573430557e-05, 'epoch': 0.62}\n",
      "{'loss': 3.1505, 'grad_norm': 378.61688232421875, 'learning_rate': 1.842100884922812e-05, 'epoch': 0.62}\n",
      "{'loss': 3.6061, 'grad_norm': 368.7760925292969, 'learning_rate': 1.8402339760403715e-05, 'epoch': 0.63}\n",
      "{'loss': 3.1004, 'grad_norm': 335.7938232421875, 'learning_rate': 1.8383570529462273e-05, 'epoch': 0.63}\n",
      "{'loss': 3.4779, 'grad_norm': 608.5419921875, 'learning_rate': 1.8364701380102267e-05, 'epoch': 0.63}\n",
      "{'loss': 3.6583, 'grad_norm': 453.32080078125, 'learning_rate': 1.834573253721303e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3538, 'grad_norm': 473.1178894042969, 'learning_rate': 1.8326664226872063e-05, 'epoch': 0.64}\n",
      "{'loss': 3.5992, 'grad_norm': 227.5435028076172, 'learning_rate': 1.8307496676342384e-05, 'epoch': 0.64}\n",
      "{'loss': 3.5169, 'grad_norm': 239.9166717529297, 'learning_rate': 1.828823011406977e-05, 'epoch': 0.65}\n",
      "{'loss': 3.6663, 'grad_norm': 254.79383850097656, 'learning_rate': 1.8268864769680054e-05, 'epoch': 0.65}\n",
      "{'loss': 3.4983, 'grad_norm': 203.3697967529297, 'learning_rate': 1.824940087397641e-05, 'epoch': 0.65}\n",
      "{'loss': 3.9071, 'grad_norm': 255.58706665039062, 'learning_rate': 1.8229838658936566e-05, 'epoch': 0.66}\n",
      "{'loss': 3.8385, 'grad_norm': 357.4445495605469, 'learning_rate': 1.8210178357710057e-05, 'epoch': 0.66}\n",
      "{'loss': 3.747, 'grad_norm': 261.80535888671875, 'learning_rate': 1.819042020461545e-05, 'epoch': 0.66}\n",
      "{'loss': 4.0473, 'grad_norm': 281.1827697753906, 'learning_rate': 1.8170564435137542e-05, 'epoch': 0.67}\n",
      "{'loss': 3.963, 'grad_norm': 276.4062805175781, 'learning_rate': 1.8150611285924556e-05, 'epoch': 0.67}\n",
      "{'loss': 4.1137, 'grad_norm': 257.4859619140625, 'learning_rate': 1.8130560994785325e-05, 'epoch': 0.67}\n",
      "{'loss': 4.0075, 'grad_norm': 227.27186584472656, 'learning_rate': 1.8110413800686456e-05, 'epoch': 0.68}\n",
      "{'loss': 4.0944, 'grad_norm': 209.96896362304688, 'learning_rate': 1.8090169943749477e-05, 'epoch': 0.68}\n",
      "{'loss': 4.0706, 'grad_norm': 236.29586791992188, 'learning_rate': 1.8069829665247975e-05, 'epoch': 0.68}\n",
      "{'loss': 3.7988, 'grad_norm': 253.44863891601562, 'learning_rate': 1.8049393207604734e-05, 'epoch': 0.68}\n",
      "{'loss': 4.1506, 'grad_norm': 172.69796752929688, 'learning_rate': 1.8028860814388826e-05, 'epoch': 0.69}\n",
      "{'loss': 3.8004, 'grad_norm': 201.54898071289062, 'learning_rate': 1.8008232730312724e-05, 'epoch': 0.69}\n",
      "{'loss': 4.0954, 'grad_norm': 239.25157165527344, 'learning_rate': 1.7987509201229378e-05, 'epoch': 0.69}\n",
      "{'loss': 3.9949, 'grad_norm': 256.1719665527344, 'learning_rate': 1.7966690474129285e-05, 'epoch': 0.7}\n",
      "{'loss': 4.1024, 'grad_norm': 195.30233764648438, 'learning_rate': 1.7945776797137544e-05, 'epoch': 0.7}\n",
      "{'loss': 3.6817, 'grad_norm': 225.9134521484375, 'learning_rate': 1.7924768419510906e-05, 'epoch': 0.7}\n",
      "{'loss': 4.1511, 'grad_norm': 193.69857788085938, 'learning_rate': 1.7903665591634794e-05, 'epoch': 0.71}\n",
      "{'loss': 3.6439, 'grad_norm': 233.27627563476562, 'learning_rate': 1.7882468565020327e-05, 'epoch': 0.71}\n",
      "{'loss': 3.8499, 'grad_norm': 233.2991485595703, 'learning_rate': 1.786117759230132e-05, 'epoch': 0.71}\n",
      "{'loss': 3.6448, 'grad_norm': 287.9501037597656, 'learning_rate': 1.7839792927231253e-05, 'epoch': 0.72}\n",
      "{'loss': 4.0365, 'grad_norm': 234.7271728515625, 'learning_rate': 1.78183148246803e-05, 'epoch': 0.72}\n",
      "{'loss': 3.8103, 'grad_norm': 228.12147521972656, 'learning_rate': 1.7796743540632226e-05, 'epoch': 0.72}\n",
      "{'loss': 4.0825, 'grad_norm': 258.7074890136719, 'learning_rate': 1.777507933218138e-05, 'epoch': 0.73}\n",
      "{'loss': 4.0034, 'grad_norm': 243.20013427734375, 'learning_rate': 1.7753322457529615e-05, 'epoch': 0.73}\n",
      "{'loss': 3.8797, 'grad_norm': 267.5581359863281, 'learning_rate': 1.7731473175983215e-05, 'epoch': 0.73}\n",
      "{'loss': 3.8175, 'grad_norm': 257.2535095214844, 'learning_rate': 1.7709531747949796e-05, 'epoch': 0.74}\n",
      "{'loss': 3.8978, 'grad_norm': 236.46435546875, 'learning_rate': 1.7687498434935224e-05, 'epoch': 0.74}\n",
      "{'loss': 3.7081, 'grad_norm': 282.7410583496094, 'learning_rate': 1.7665373499540464e-05, 'epoch': 0.74}\n",
      "{'loss': 3.8133, 'grad_norm': 365.6062316894531, 'learning_rate': 1.7643157205458483e-05, 'epoch': 0.75}\n",
      "{'loss': 3.8235, 'grad_norm': 227.24229431152344, 'learning_rate': 1.7620849817471094e-05, 'epoch': 0.75}\n",
      "{'loss': 3.373, 'grad_norm': 296.7728271484375, 'learning_rate': 1.759845160144579e-05, 'epoch': 0.75}\n",
      "{'loss': 3.5041, 'grad_norm': 276.9547424316406, 'learning_rate': 1.7575962824332595e-05, 'epoch': 0.76}\n",
      "{'loss': 3.8357, 'grad_norm': 265.7020263671875, 'learning_rate': 1.7553383754160864e-05, 'epoch': 0.76}\n",
      "{'loss': 3.6945, 'grad_norm': 242.42933654785156, 'learning_rate': 1.7530714660036112e-05, 'epoch': 0.76}\n",
      "{'loss': 4.0331, 'grad_norm': 308.3870849609375, 'learning_rate': 1.7507955812136775e-05, 'epoch': 0.76}\n",
      "{'loss': 3.8315, 'grad_norm': 273.7275390625, 'learning_rate': 1.7485107481711014e-05, 'epoch': 0.77}\n",
      "{'loss': 3.5928, 'grad_norm': 269.6811828613281, 'learning_rate': 1.7462169941073478e-05, 'epoch': 0.77}\n",
      "{'loss': 3.7827, 'grad_norm': 288.7001037597656, 'learning_rate': 1.7439143463602052e-05, 'epoch': 0.77}\n",
      "{'loss': 4.0602, 'grad_norm': 290.8311462402344, 'learning_rate': 1.74160283237346e-05, 'epoch': 0.78}\n",
      "{'loss': 3.6739, 'grad_norm': 366.4314270019531, 'learning_rate': 1.7392824796965703e-05, 'epoch': 0.78}\n",
      "{'loss': 3.5044, 'grad_norm': 343.53680419921875, 'learning_rate': 1.7369533159843368e-05, 'epoch': 0.78}\n",
      "{'loss': 2.9619, 'grad_norm': 320.541748046875, 'learning_rate': 1.734615368996573e-05, 'epoch': 0.79}\n",
      "{'loss': 3.0017, 'grad_norm': 358.0092468261719, 'learning_rate': 1.7322686665977738e-05, 'epoch': 0.79}\n",
      "{'loss': 3.1052, 'grad_norm': 345.7085266113281, 'learning_rate': 1.7299132367567856e-05, 'epoch': 0.79}\n",
      "{'loss': 3.9739, 'grad_norm': 378.9649658203125, 'learning_rate': 1.7275491075464716e-05, 'epoch': 0.8}\n",
      "{'loss': 3.4665, 'grad_norm': 522.474365234375, 'learning_rate': 1.7251763071433767e-05, 'epoch': 0.8}\n",
      "{'loss': 3.4397, 'grad_norm': 189.32200622558594, 'learning_rate': 1.7227948638273918e-05, 'epoch': 0.8}\n",
      "{'loss': 3.5687, 'grad_norm': 180.89002990722656, 'learning_rate': 1.7204048059814175e-05, 'epoch': 0.81}\n",
      "{'loss': 3.6708, 'grad_norm': 213.6588134765625, 'learning_rate': 1.7180061620910263e-05, 'epoch': 0.81}\n",
      "{'loss': 3.6975, 'grad_norm': 179.828125, 'learning_rate': 1.715598960744121e-05, 'epoch': 0.81}\n",
      "{'loss': 3.5792, 'grad_norm': 245.00982666015625, 'learning_rate': 1.7131832306305964e-05, 'epoch': 0.82}\n",
      "{'loss': 3.7479, 'grad_norm': 207.39306640625, 'learning_rate': 1.710759000541995e-05, 'epoch': 0.82}\n",
      "{'loss': 3.8106, 'grad_norm': 250.56822204589844, 'learning_rate': 1.7083262993711663e-05, 'epoch': 0.82}\n",
      "{'loss': 3.8781, 'grad_norm': 253.75485229492188, 'learning_rate': 1.7058851561119198e-05, 'epoch': 0.83}\n",
      "{'loss': 3.8536, 'grad_norm': 201.4251708984375, 'learning_rate': 1.7034355998586828e-05, 'epoch': 0.83}\n",
      "{'loss': 4.2335, 'grad_norm': 262.30377197265625, 'learning_rate': 1.7009776598061496e-05, 'epoch': 0.83}\n",
      "{'loss': 3.9617, 'grad_norm': 202.80520629882812, 'learning_rate': 1.6985113652489374e-05, 'epoch': 0.84}\n",
      "{'loss': 3.8283, 'grad_norm': 233.06814575195312, 'learning_rate': 1.6960367455812336e-05, 'epoch': 0.84}\n",
      "{'loss': 3.96, 'grad_norm': 259.1177673339844, 'learning_rate': 1.6935538302964496e-05, 'epoch': 0.84}\n",
      "{'loss': 4.2096, 'grad_norm': 248.18994140625, 'learning_rate': 1.691062648986865e-05, 'epoch': 0.84}\n",
      "{'loss': 3.9578, 'grad_norm': 219.859619140625, 'learning_rate': 1.6885632313432772e-05, 'epoch': 0.85}\n",
      "{'loss': 3.7402, 'grad_norm': 226.76007080078125, 'learning_rate': 1.686055607154648e-05, 'epoch': 0.85}\n",
      "{'loss': 3.6938, 'grad_norm': 265.12255859375, 'learning_rate': 1.6835398063077476e-05, 'epoch': 0.85}\n",
      "{'loss': 3.6796, 'grad_norm': 246.4375762939453, 'learning_rate': 1.6810158587867973e-05, 'epoch': 0.86}\n",
      "{'loss': 3.9773, 'grad_norm': 244.4228515625, 'learning_rate': 1.6784837946731148e-05, 'epoch': 0.86}\n",
      "{'loss': 3.79, 'grad_norm': 320.3280029296875, 'learning_rate': 1.6759436441447544e-05, 'epoch': 0.86}\n",
      "{'loss': 3.6145, 'grad_norm': 274.9872131347656, 'learning_rate': 1.673395437476146e-05, 'epoch': 0.87}\n",
      "{'loss': 3.7488, 'grad_norm': 301.2032775878906, 'learning_rate': 1.6708392050377365e-05, 'epoch': 0.87}\n",
      "{'loss': 3.8057, 'grad_norm': 269.6919860839844, 'learning_rate': 1.668274977295626e-05, 'epoch': 0.87}\n",
      "{'loss': 3.7773, 'grad_norm': 229.89903259277344, 'learning_rate': 1.6657027848112064e-05, 'epoch': 0.88}\n",
      "{'loss': 4.1109, 'grad_norm': 279.5765686035156, 'learning_rate': 1.6631226582407954e-05, 'epoch': 0.88}\n",
      "{'loss': 3.6196, 'grad_norm': 293.50775146484375, 'learning_rate': 1.660534628335273e-05, 'epoch': 0.88}\n",
      "{'loss': 3.4464, 'grad_norm': 227.59181213378906, 'learning_rate': 1.657938725939713e-05, 'epoch': 0.89}\n",
      "{'loss': 3.6343, 'grad_norm': 225.41326904296875, 'learning_rate': 1.6553349819930167e-05, 'epoch': 0.89}\n",
      "{'loss': 3.791, 'grad_norm': 203.53965759277344, 'learning_rate': 1.6527234275275445e-05, 'epoch': 0.89}\n",
      "{'loss': 3.9601, 'grad_norm': 260.8529968261719, 'learning_rate': 1.6501040936687444e-05, 'epoch': 0.9}\n",
      "{'loss': 3.431, 'grad_norm': 232.5889892578125, 'learning_rate': 1.6474770116347824e-05, 'epoch': 0.9}\n",
      "{'loss': 3.7655, 'grad_norm': 257.8642883300781, 'learning_rate': 1.6448422127361707e-05, 'epoch': 0.9}\n",
      "{'loss': 3.6802, 'grad_norm': 313.7982482910156, 'learning_rate': 1.6421997283753928e-05, 'epoch': 0.91}\n",
      "{'loss': 3.5705, 'grad_norm': 269.6110534667969, 'learning_rate': 1.6395495900465306e-05, 'epoch': 0.91}\n",
      "{'loss': 3.6542, 'grad_norm': 300.8401794433594, 'learning_rate': 1.6368918293348893e-05, 'epoch': 0.91}\n",
      "{'loss': 3.8497, 'grad_norm': 280.759033203125, 'learning_rate': 1.63422647791662e-05, 'epoch': 0.92}\n",
      "{'loss': 3.7817, 'grad_norm': 300.58587646484375, 'learning_rate': 1.6315535675583425e-05, 'epoch': 0.92}\n",
      "{'loss': 3.8347, 'grad_norm': 310.1039123535156, 'learning_rate': 1.6288731301167667e-05, 'epoch': 0.92}\n",
      "{'loss': 3.8355, 'grad_norm': 311.36077880859375, 'learning_rate': 1.626185197538314e-05, 'epoch': 0.92}\n",
      "{'loss': 3.6746, 'grad_norm': 381.5538024902344, 'learning_rate': 1.6234898018587336e-05, 'epoch': 0.93}\n",
      "{'loss': 3.714, 'grad_norm': 308.0241394042969, 'learning_rate': 1.6207869752027248e-05, 'epoch': 0.93}\n",
      "{'loss': 3.4817, 'grad_norm': 318.5294189453125, 'learning_rate': 1.6180767497835503e-05, 'epoch': 0.93}\n",
      "{'loss': 3.3901, 'grad_norm': 435.1400146484375, 'learning_rate': 1.6153591579026545e-05, 'epoch': 0.94}\n",
      "{'loss': 3.0869, 'grad_norm': 273.54168701171875, 'learning_rate': 1.6126342319492784e-05, 'epoch': 0.94}\n",
      "{'loss': 3.3272, 'grad_norm': 346.5904846191406, 'learning_rate': 1.609902004400073e-05, 'epoch': 0.94}\n",
      "{'loss': 3.3269, 'grad_norm': 400.756103515625, 'learning_rate': 1.6071625078187113e-05, 'epoch': 0.95}\n",
      "{'loss': 3.3844, 'grad_norm': 410.6910705566406, 'learning_rate': 1.6044157748555024e-05, 'epoch': 0.95}\n",
      "{'loss': 3.2422, 'grad_norm': 408.6650390625, 'learning_rate': 1.6016618382470014e-05, 'epoch': 0.95}\n",
      "{'loss': 3.6854, 'grad_norm': 451.6663818359375, 'learning_rate': 1.598900730815617e-05, 'epoch': 0.96}\n",
      "{'loss': 3.4134, 'grad_norm': 533.9920654296875, 'learning_rate': 1.5961324854692254e-05, 'epoch': 0.96}\n",
      "{'loss': 3.6918, 'grad_norm': 189.71005249023438, 'learning_rate': 1.593357135200773e-05, 'epoch': 0.96}\n",
      "{'loss': 3.679, 'grad_norm': 210.22906494140625, 'learning_rate': 1.5905747130878853e-05, 'epoch': 0.97}\n",
      "{'loss': 4.0425, 'grad_norm': 263.99237060546875, 'learning_rate': 1.5877852522924733e-05, 'epoch': 0.97}\n",
      "{'loss': 4.0937, 'grad_norm': 346.00360107421875, 'learning_rate': 1.5849887860603374e-05, 'epoch': 0.97}\n",
      "{'loss': 3.8139, 'grad_norm': 289.4018859863281, 'learning_rate': 1.582185347720771e-05, 'epoch': 0.98}\n",
      "{'loss': 3.677, 'grad_norm': 307.5521545410156, 'learning_rate': 1.5793749706861637e-05, 'epoch': 0.98}\n",
      "{'loss': 4.0964, 'grad_norm': 280.57330322265625, 'learning_rate': 1.576557688451603e-05, 'epoch': 0.98}\n",
      "{'loss': 4.07, 'grad_norm': 377.9961242675781, 'learning_rate': 1.5737335345944758e-05, 'epoch': 0.99}\n",
      "{'loss': 3.8657, 'grad_norm': 258.3074035644531, 'learning_rate': 1.570902542774066e-05, 'epoch': 0.99}\n",
      "{'loss': 3.4727, 'grad_norm': 310.66571044921875, 'learning_rate': 1.568064746731156e-05, 'epoch': 0.99}\n",
      "{'loss': 3.605, 'grad_norm': 313.0718688964844, 'learning_rate': 1.5652201802876227e-05, 'epoch': 1.0}\n",
      "{'loss': 3.0715, 'grad_norm': 373.41131591796875, 'learning_rate': 1.5623688773460358e-05, 'epoch': 1.0}\n",
      "{'loss': 3.0948, 'grad_norm': 537.1317138671875, 'learning_rate': 1.559510871889252e-05, 'epoch': 1.0}\n",
      "{'loss': 3.7736, 'grad_norm': 183.36773681640625, 'learning_rate': 1.556646197980012e-05, 'epoch': 1.0}\n",
      "{'loss': 3.5218, 'grad_norm': 192.42735290527344, 'learning_rate': 1.553774889760533e-05, 'epoch': 1.01}\n",
      "{'loss': 3.5428, 'grad_norm': 172.6455078125, 'learning_rate': 1.5508969814521026e-05, 'epoch': 1.01}\n",
      "{'loss': 3.4367, 'grad_norm': 188.0196533203125, 'learning_rate': 1.5480125073546705e-05, 'epoch': 1.01}\n",
      "{'loss': 3.6404, 'grad_norm': 190.76451110839844, 'learning_rate': 1.5451215018464386e-05, 'epoch': 1.02}\n",
      "{'loss': 3.8438, 'grad_norm': 222.21128845214844, 'learning_rate': 1.542223999383455e-05, 'epoch': 1.02}\n",
      "{'loss': 3.8146, 'grad_norm': 201.40220642089844, 'learning_rate': 1.5393200344991993e-05, 'epoch': 1.02}\n",
      "{'loss': 3.9612, 'grad_norm': 203.3741912841797, 'learning_rate': 1.5364096418041723e-05, 'epoch': 1.03}\n",
      "{'loss': 3.8593, 'grad_norm': 253.8621368408203, 'learning_rate': 1.533492855985485e-05, 'epoch': 1.03}\n",
      "{'loss': 3.8283, 'grad_norm': 281.75384521484375, 'learning_rate': 1.530569711806443e-05, 'epoch': 1.03}\n",
      "{'loss': 4.1273, 'grad_norm': 275.68243408203125, 'learning_rate': 1.527640244106133e-05, 'epoch': 1.04}\n",
      "{'loss': 3.971, 'grad_norm': 240.1037139892578, 'learning_rate': 1.524704487799008e-05, 'epoch': 1.04}\n",
      "{'loss': 4.0375, 'grad_norm': 302.9636535644531, 'learning_rate': 1.5217624778744718e-05, 'epoch': 1.04}\n",
      "{'loss': 4.0873, 'grad_norm': 280.4884338378906, 'learning_rate': 1.5188142493964595e-05, 'epoch': 1.04}\n",
      "{'loss': 3.9474, 'grad_norm': 271.6686096191406, 'learning_rate': 1.5158598375030218e-05, 'epoch': 1.05}\n",
      "{'loss': 3.9183, 'grad_norm': 285.09222412109375, 'learning_rate': 1.5128992774059063e-05, 'epoch': 1.05}\n",
      "{'loss': 3.7232, 'grad_norm': 299.57183837890625, 'learning_rate': 1.5099326043901361e-05, 'epoch': 1.05}\n",
      "{'loss': 3.752, 'grad_norm': 296.18157958984375, 'learning_rate': 1.5069598538135905e-05, 'epoch': 1.06}\n",
      "{'loss': 3.8363, 'grad_norm': 302.93145751953125, 'learning_rate': 1.503981061106584e-05, 'epoch': 1.06}\n",
      "{'loss': 4.0865, 'grad_norm': 286.0591125488281, 'learning_rate': 1.5009962617714425e-05, 'epoch': 1.06}\n",
      "{'loss': 3.9612, 'grad_norm': 318.4242248535156, 'learning_rate': 1.4980054913820814e-05, 'epoch': 1.07}\n",
      "{'loss': 3.4397, 'grad_norm': 313.8877258300781, 'learning_rate': 1.4950087855835816e-05, 'epoch': 1.07}\n",
      "{'loss': 3.9223, 'grad_norm': 357.3001403808594, 'learning_rate': 1.4920061800917637e-05, 'epoch': 1.07}\n",
      "{'loss': 3.7615, 'grad_norm': 323.5860900878906, 'learning_rate': 1.4889977106927642e-05, 'epoch': 1.08}\n",
      "{'loss': 3.6263, 'grad_norm': 398.5797424316406, 'learning_rate': 1.485983413242606e-05, 'epoch': 1.08}\n",
      "{'loss': 3.7276, 'grad_norm': 423.0249328613281, 'learning_rate': 1.4829633236667746e-05, 'epoch': 1.08}\n",
      "{'loss': 3.5561, 'grad_norm': 355.04248046875, 'learning_rate': 1.4799374779597866e-05, 'epoch': 1.09}\n",
      "{'loss': 3.8209, 'grad_norm': 375.3748474121094, 'learning_rate': 1.476905912184763e-05, 'epoch': 1.09}\n",
      "{'loss': 3.572, 'grad_norm': 398.02618408203125, 'learning_rate': 1.4738686624729987e-05, 'epoch': 1.09}\n",
      "{'loss': 3.638, 'grad_norm': 351.74676513671875, 'learning_rate': 1.470825765023532e-05, 'epoch': 1.1}\n",
      "{'loss': 3.6787, 'grad_norm': 401.6466979980469, 'learning_rate': 1.4677772561027121e-05, 'epoch': 1.1}\n",
      "{'loss': 3.5346, 'grad_norm': 359.5524597167969, 'learning_rate': 1.4647231720437687e-05, 'epoch': 1.1}\n",
      "{'loss': 3.7521, 'grad_norm': 415.85772705078125, 'learning_rate': 1.4616635492463775e-05, 'epoch': 1.11}\n",
      "{'loss': 3.6696, 'grad_norm': 300.148681640625, 'learning_rate': 1.4585984241762268e-05, 'epoch': 1.11}\n",
      "{'loss': 3.5011, 'grad_norm': 359.0833435058594, 'learning_rate': 1.4555278333645833e-05, 'epoch': 1.11}\n",
      "{'loss': 3.6281, 'grad_norm': 371.59014892578125, 'learning_rate': 1.4524518134078565e-05, 'epoch': 1.12}\n",
      "{'loss': 3.7777, 'grad_norm': 329.5606994628906, 'learning_rate': 1.4493704009671614e-05, 'epoch': 1.12}\n",
      "{'loss': 3.3799, 'grad_norm': 408.97021484375, 'learning_rate': 1.446283632767884e-05, 'epoch': 1.12}\n",
      "{'loss': 3.5102, 'grad_norm': 318.2907409667969, 'learning_rate': 1.4431915455992416e-05, 'epoch': 1.12}\n",
      "{'loss': 3.5127, 'grad_norm': 336.3554382324219, 'learning_rate': 1.440094176313844e-05, 'epoch': 1.13}\n",
      "{'loss': 3.6893, 'grad_norm': 328.5307312011719, 'learning_rate': 1.4369915618272568e-05, 'epoch': 1.13}\n",
      "{'loss': 3.3297, 'grad_norm': 364.7291259765625, 'learning_rate': 1.4338837391175582e-05, 'epoch': 1.13}\n",
      "{'loss': 3.113, 'grad_norm': 326.18853759765625, 'learning_rate': 1.4307707452249013e-05, 'epoch': 1.14}\n",
      "{'loss': 3.8404, 'grad_norm': 339.25146484375, 'learning_rate': 1.42765261725107e-05, 'epoch': 1.14}\n",
      "{'loss': 3.3531, 'grad_norm': 381.1528625488281, 'learning_rate': 1.424529392359039e-05, 'epoch': 1.14}\n",
      "{'loss': 3.365, 'grad_norm': 387.93890380859375, 'learning_rate': 1.4214011077725293e-05, 'epoch': 1.15}\n",
      "{'loss': 2.7689, 'grad_norm': 402.8507385253906, 'learning_rate': 1.4182678007755653e-05, 'epoch': 1.15}\n",
      "{'loss': 2.7849, 'grad_norm': 300.97906494140625, 'learning_rate': 1.4151295087120307e-05, 'epoch': 1.15}\n",
      "{'loss': 3.1254, 'grad_norm': 563.7349853515625, 'learning_rate': 1.4119862689852224e-05, 'epoch': 1.16}\n",
      "{'loss': 3.1857, 'grad_norm': 563.33203125, 'learning_rate': 1.4088381190574051e-05, 'epoch': 1.16}\n",
      "{'loss': 3.3636, 'grad_norm': 257.908203125, 'learning_rate': 1.4056850964493668e-05, 'epoch': 1.16}\n",
      "{'loss': 3.6198, 'grad_norm': 203.20767211914062, 'learning_rate': 1.4025272387399676e-05, 'epoch': 1.17}\n",
      "{'loss': 3.4456, 'grad_norm': 221.9078826904297, 'learning_rate': 1.3993645835656955e-05, 'epoch': 1.17}\n",
      "{'loss': 3.3665, 'grad_norm': 222.35653686523438, 'learning_rate': 1.3961971686202163e-05, 'epoch': 1.17}\n",
      "{'loss': 3.535, 'grad_norm': 229.93624877929688, 'learning_rate': 1.3930250316539237e-05, 'epoch': 1.18}\n",
      "{'loss': 3.5742, 'grad_norm': 268.2081298828125, 'learning_rate': 1.3898482104734909e-05, 'epoch': 1.18}\n",
      "{'loss': 3.7538, 'grad_norm': 232.27975463867188, 'learning_rate': 1.3866667429414188e-05, 'epoch': 1.18}\n",
      "{'loss': 4.1067, 'grad_norm': 272.919189453125, 'learning_rate': 1.383480666975586e-05, 'epoch': 1.19}\n",
      "{'loss': 4.0732, 'grad_norm': 317.59637451171875, 'learning_rate': 1.3802900205487948e-05, 'epoch': 1.19}\n",
      "{'loss': 4.0229, 'grad_norm': 265.5113220214844, 'learning_rate': 1.3770948416883205e-05, 'epoch': 1.19}\n",
      "{'loss': 3.909, 'grad_norm': 242.55148315429688, 'learning_rate': 1.3738951684754585e-05, 'epoch': 1.2}\n",
      "{'loss': 3.6926, 'grad_norm': 274.654541015625, 'learning_rate': 1.3706910390450679e-05, 'epoch': 1.2}\n",
      "{'loss': 3.8036, 'grad_norm': 321.1575622558594, 'learning_rate': 1.3674824915851193e-05, 'epoch': 1.2}\n",
      "{'loss': 3.8652, 'grad_norm': 432.94940185546875, 'learning_rate': 1.3642695643362398e-05, 'epoch': 1.2}\n",
      "{'loss': 3.8427, 'grad_norm': 331.1347961425781, 'learning_rate': 1.3610522955912551e-05, 'epoch': 1.21}\n",
      "{'loss': 3.6117, 'grad_norm': 259.0882873535156, 'learning_rate': 1.3578307236947348e-05, 'epoch': 1.21}\n",
      "{'loss': 3.9156, 'grad_norm': 263.1017761230469, 'learning_rate': 1.3546048870425356e-05, 'epoch': 1.21}\n",
      "{'loss': 3.6381, 'grad_norm': 338.0381164550781, 'learning_rate': 1.3513748240813429e-05, 'epoch': 1.22}\n",
      "{'loss': 3.6697, 'grad_norm': 335.01824951171875, 'learning_rate': 1.3481405733082118e-05, 'epoch': 1.22}\n",
      "{'loss': 3.7028, 'grad_norm': 345.1737060546875, 'learning_rate': 1.3449021732701106e-05, 'epoch': 1.22}\n",
      "{'loss': 3.7279, 'grad_norm': 293.1334228515625, 'learning_rate': 1.3416596625634595e-05, 'epoch': 1.23}\n",
      "{'loss': 3.9064, 'grad_norm': 314.87420654296875, 'learning_rate': 1.3384130798336705e-05, 'epoch': 1.23}\n",
      "{'loss': 4.0129, 'grad_norm': 334.58892822265625, 'learning_rate': 1.3351624637746885e-05, 'epoch': 1.23}\n",
      "{'loss': 3.6124, 'grad_norm': 358.75762939453125, 'learning_rate': 1.3319078531285286e-05, 'epoch': 1.24}\n",
      "{'loss': 3.5568, 'grad_norm': 290.2950134277344, 'learning_rate': 1.3286492866848143e-05, 'epoch': 1.24}\n",
      "{'loss': 3.7259, 'grad_norm': 348.2655029296875, 'learning_rate': 1.3253868032803171e-05, 'epoch': 1.24}\n",
      "{'loss': 3.4806, 'grad_norm': 311.4715270996094, 'learning_rate': 1.3221204417984907e-05, 'epoch': 1.25}\n",
      "{'loss': 3.6536, 'grad_norm': 284.98504638671875, 'learning_rate': 1.3188502411690101e-05, 'epoch': 1.25}\n",
      "{'loss': 3.6713, 'grad_norm': 274.98016357421875, 'learning_rate': 1.3155762403673065e-05, 'epoch': 1.25}\n",
      "{'loss': 3.7235, 'grad_norm': 284.0035400390625, 'learning_rate': 1.3122984784141021e-05, 'epoch': 1.26}\n",
      "{'loss': 3.2992, 'grad_norm': 283.0602722167969, 'learning_rate': 1.3090169943749475e-05, 'epoch': 1.26}\n",
      "{'loss': 3.6191, 'grad_norm': 284.5191345214844, 'learning_rate': 1.3057318273597531e-05, 'epoch': 1.26}\n",
      "{'loss': 3.2969, 'grad_norm': 302.2982177734375, 'learning_rate': 1.3024430165223245e-05, 'epoch': 1.27}\n",
      "{'loss': 3.4499, 'grad_norm': 241.2523651123047, 'learning_rate': 1.2991506010598965e-05, 'epoch': 1.27}\n",
      "{'loss': 3.3512, 'grad_norm': 246.1725311279297, 'learning_rate': 1.2958546202126638e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1214, 'grad_norm': 280.9154052734375, 'learning_rate': 1.2925551132633164e-05, 'epoch': 1.28}\n",
      "{'loss': 3.6704, 'grad_norm': 261.22088623046875, 'learning_rate': 1.2892521195365679e-05, 'epoch': 1.28}\n",
      "{'loss': 3.8046, 'grad_norm': 288.2083740234375, 'learning_rate': 1.2859456783986892e-05, 'epoch': 1.28}\n",
      "{'loss': 3.4525, 'grad_norm': 312.58502197265625, 'learning_rate': 1.2826358292570398e-05, 'epoch': 1.28}\n",
      "{'loss': 3.3964, 'grad_norm': 362.14874267578125, 'learning_rate': 1.2793226115595951e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1308, 'grad_norm': 238.31082153320312, 'learning_rate': 1.2760060647944794e-05, 'epoch': 1.29}\n",
      "{'loss': 3.5573, 'grad_norm': 302.9606018066406, 'learning_rate': 1.2726862284894939e-05, 'epoch': 1.29}\n",
      "{'loss': 3.8729, 'grad_norm': 251.40792846679688, 'learning_rate': 1.2693631422116455e-05, 'epoch': 1.3}\n",
      "{'loss': 2.8582, 'grad_norm': 328.90521240234375, 'learning_rate': 1.2660368455666752e-05, 'epoch': 1.3}\n",
      "{'loss': 3.3253, 'grad_norm': 279.33770751953125, 'learning_rate': 1.262707378198587e-05, 'epoch': 1.3}\n",
      "{'loss': 3.1682, 'grad_norm': 359.2819519042969, 'learning_rate': 1.2593747797891743e-05, 'epoch': 1.31}\n",
      "{'loss': 2.8662, 'grad_norm': 265.2848205566406, 'learning_rate': 1.2560390900575472e-05, 'epoch': 1.31}\n",
      "{'loss': 3.5508, 'grad_norm': 459.4678955078125, 'learning_rate': 1.2527003487596598e-05, 'epoch': 1.31}\n",
      "{'loss': 2.9489, 'grad_norm': 322.06622314453125, 'learning_rate': 1.2493585956878354e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1273, 'grad_norm': 290.7131042480469, 'learning_rate': 1.2460138706702929e-05, 'epoch': 1.32}\n",
      "{'loss': 3.4924, 'grad_norm': 175.58670043945312, 'learning_rate': 1.242666213570672e-05, 'epoch': 1.32}\n",
      "{'loss': 3.4871, 'grad_norm': 190.61854553222656, 'learning_rate': 1.2393156642875579e-05, 'epoch': 1.33}\n",
      "{'loss': 3.517, 'grad_norm': 171.37786865234375, 'learning_rate': 1.2359622627540059e-05, 'epoch': 1.33}\n",
      "{'loss': 3.3959, 'grad_norm': 168.6323699951172, 'learning_rate': 1.2326060489370655e-05, 'epoch': 1.33}\n",
      "{'loss': 3.4086, 'grad_norm': 231.1112060546875, 'learning_rate': 1.229247062837304e-05, 'epoch': 1.34}\n",
      "{'loss': 3.437, 'grad_norm': 216.8575897216797, 'learning_rate': 1.2258853444883297e-05, 'epoch': 1.34}\n",
      "{'loss': 3.5516, 'grad_norm': 228.28941345214844, 'learning_rate': 1.2225209339563144e-05, 'epoch': 1.34}\n",
      "{'loss': 3.6696, 'grad_norm': 238.57398986816406, 'learning_rate': 1.219153871339518e-05, 'epoch': 1.35}\n",
      "{'loss': 3.9555, 'grad_norm': 308.90185546875, 'learning_rate': 1.2157841967678064e-05, 'epoch': 1.35}\n",
      "{'loss': 3.6502, 'grad_norm': 299.10748291015625, 'learning_rate': 1.2124119504021776e-05, 'epoch': 1.35}\n",
      "{'loss': 3.7835, 'grad_norm': 265.5299377441406, 'learning_rate': 1.2090371724342804e-05, 'epoch': 1.36}\n",
      "{'loss': 4.0837, 'grad_norm': 339.8041076660156, 'learning_rate': 1.2056599030859367e-05, 'epoch': 1.36}\n",
      "{'loss': 3.6538, 'grad_norm': 319.4033508300781, 'learning_rate': 1.2022801826086609e-05, 'epoch': 1.36}\n",
      "{'loss': 3.6627, 'grad_norm': 340.22296142578125, 'learning_rate': 1.1988980512831809e-05, 'epoch': 1.36}\n",
      "{'loss': 3.8491, 'grad_norm': 264.0929260253906, 'learning_rate': 1.195513549418959e-05, 'epoch': 1.37}\n",
      "{'loss': 3.4962, 'grad_norm': 265.4762268066406, 'learning_rate': 1.1921267173537085e-05, 'epoch': 1.37}\n",
      "{'loss': 3.3344, 'grad_norm': 288.8684387207031, 'learning_rate': 1.1887375954529167e-05, 'epoch': 1.37}\n",
      "{'loss': 3.8882, 'grad_norm': 247.17770385742188, 'learning_rate': 1.1853462241093614e-05, 'epoch': 1.38}\n",
      "{'loss': 3.5546, 'grad_norm': 305.73248291015625, 'learning_rate': 1.1819526437426298e-05, 'epoch': 1.38}\n",
      "{'loss': 3.5288, 'grad_norm': 290.132080078125, 'learning_rate': 1.1785568947986368e-05, 'epoch': 1.38}\n",
      "{'loss': 3.6445, 'grad_norm': 245.9775390625, 'learning_rate': 1.1751590177491441e-05, 'epoch': 1.39}\n",
      "{'loss': 3.6305, 'grad_norm': 284.5626220703125, 'learning_rate': 1.1717590530912764e-05, 'epoch': 1.39}\n",
      "{'loss': 3.3522, 'grad_norm': 234.50094604492188, 'learning_rate': 1.1683570413470384e-05, 'epoch': 1.39}\n",
      "{'loss': 3.2212, 'grad_norm': 260.24212646484375, 'learning_rate': 1.164953023062835e-05, 'epoch': 1.4}\n",
      "{'loss': 3.4719, 'grad_norm': 326.0618591308594, 'learning_rate': 1.1615470388089836e-05, 'epoch': 1.4}\n",
      "{'loss': 3.7557, 'grad_norm': 277.2721862792969, 'learning_rate': 1.1581391291792336e-05, 'epoch': 1.4}\n",
      "{'loss': 3.4395, 'grad_norm': 298.3232421875, 'learning_rate': 1.1547293347902813e-05, 'epoch': 1.41}\n",
      "{'loss': 3.6284, 'grad_norm': 241.5583953857422, 'learning_rate': 1.151317696281287e-05, 'epoch': 1.41}\n",
      "{'loss': 3.4159, 'grad_norm': 222.34295654296875, 'learning_rate': 1.1479042543133895e-05, 'epoch': 1.41}\n",
      "{'loss': 3.0541, 'grad_norm': 201.26718139648438, 'learning_rate': 1.1444890495692214e-05, 'epoch': 1.42}\n",
      "{'loss': 3.9752, 'grad_norm': 268.0467834472656, 'learning_rate': 1.1410721227524256e-05, 'epoch': 1.42}\n",
      "{'loss': 3.7982, 'grad_norm': 216.8923797607422, 'learning_rate': 1.1376535145871685e-05, 'epoch': 1.42}\n",
      "{'loss': 3.203, 'grad_norm': 249.0296173095703, 'learning_rate': 1.1342332658176556e-05, 'epoch': 1.43}\n",
      "{'loss': 3.6622, 'grad_norm': 219.9296875, 'learning_rate': 1.1308114172076464e-05, 'epoch': 1.43}\n",
      "{'loss': 3.235, 'grad_norm': 241.98097229003906, 'learning_rate': 1.1273880095399667e-05, 'epoch': 1.43}\n",
      "{'loss': 3.3909, 'grad_norm': 246.54782104492188, 'learning_rate': 1.1239630836160246e-05, 'epoch': 1.44}\n",
      "{'loss': 3.1758, 'grad_norm': 253.1094970703125, 'learning_rate': 1.1205366802553231e-05, 'epoch': 1.44}\n",
      "{'loss': 3.2892, 'grad_norm': 322.36737060546875, 'learning_rate': 1.1171088402949739e-05, 'epoch': 1.44}\n",
      "{'loss': 3.5062, 'grad_norm': 305.8359069824219, 'learning_rate': 1.1136796045892102e-05, 'epoch': 1.44}\n",
      "{'loss': 3.6161, 'grad_norm': 256.3968811035156, 'learning_rate': 1.1102490140089009e-05, 'epoch': 1.45}\n",
      "{'loss': 3.4902, 'grad_norm': 293.2936706542969, 'learning_rate': 1.1068171094410618e-05, 'epoch': 1.45}\n",
      "{'loss': 3.2456, 'grad_norm': 227.0699005126953, 'learning_rate': 1.10338393178837e-05, 'epoch': 1.45}\n",
      "{'loss': 3.4629, 'grad_norm': 289.85955810546875, 'learning_rate': 1.0999495219686762e-05, 'epoch': 1.46}\n",
      "{'loss': 3.4357, 'grad_norm': 301.1109619140625, 'learning_rate': 1.0965139209145153e-05, 'epoch': 1.46}\n",
      "{'loss': 3.344, 'grad_norm': 345.1794128417969, 'learning_rate': 1.0930771695726201e-05, 'epoch': 1.46}\n",
      "{'loss': 2.9472, 'grad_norm': 265.6766357421875, 'learning_rate': 1.0896393089034336e-05, 'epoch': 1.47}\n",
      "{'loss': 2.8742, 'grad_norm': 440.6820068359375, 'learning_rate': 1.0862003798806195e-05, 'epoch': 1.47}\n",
      "{'loss': 2.5608, 'grad_norm': 330.95013427734375, 'learning_rate': 1.0827604234905749e-05, 'epoch': 1.47}\n",
      "{'loss': 2.9496, 'grad_norm': 314.1139831542969, 'learning_rate': 1.079319480731941e-05, 'epoch': 1.48}\n",
      "{'loss': 3.3622, 'grad_norm': 391.30499267578125, 'learning_rate': 1.0758775926151155e-05, 'epoch': 1.48}\n",
      "{'loss': 3.4482, 'grad_norm': 190.32562255859375, 'learning_rate': 1.0724348001617626e-05, 'epoch': 1.48}\n",
      "{'loss': 3.5306, 'grad_norm': 178.54342651367188, 'learning_rate': 1.0689911444043249e-05, 'epoch': 1.49}\n",
      "{'loss': 3.3611, 'grad_norm': 154.67904663085938, 'learning_rate': 1.0655466663855349e-05, 'epoch': 1.49}\n",
      "{'loss': 3.3546, 'grad_norm': 177.05740356445312, 'learning_rate': 1.0621014071579241e-05, 'epoch': 1.49}\n",
      "{'loss': 3.4756, 'grad_norm': 197.32528686523438, 'learning_rate': 1.0586554077833346e-05, 'epoch': 1.5}\n",
      "{'loss': 3.5979, 'grad_norm': 255.38365173339844, 'learning_rate': 1.0552087093324314e-05, 'epoch': 1.5}\n",
      "{'loss': 3.4662, 'grad_norm': 235.41165161132812, 'learning_rate': 1.0517613528842096e-05, 'epoch': 1.5}\n",
      "{'loss': 3.6846, 'grad_norm': 282.8819580078125, 'learning_rate': 1.0483133795255072e-05, 'epoch': 1.51}\n",
      "{'loss': 3.9239, 'grad_norm': 244.98492431640625, 'learning_rate': 1.044864830350515e-05, 'epoch': 1.51}\n",
      "{'loss': 3.5119, 'grad_norm': 222.9047088623047, 'learning_rate': 1.0414157464602866e-05, 'epoch': 1.51}\n",
      "{'loss': 3.9033, 'grad_norm': 276.60308837890625, 'learning_rate': 1.0379661689622477e-05, 'epoch': 1.52}\n",
      "{'loss': 3.9359, 'grad_norm': 269.7746887207031, 'learning_rate': 1.0345161389697083e-05, 'epoch': 1.52}\n",
      "{'loss': 3.9498, 'grad_norm': 312.48809814453125, 'learning_rate': 1.0310656976013704e-05, 'epoch': 1.52}\n",
      "{'loss': 3.864, 'grad_norm': 280.66204833984375, 'learning_rate': 1.027614885980839e-05, 'epoch': 1.52}\n",
      "{'loss': 3.5826, 'grad_norm': 280.4068298339844, 'learning_rate': 1.0241637452361323e-05, 'epoch': 1.53}\n",
      "{'loss': 3.5204, 'grad_norm': 322.6304626464844, 'learning_rate': 1.0207123164991912e-05, 'epoch': 1.53}\n",
      "{'loss': 3.796, 'grad_norm': 217.24777221679688, 'learning_rate': 1.0172606409053887e-05, 'epoch': 1.53}\n",
      "{'loss': 3.9354, 'grad_norm': 243.44241333007812, 'learning_rate': 1.0138087595930394e-05, 'epoch': 1.54}\n",
      "{'loss': 3.713, 'grad_norm': 237.39935302734375, 'learning_rate': 1.0103567137029111e-05, 'epoch': 1.54}\n",
      "{'loss': 3.46, 'grad_norm': 240.3647003173828, 'learning_rate': 1.0069045443777318e-05, 'epoch': 1.54}\n",
      "{'loss': 3.6392, 'grad_norm': 233.58615112304688, 'learning_rate': 1.0034522927617014e-05, 'epoch': 1.55}\n",
      "{'loss': 3.6715, 'grad_norm': 219.167724609375, 'learning_rate': 1e-05, 'epoch': 1.55}\n",
      "{'loss': 3.0664, 'grad_norm': 254.32472229003906, 'learning_rate': 9.965477072382989e-06, 'epoch': 1.55}\n",
      "{'loss': 3.4204, 'grad_norm': 292.25128173828125, 'learning_rate': 9.930954556222683e-06, 'epoch': 1.56}\n",
      "{'loss': 3.6312, 'grad_norm': 226.28054809570312, 'learning_rate': 9.896432862970892e-06, 'epoch': 1.56}\n",
      "{'loss': 3.3208, 'grad_norm': 264.69158935546875, 'learning_rate': 9.861912404069608e-06, 'epoch': 1.56}\n",
      "{'loss': 3.733, 'grad_norm': 298.3951110839844, 'learning_rate': 9.827393590946116e-06, 'epoch': 1.57}\n",
      "{'loss': 3.2317, 'grad_norm': 280.9689636230469, 'learning_rate': 9.79287683500809e-06, 'epoch': 1.57}\n",
      "{'loss': 3.8177, 'grad_norm': 295.49530029296875, 'learning_rate': 9.75836254763868e-06, 'epoch': 1.57}\n",
      "{'loss': 3.6656, 'grad_norm': 311.06549072265625, 'learning_rate': 9.723851140191613e-06, 'epoch': 1.58}\n",
      "{'loss': 3.6574, 'grad_norm': 265.635009765625, 'learning_rate': 9.689343023986303e-06, 'epoch': 1.58}\n",
      "{'loss': 3.4428, 'grad_norm': 332.8334045410156, 'learning_rate': 9.654838610302922e-06, 'epoch': 1.58}\n",
      "{'loss': 3.8277, 'grad_norm': 297.5727844238281, 'learning_rate': 9.620338310377526e-06, 'epoch': 1.59}\n",
      "{'loss': 3.3208, 'grad_norm': 298.1198425292969, 'learning_rate': 9.58584253539714e-06, 'epoch': 1.59}\n",
      "{'loss': 2.7613, 'grad_norm': 270.1164245605469, 'learning_rate': 9.551351696494854e-06, 'epoch': 1.59}\n",
      "{'loss': 3.4841, 'grad_norm': 262.400634765625, 'learning_rate': 9.516866204744932e-06, 'epoch': 1.6}\n",
      "{'loss': 3.7522, 'grad_norm': 363.6531982421875, 'learning_rate': 9.482386471157905e-06, 'epoch': 1.6}\n",
      "{'loss': 3.5577, 'grad_norm': 328.5450744628906, 'learning_rate': 9.447912906675687e-06, 'epoch': 1.6}\n",
      "{'loss': 3.3486, 'grad_norm': 316.1466369628906, 'learning_rate': 9.413445922166654e-06, 'epoch': 1.6}\n",
      "{'loss': 2.749, 'grad_norm': 295.333251953125, 'learning_rate': 9.378985928420764e-06, 'epoch': 1.61}\n",
      "{'loss': 3.3079, 'grad_norm': 288.597412109375, 'learning_rate': 9.344533336144653e-06, 'epoch': 1.61}\n",
      "{'loss': 2.8997, 'grad_norm': 251.40469360351562, 'learning_rate': 9.310088555956751e-06, 'epoch': 1.61}\n",
      "{'loss': 2.7704, 'grad_norm': 357.1193542480469, 'learning_rate': 9.275651998382377e-06, 'epoch': 1.62}\n",
      "{'loss': 2.9499, 'grad_norm': 395.808349609375, 'learning_rate': 9.241224073848848e-06, 'epoch': 1.62}\n",
      "{'loss': 2.9555, 'grad_norm': 272.1222839355469, 'learning_rate': 9.206805192680592e-06, 'epoch': 1.62}\n",
      "{'loss': 3.4129, 'grad_norm': 314.55853271484375, 'learning_rate': 9.172395765094255e-06, 'epoch': 1.63}\n",
      "{'loss': 2.8552, 'grad_norm': 371.7373046875, 'learning_rate': 9.137996201193807e-06, 'epoch': 1.63}\n",
      "{'loss': 3.0403, 'grad_norm': 388.0318603515625, 'learning_rate': 9.103606910965666e-06, 'epoch': 1.63}\n",
      "{'loss': 2.9417, 'grad_norm': 461.9571533203125, 'learning_rate': 9.069228304273802e-06, 'epoch': 1.64}\n",
      "{'loss': 3.3882, 'grad_norm': 548.137451171875, 'learning_rate': 9.034860790854848e-06, 'epoch': 1.64}\n",
      "{'loss': 3.2538, 'grad_norm': 156.44183349609375, 'learning_rate': 9.00050478031324e-06, 'epoch': 1.64}\n",
      "{'loss': 3.2421, 'grad_norm': 181.922607421875, 'learning_rate': 8.966160682116301e-06, 'epoch': 1.65}\n",
      "{'loss': 3.3424, 'grad_norm': 162.79000854492188, 'learning_rate': 8.931828905589385e-06, 'epoch': 1.65}\n",
      "{'loss': 3.2627, 'grad_norm': 192.02467346191406, 'learning_rate': 8.897509859910996e-06, 'epoch': 1.65}\n",
      "{'loss': 3.3496, 'grad_norm': 236.33470153808594, 'learning_rate': 8.863203954107902e-06, 'epoch': 1.66}\n",
      "{'loss': 3.4396, 'grad_norm': 349.2449645996094, 'learning_rate': 8.828911597050263e-06, 'epoch': 1.66}\n",
      "{'loss': 3.6792, 'grad_norm': 303.7754821777344, 'learning_rate': 8.79463319744677e-06, 'epoch': 1.66}\n",
      "{'loss': 3.618, 'grad_norm': 250.11863708496094, 'learning_rate': 8.760369163839759e-06, 'epoch': 1.67}\n",
      "{'loss': 3.8887, 'grad_norm': 281.2517395019531, 'learning_rate': 8.726119904600337e-06, 'epoch': 1.67}\n",
      "{'loss': 4.0339, 'grad_norm': 319.9852294921875, 'learning_rate': 8.691885827923541e-06, 'epoch': 1.67}\n",
      "{'loss': 3.8021, 'grad_norm': 251.4886016845703, 'learning_rate': 8.657667341823449e-06, 'epoch': 1.68}\n",
      "{'loss': 3.5653, 'grad_norm': 247.47442626953125, 'learning_rate': 8.62346485412832e-06, 'epoch': 1.68}\n",
      "{'loss': 3.9279, 'grad_norm': 274.34197998046875, 'learning_rate': 8.58927877247575e-06, 'epoch': 1.68}\n",
      "{'loss': 3.8884, 'grad_norm': 360.19012451171875, 'learning_rate': 8.55510950430779e-06, 'epoch': 1.68}\n",
      "{'loss': 3.7888, 'grad_norm': 285.7602844238281, 'learning_rate': 8.520957456866107e-06, 'epoch': 1.69}\n",
      "{'loss': 3.6571, 'grad_norm': 313.18963623046875, 'learning_rate': 8.48682303718713e-06, 'epoch': 1.69}\n",
      "{'loss': 3.6042, 'grad_norm': 292.20709228515625, 'learning_rate': 8.452706652097187e-06, 'epoch': 1.69}\n",
      "{'loss': 3.179, 'grad_norm': 238.78756713867188, 'learning_rate': 8.418608708207667e-06, 'epoch': 1.7}\n",
      "{'loss': 3.4542, 'grad_norm': 248.16807556152344, 'learning_rate': 8.384529611910164e-06, 'epoch': 1.7}\n",
      "{'loss': 3.2384, 'grad_norm': 210.8018798828125, 'learning_rate': 8.35046976937165e-06, 'epoch': 1.7}\n",
      "{'loss': 3.369, 'grad_norm': 206.96884155273438, 'learning_rate': 8.316429586529616e-06, 'epoch': 1.71}\n",
      "{'loss': 3.1661, 'grad_norm': 308.1412353515625, 'learning_rate': 8.28240946908724e-06, 'epoch': 1.71}\n",
      "{'loss': 3.3321, 'grad_norm': 273.1292419433594, 'learning_rate': 8.24840982250856e-06, 'epoch': 1.71}\n",
      "{'loss': 3.7815, 'grad_norm': 246.84054565429688, 'learning_rate': 8.214431052013636e-06, 'epoch': 1.72}\n",
      "{'loss': 3.8752, 'grad_norm': 227.55712890625, 'learning_rate': 8.180473562573705e-06, 'epoch': 1.72}\n",
      "{'loss': 3.4437, 'grad_norm': 234.55914306640625, 'learning_rate': 8.146537758906388e-06, 'epoch': 1.72}\n",
      "{'loss': 3.16, 'grad_norm': 227.4876251220703, 'learning_rate': 8.112624045470834e-06, 'epoch': 1.73}\n",
      "{'loss': 3.1117, 'grad_norm': 235.3656463623047, 'learning_rate': 8.078732826462917e-06, 'epoch': 1.73}\n",
      "{'loss': 3.4653, 'grad_norm': 336.0472106933594, 'learning_rate': 8.044864505810415e-06, 'epoch': 1.73}\n",
      "{'loss': 3.3033, 'grad_norm': 235.77066040039062, 'learning_rate': 8.011019487168193e-06, 'epoch': 1.74}\n",
      "{'loss': 3.3932, 'grad_norm': 301.1643371582031, 'learning_rate': 7.977198173913394e-06, 'epoch': 1.74}\n",
      "{'loss': 3.3469, 'grad_norm': 231.7421112060547, 'learning_rate': 7.943400969140635e-06, 'epoch': 1.74}\n",
      "{'loss': 3.4169, 'grad_norm': 197.64918518066406, 'learning_rate': 7.909628275657199e-06, 'epoch': 1.75}\n",
      "{'loss': 3.2868, 'grad_norm': 216.56979370117188, 'learning_rate': 7.875880495978227e-06, 'epoch': 1.75}\n",
      "{'loss': 3.3161, 'grad_norm': 196.31817626953125, 'learning_rate': 7.84215803232194e-06, 'epoch': 1.75}\n",
      "{'loss': 3.5244, 'grad_norm': 198.25759887695312, 'learning_rate': 7.808461286604828e-06, 'epoch': 1.76}\n",
      "{'loss': 3.4322, 'grad_norm': 201.33680725097656, 'learning_rate': 7.774790660436857e-06, 'epoch': 1.76}\n",
      "{'loss': 2.9572, 'grad_norm': 226.71401977539062, 'learning_rate': 7.741146555116708e-06, 'epoch': 1.76}\n",
      "{'loss': 3.1124, 'grad_norm': 212.16433715820312, 'learning_rate': 7.707529371626966e-06, 'epoch': 1.76}\n",
      "{'loss': 3.1107, 'grad_norm': 219.46945190429688, 'learning_rate': 7.67393951062935e-06, 'epoch': 1.77}\n",
      "{'loss': 3.4089, 'grad_norm': 222.55691528320312, 'learning_rate': 7.640377372459944e-06, 'epoch': 1.77}\n",
      "{'loss': 3.4343, 'grad_norm': 177.10040283203125, 'learning_rate': 7.606843357124426e-06, 'epoch': 1.77}\n",
      "{'loss': 3.7145, 'grad_norm': 255.79283142089844, 'learning_rate': 7.573337864293283e-06, 'epoch': 1.78}\n",
      "{'loss': 2.9097, 'grad_norm': 203.22320556640625, 'learning_rate': 7.539861293297073e-06, 'epoch': 1.78}\n",
      "{'loss': 2.9875, 'grad_norm': 242.09384155273438, 'learning_rate': 7.506414043121647e-06, 'epoch': 1.78}\n",
      "{'loss': 3.2757, 'grad_norm': 250.0952606201172, 'learning_rate': 7.472996512403403e-06, 'epoch': 1.79}\n",
      "{'loss': 2.758, 'grad_norm': 199.7403106689453, 'learning_rate': 7.4396090994245295e-06, 'epoch': 1.79}\n",
      "{'loss': 2.3695, 'grad_norm': 296.4782409667969, 'learning_rate': 7.406252202108258e-06, 'epoch': 1.79}\n",
      "{'loss': 3.3033, 'grad_norm': 338.37982177734375, 'learning_rate': 7.372926218014131e-06, 'epoch': 1.8}\n",
      "{'loss': 2.9804, 'grad_norm': 273.59088134765625, 'learning_rate': 7.33963154433325e-06, 'epoch': 1.8}\n",
      "{'loss': 3.3845, 'grad_norm': 169.11671447753906, 'learning_rate': 7.306368577883547e-06, 'epoch': 1.8}\n",
      "{'loss': 3.5195, 'grad_norm': 202.8220977783203, 'learning_rate': 7.273137715105063e-06, 'epoch': 1.81}\n",
      "{'loss': 3.3364, 'grad_norm': 185.3257598876953, 'learning_rate': 7.239939352055208e-06, 'epoch': 1.81}\n",
      "{'loss': 3.4495, 'grad_norm': 153.33270263671875, 'learning_rate': 7.2067738844040516e-06, 'epoch': 1.81}\n",
      "{'loss': 3.4283, 'grad_norm': 165.542724609375, 'learning_rate': 7.173641707429606e-06, 'epoch': 1.82}\n",
      "{'loss': 3.476, 'grad_norm': 183.89883422851562, 'learning_rate': 7.140543216013109e-06, 'epoch': 1.82}\n",
      "{'loss': 3.4228, 'grad_norm': 189.70327758789062, 'learning_rate': 7.107478804634324e-06, 'epoch': 1.82}\n",
      "{'loss': 3.6358, 'grad_norm': 218.91038513183594, 'learning_rate': 7.07444886736684e-06, 'epoch': 1.83}\n",
      "{'loss': 3.7053, 'grad_norm': 212.86280822753906, 'learning_rate': 7.041453797873363e-06, 'epoch': 1.83}\n",
      "{'loss': 3.7793, 'grad_norm': 223.43553161621094, 'learning_rate': 7.008493989401039e-06, 'epoch': 1.83}\n",
      "{'loss': 3.7195, 'grad_norm': 222.88180541992188, 'learning_rate': 6.975569834776757e-06, 'epoch': 1.84}\n",
      "{'loss': 3.5533, 'grad_norm': 200.8367462158203, 'learning_rate': 6.942681726402474e-06, 'epoch': 1.84}\n",
      "{'loss': 3.3269, 'grad_norm': 208.7314453125, 'learning_rate': 6.909830056250527e-06, 'epoch': 1.84}\n",
      "{'loss': 3.584, 'grad_norm': 213.197265625, 'learning_rate': 6.8770152158589806e-06, 'epoch': 1.84}\n",
      "{'loss': 3.5681, 'grad_norm': 255.65769958496094, 'learning_rate': 6.844237596326941e-06, 'epoch': 1.85}\n",
      "{'loss': 3.6036, 'grad_norm': 190.76535034179688, 'learning_rate': 6.811497588309901e-06, 'epoch': 1.85}\n",
      "{'loss': 3.7522, 'grad_norm': 221.70169067382812, 'learning_rate': 6.778795582015096e-06, 'epoch': 1.85}\n",
      "{'loss': 3.83, 'grad_norm': 240.83877563476562, 'learning_rate': 6.746131967196834e-06, 'epoch': 1.86}\n",
      "{'loss': 3.7992, 'grad_norm': 234.40875244140625, 'learning_rate': 6.7135071331518575e-06, 'epoch': 1.86}\n",
      "{'loss': 3.4172, 'grad_norm': 220.48802185058594, 'learning_rate': 6.680921468714718e-06, 'epoch': 1.86}\n",
      "{'loss': 3.6201, 'grad_norm': 249.543212890625, 'learning_rate': 6.648375362253119e-06, 'epoch': 1.87}\n",
      "{'loss': 3.4473, 'grad_norm': 225.89869689941406, 'learning_rate': 6.615869201663296e-06, 'epoch': 1.87}\n",
      "{'loss': 3.4818, 'grad_norm': 217.48434448242188, 'learning_rate': 6.583403374365406e-06, 'epoch': 1.87}\n",
      "{'loss': 3.4065, 'grad_norm': 219.27342224121094, 'learning_rate': 6.550978267298893e-06, 'epoch': 1.88}\n",
      "{'loss': 4.0498, 'grad_norm': 250.91702270507812, 'learning_rate': 6.518594266917883e-06, 'epoch': 1.88}\n",
      "{'loss': 3.5251, 'grad_norm': 201.4399871826172, 'learning_rate': 6.486251759186573e-06, 'epoch': 1.88}\n",
      "{'loss': 3.372, 'grad_norm': 209.4516143798828, 'learning_rate': 6.453951129574644e-06, 'epoch': 1.89}\n",
      "{'loss': 3.3432, 'grad_norm': 267.96270751953125, 'learning_rate': 6.421692763052654e-06, 'epoch': 1.89}\n",
      "{'loss': 3.2954, 'grad_norm': 183.48776245117188, 'learning_rate': 6.3894770440874525e-06, 'epoch': 1.89}\n",
      "{'loss': 3.8446, 'grad_norm': 177.5939178466797, 'learning_rate': 6.357304356637606e-06, 'epoch': 1.9}\n",
      "{'loss': 3.7283, 'grad_norm': 200.08349609375, 'learning_rate': 6.325175084148809e-06, 'epoch': 1.9}\n",
      "{'loss': 3.2606, 'grad_norm': 241.51651000976562, 'learning_rate': 6.293089609549325e-06, 'epoch': 1.9}\n",
      "{'loss': 3.4675, 'grad_norm': 221.7435302734375, 'learning_rate': 6.261048315245419e-06, 'epoch': 1.91}\n",
      "{'loss': 3.7641, 'grad_norm': 216.3920440673828, 'learning_rate': 6.229051583116796e-06, 'epoch': 1.91}\n",
      "{'loss': 3.7511, 'grad_norm': 249.19296264648438, 'learning_rate': 6.197099794512056e-06, 'epoch': 1.91}\n",
      "{'loss': 3.4566, 'grad_norm': 206.0795440673828, 'learning_rate': 6.165193330244144e-06, 'epoch': 1.92}\n",
      "{'loss': 3.3947, 'grad_norm': 183.5465087890625, 'learning_rate': 6.133332570585813e-06, 'epoch': 1.92}\n",
      "{'loss': 3.5857, 'grad_norm': 227.02333068847656, 'learning_rate': 6.101517895265094e-06, 'epoch': 1.92}\n",
      "{'loss': 3.4406, 'grad_norm': 203.08238220214844, 'learning_rate': 6.069749683460765e-06, 'epoch': 1.92}\n",
      "{'loss': 3.6075, 'grad_norm': 216.77874755859375, 'learning_rate': 6.03802831379784e-06, 'epoch': 1.93}\n",
      "{'loss': 3.7669, 'grad_norm': 288.790771484375, 'learning_rate': 6.006354164343047e-06, 'epoch': 1.93}\n",
      "{'loss': 3.3871, 'grad_norm': 179.71865844726562, 'learning_rate': 5.9747276126003265e-06, 'epoch': 1.93}\n",
      "{'loss': 3.5861, 'grad_norm': 224.923095703125, 'learning_rate': 5.943149035506337e-06, 'epoch': 1.94}\n",
      "{'loss': 2.9376, 'grad_norm': 199.3828125, 'learning_rate': 5.911618809425952e-06, 'epoch': 1.94}\n",
      "{'loss': 2.9087, 'grad_norm': 173.73973083496094, 'learning_rate': 5.880137310147782e-06, 'epoch': 1.94}\n",
      "{'loss': 3.1385, 'grad_norm': 167.63311767578125, 'learning_rate': 5.848704912879699e-06, 'epoch': 1.95}\n",
      "{'loss': 2.6682, 'grad_norm': 188.02784729003906, 'learning_rate': 5.8173219922443516e-06, 'epoch': 1.95}\n",
      "{'loss': 3.272, 'grad_norm': 204.3270721435547, 'learning_rate': 5.785988922274711e-06, 'epoch': 1.95}\n",
      "{'loss': 3.2964, 'grad_norm': 294.686767578125, 'learning_rate': 5.754706076409613e-06, 'epoch': 1.96}\n",
      "{'loss': 3.121, 'grad_norm': 282.8363952636719, 'learning_rate': 5.723473827489301e-06, 'epoch': 1.96}\n",
      "{'loss': 3.3243, 'grad_norm': 146.3834991455078, 'learning_rate': 5.692292547750989e-06, 'epoch': 1.96}\n",
      "{'loss': 3.4224, 'grad_norm': 165.16290283203125, 'learning_rate': 5.66116260882442e-06, 'epoch': 1.97}\n",
      "{'loss': 3.8589, 'grad_norm': 170.9505157470703, 'learning_rate': 5.630084381727434e-06, 'epoch': 1.97}\n",
      "{'loss': 3.9381, 'grad_norm': 193.3003387451172, 'learning_rate': 5.599058236861559e-06, 'epoch': 1.97}\n",
      "{'loss': 3.7761, 'grad_norm': 212.75025939941406, 'learning_rate': 5.5680845440075885e-06, 'epoch': 1.98}\n",
      "{'loss': 3.2734, 'grad_norm': 202.0560302734375, 'learning_rate': 5.537163672321161e-06, 'epoch': 1.98}\n",
      "{'loss': 3.2997, 'grad_norm': 196.40744018554688, 'learning_rate': 5.5062959903283855e-06, 'epoch': 1.98}\n",
      "{'loss': 3.4696, 'grad_norm': 173.98974609375, 'learning_rate': 5.475481865921441e-06, 'epoch': 1.99}\n",
      "{'loss': 3.5799, 'grad_norm': 223.36419677734375, 'learning_rate': 5.444721666354169e-06, 'epoch': 1.99}\n",
      "{'loss': 3.4144, 'grad_norm': 262.3838806152344, 'learning_rate': 5.414015758237734e-06, 'epoch': 1.99}\n",
      "{'loss': 3.1706, 'grad_norm': 244.71107482910156, 'learning_rate': 5.3833645075362295e-06, 'epoch': 2.0}\n",
      "{'loss': 3.1506, 'grad_norm': 221.81089782714844, 'learning_rate': 5.352768279562315e-06, 'epoch': 2.0}\n",
      "{'loss': 3.6307, 'grad_norm': 396.79071044921875, 'learning_rate': 5.32222743897288e-06, 'epoch': 2.0}\n",
      "{'loss': 3.2006, 'grad_norm': 131.85186767578125, 'learning_rate': 5.2917423497646834e-06, 'epoch': 2.0}\n",
      "{'loss': 3.2692, 'grad_norm': 130.96817016601562, 'learning_rate': 5.2613133752700145e-06, 'epoch': 2.01}\n",
      "{'loss': 3.427, 'grad_norm': 127.5953369140625, 'learning_rate': 5.230940878152371e-06, 'epoch': 2.01}\n",
      "{'loss': 3.2401, 'grad_norm': 120.60400390625, 'learning_rate': 5.200625220402139e-06, 'epoch': 2.01}\n",
      "{'loss': 3.3104, 'grad_norm': 133.56692504882812, 'learning_rate': 5.1703667633322575e-06, 'epoch': 2.02}\n",
      "{'loss': 3.468, 'grad_norm': 154.7547607421875, 'learning_rate': 5.14016586757394e-06, 'epoch': 2.02}\n",
      "{'loss': 3.4802, 'grad_norm': 143.68907165527344, 'learning_rate': 5.110022893072361e-06, 'epoch': 2.02}\n",
      "{'loss': 3.5098, 'grad_norm': 155.451904296875, 'learning_rate': 5.079938199082363e-06, 'epoch': 2.03}\n",
      "{'loss': 3.7608, 'grad_norm': 178.9055633544922, 'learning_rate': 5.049912144164186e-06, 'epoch': 2.03}\n",
      "{'loss': 3.7038, 'grad_norm': 182.4522247314453, 'learning_rate': 5.019945086179192e-06, 'epoch': 2.03}\n",
      "{'loss': 3.5626, 'grad_norm': 171.32090759277344, 'learning_rate': 4.9900373822855805e-06, 'epoch': 2.04}\n",
      "{'loss': 3.6174, 'grad_norm': 187.68310546875, 'learning_rate': 4.960189388934163e-06, 'epoch': 2.04}\n",
      "{'loss': 3.7614, 'grad_norm': 212.96987915039062, 'learning_rate': 4.930401461864099e-06, 'epoch': 2.04}\n",
      "{'loss': 3.8492, 'grad_norm': 206.99264526367188, 'learning_rate': 4.900673956098644e-06, 'epoch': 2.04}\n",
      "{'loss': 3.4679, 'grad_norm': 178.2736358642578, 'learning_rate': 4.87100722594094e-06, 'epoch': 2.05}\n",
      "{'loss': 3.537, 'grad_norm': 193.28594970703125, 'learning_rate': 4.841401624969782e-06, 'epoch': 2.05}\n",
      "{'loss': 3.315, 'grad_norm': 182.92880249023438, 'learning_rate': 4.811857506035407e-06, 'epoch': 2.05}\n",
      "{'loss': 3.4659, 'grad_norm': 190.2095947265625, 'learning_rate': 4.7823752212552855e-06, 'epoch': 2.06}\n",
      "{'loss': 3.6656, 'grad_norm': 154.00662231445312, 'learning_rate': 4.75295512200992e-06, 'epoch': 2.06}\n",
      "{'loss': 3.1674, 'grad_norm': 167.06988525390625, 'learning_rate': 4.7235975589386715e-06, 'epoch': 2.06}\n",
      "{'loss': 3.6486, 'grad_norm': 199.1947479248047, 'learning_rate': 4.694302881935574e-06, 'epoch': 2.07}\n",
      "{'loss': 3.7425, 'grad_norm': 204.98707580566406, 'learning_rate': 4.66507144014515e-06, 'epoch': 2.07}\n",
      "{'loss': 3.3951, 'grad_norm': 211.66046142578125, 'learning_rate': 4.635903581958276e-06, 'epoch': 2.07}\n",
      "{'loss': 3.5653, 'grad_norm': 223.82276916503906, 'learning_rate': 4.606799655008009e-06, 'epoch': 2.08}\n",
      "{'loss': 3.3953, 'grad_norm': 211.33358764648438, 'learning_rate': 4.5777600061654505e-06, 'epoch': 2.08}\n",
      "{'loss': 3.3478, 'grad_norm': 208.68157958984375, 'learning_rate': 4.5487849815356145e-06, 'epoch': 2.08}\n",
      "{'loss': 3.2597, 'grad_norm': 200.57821655273438, 'learning_rate': 4.519874926453303e-06, 'epoch': 2.09}\n",
      "{'loss': 3.3018, 'grad_norm': 179.1595001220703, 'learning_rate': 4.491030185478976e-06, 'epoch': 2.09}\n",
      "{'loss': 3.5413, 'grad_norm': 203.1044921875, 'learning_rate': 4.462251102394669e-06, 'epoch': 2.09}\n",
      "{'loss': 3.2256, 'grad_norm': 230.39097595214844, 'learning_rate': 4.433538020199882e-06, 'epoch': 2.1}\n",
      "{'loss': 3.2182, 'grad_norm': 217.7229766845703, 'learning_rate': 4.404891281107482e-06, 'epoch': 2.1}\n",
      "{'loss': 3.317, 'grad_norm': 192.1566925048828, 'learning_rate': 4.3763112265396445e-06, 'epoch': 2.1}\n",
      "{'loss': 3.3684, 'grad_norm': 197.2235870361328, 'learning_rate': 4.347798197123777e-06, 'epoch': 2.11}\n",
      "{'loss': 3.2182, 'grad_norm': 211.42811584472656, 'learning_rate': 4.319352532688444e-06, 'epoch': 2.11}\n",
      "{'loss': 3.3256, 'grad_norm': 210.90611267089844, 'learning_rate': 4.290974572259342e-06, 'epoch': 2.11}\n",
      "{'loss': 3.2452, 'grad_norm': 224.13253784179688, 'learning_rate': 4.262664654055247e-06, 'epoch': 2.12}\n",
      "{'loss': 2.8581, 'grad_norm': 163.63319396972656, 'learning_rate': 4.234423115483971e-06, 'epoch': 2.12}\n",
      "{'loss': 3.5409, 'grad_norm': 206.8763427734375, 'learning_rate': 4.206250293138366e-06, 'epoch': 2.12}\n",
      "{'loss': 3.2685, 'grad_norm': 247.46778869628906, 'learning_rate': 4.178146522792296e-06, 'epoch': 2.12}\n",
      "{'loss': 3.2282, 'grad_norm': 233.4980926513672, 'learning_rate': 4.15011213939663e-06, 'epoch': 2.13}\n",
      "{'loss': 3.325, 'grad_norm': 285.1667785644531, 'learning_rate': 4.12214747707527e-06, 'epoch': 2.13}\n",
      "{'loss': 2.89, 'grad_norm': 186.780029296875, 'learning_rate': 4.094252869121153e-06, 'epoch': 2.13}\n",
      "{'loss': 3.4228, 'grad_norm': 233.66085815429688, 'learning_rate': 4.066428647992275e-06, 'epoch': 2.14}\n",
      "{'loss': 3.2915, 'grad_norm': 324.1951599121094, 'learning_rate': 4.038675145307747e-06, 'epoch': 2.14}\n",
      "{'loss': 2.7073, 'grad_norm': 262.63299560546875, 'learning_rate': 4.010992691843829e-06, 'epoch': 2.14}\n",
      "{'loss': 3.0848, 'grad_norm': 215.17791748046875, 'learning_rate': 3.98338161752999e-06, 'epoch': 2.15}\n",
      "{'loss': 2.9155, 'grad_norm': 224.71377563476562, 'learning_rate': 3.955842251444978e-06, 'epoch': 2.15}\n",
      "{'loss': 2.3369, 'grad_norm': 159.93724060058594, 'learning_rate': 3.9283749218128885e-06, 'epoch': 2.15}\n",
      "{'loss': 3.039, 'grad_norm': 203.04489135742188, 'learning_rate': 3.900979955999271e-06, 'epoch': 2.16}\n",
      "{'loss': 3.0009, 'grad_norm': 263.4537658691406, 'learning_rate': 3.8736576805072165e-06, 'epoch': 2.16}\n",
      "{'loss': 3.5339, 'grad_norm': 127.3361587524414, 'learning_rate': 3.846408420973456e-06, 'epoch': 2.16}\n",
      "{'loss': 3.0545, 'grad_norm': 136.48434448242188, 'learning_rate': 3.819232502164499e-06, 'epoch': 2.17}\n",
      "{'loss': 3.2704, 'grad_norm': 147.41848754882812, 'learning_rate': 3.792130247972756e-06, 'epoch': 2.17}\n",
      "{'loss': 3.233, 'grad_norm': 122.13704681396484, 'learning_rate': 3.7651019814126656e-06, 'epoch': 2.17}\n",
      "{'loss': 3.2546, 'grad_norm': 122.26329040527344, 'learning_rate': 3.738148024616863e-06, 'epoch': 2.18}\n",
      "{'loss': 3.521, 'grad_norm': 139.8589324951172, 'learning_rate': 3.7112686988323353e-06, 'epoch': 2.18}\n",
      "{'loss': 3.3806, 'grad_norm': 142.46641540527344, 'learning_rate': 3.684464324416578e-06, 'epoch': 2.18}\n",
      "{'loss': 3.5299, 'grad_norm': 148.9589385986328, 'learning_rate': 3.6577352208338015e-06, 'epoch': 2.19}\n",
      "{'loss': 3.4555, 'grad_norm': 175.20645141601562, 'learning_rate': 3.6310817066511106e-06, 'epoch': 2.19}\n",
      "{'loss': 3.5734, 'grad_norm': 150.87619018554688, 'learning_rate': 3.604504099534696e-06, 'epoch': 2.19}\n",
      "{'loss': 3.977, 'grad_norm': 193.1738739013672, 'learning_rate': 3.578002716246074e-06, 'epoch': 2.2}\n",
      "{'loss': 3.5262, 'grad_norm': 182.48597717285156, 'learning_rate': 3.5515778726382967e-06, 'epoch': 2.2}\n",
      "{'loss': 3.4388, 'grad_norm': 159.42596435546875, 'learning_rate': 3.525229883652177e-06, 'epoch': 2.2}\n",
      "{'loss': 3.6092, 'grad_norm': 163.59400939941406, 'learning_rate': 3.4989590633125583e-06, 'epoch': 2.2}\n",
      "{'loss': 3.9703, 'grad_norm': 200.42320251464844, 'learning_rate': 3.4727657247245607e-06, 'epoch': 2.21}\n",
      "{'loss': 3.3816, 'grad_norm': 170.78640747070312, 'learning_rate': 3.446650180069837e-06, 'epoch': 2.21}\n",
      "{'loss': 3.3236, 'grad_norm': 173.16168212890625, 'learning_rate': 3.4206127406028744e-06, 'epoch': 2.21}\n",
      "{'loss': 3.3449, 'grad_norm': 199.6306915283203, 'learning_rate': 3.394653716647277e-06, 'epoch': 2.22}\n",
      "{'loss': 3.5271, 'grad_norm': 216.92727661132812, 'learning_rate': 3.3687734175920505e-06, 'epoch': 2.22}\n",
      "{'loss': 3.7276, 'grad_norm': 154.44821166992188, 'learning_rate': 3.342972151887941e-06, 'epoch': 2.22}\n",
      "{'loss': 3.1326, 'grad_norm': 158.8503875732422, 'learning_rate': 3.317250227043746e-06, 'epoch': 2.23}\n",
      "{'loss': 3.6829, 'grad_norm': 192.60470581054688, 'learning_rate': 3.2916079496226407e-06, 'epoch': 2.23}\n",
      "{'loss': 3.3217, 'grad_norm': 168.75531005859375, 'learning_rate': 3.266045625238539e-06, 'epoch': 2.23}\n",
      "{'loss': 3.2562, 'grad_norm': 179.4359130859375, 'learning_rate': 3.2405635585524566e-06, 'epoch': 2.24}\n",
      "{'loss': 3.5869, 'grad_norm': 174.8531494140625, 'learning_rate': 3.21516205326885e-06, 'epoch': 2.24}\n",
      "{'loss': 3.4875, 'grad_norm': 207.8541717529297, 'learning_rate': 3.1898414121320277e-06, 'epoch': 2.24}\n",
      "{'loss': 3.4433, 'grad_norm': 193.7562255859375, 'learning_rate': 3.1646019369225277e-06, 'epoch': 2.25}\n",
      "{'loss': 3.3045, 'grad_norm': 208.1424102783203, 'learning_rate': 3.1394439284535206e-06, 'epoch': 2.25}\n",
      "{'loss': 3.4028, 'grad_norm': 181.9207305908203, 'learning_rate': 3.114367686567228e-06, 'epoch': 2.25}\n",
      "{'loss': 3.2972, 'grad_norm': 176.0227508544922, 'learning_rate': 3.089373510131354e-06, 'epoch': 2.26}\n",
      "{'loss': 3.2306, 'grad_norm': 162.24691772460938, 'learning_rate': 3.064461697035506e-06, 'epoch': 2.26}\n",
      "{'loss': 3.15, 'grad_norm': 141.5066680908203, 'learning_rate': 3.0396325441876627e-06, 'epoch': 2.26}\n",
      "{'loss': 3.1063, 'grad_norm': 206.45643615722656, 'learning_rate': 3.0148863475106315e-06, 'epoch': 2.27}\n",
      "{'loss': 3.7284, 'grad_norm': 232.1022186279297, 'learning_rate': 2.9902234019385056e-06, 'epoch': 2.27}\n",
      "{'loss': 3.1642, 'grad_norm': 174.40933227539062, 'learning_rate': 2.9656440014131737e-06, 'epoch': 2.27}\n",
      "{'loss': 3.0499, 'grad_norm': 160.45814514160156, 'learning_rate': 2.941148438880803e-06, 'epoch': 2.28}\n",
      "{'loss': 3.1218, 'grad_norm': 165.2965545654297, 'learning_rate': 2.9167370062883403e-06, 'epoch': 2.28}\n",
      "{'loss': 3.266, 'grad_norm': 167.21826171875, 'learning_rate': 2.8924099945800533e-06, 'epoch': 2.28}\n",
      "{'loss': 3.2549, 'grad_norm': 201.20924377441406, 'learning_rate': 2.8681676936940397e-06, 'epoch': 2.28}\n",
      "{'loss': 3.2117, 'grad_norm': 179.62852478027344, 'learning_rate': 2.8440103925587904e-06, 'epoch': 2.29}\n",
      "{'loss': 2.9429, 'grad_norm': 167.54046630859375, 'learning_rate': 2.8199383790897405e-06, 'epoch': 2.29}\n",
      "{'loss': 3.2576, 'grad_norm': 168.68048095703125, 'learning_rate': 2.795951940185827e-06, 'epoch': 2.29}\n",
      "{'loss': 3.1846, 'grad_norm': 165.5995635986328, 'learning_rate': 2.7720513617260857e-06, 'epoch': 2.3}\n",
      "{'loss': 3.0249, 'grad_norm': 160.0858154296875, 'learning_rate': 2.748236928566238e-06, 'epoch': 2.3}\n",
      "{'loss': 3.2935, 'grad_norm': 233.26397705078125, 'learning_rate': 2.7245089245352864e-06, 'epoch': 2.3}\n",
      "{'loss': 2.8074, 'grad_norm': 236.0468292236328, 'learning_rate': 2.700867632432145e-06, 'epoch': 2.31}\n",
      "{'loss': 2.7764, 'grad_norm': 154.14859008789062, 'learning_rate': 2.6773133340222677e-06, 'epoch': 2.31}\n",
      "{'loss': 2.8255, 'grad_norm': 217.98098754882812, 'learning_rate': 2.6538463100342773e-06, 'epoch': 2.31}\n",
      "{'loss': 2.4357, 'grad_norm': 183.581298828125, 'learning_rate': 2.6304668401566334e-06, 'epoch': 2.32}\n",
      "{'loss': 3.1277, 'grad_norm': 293.8442077636719, 'learning_rate': 2.607175203034299e-06, 'epoch': 2.32}\n",
      "{'loss': 3.3843, 'grad_norm': 116.36361694335938, 'learning_rate': 2.5839716762654e-06, 'epoch': 2.32}\n",
      "{'loss': 3.1363, 'grad_norm': 105.07633209228516, 'learning_rate': 2.56085653639795e-06, 'epoch': 2.33}\n",
      "{'loss': 3.1817, 'grad_norm': 117.49729919433594, 'learning_rate': 2.5378300589265258e-06, 'epoch': 2.33}\n",
      "{'loss': 3.1931, 'grad_norm': 113.31333923339844, 'learning_rate': 2.514892518288988e-06, 'epoch': 2.33}\n",
      "{'loss': 3.3473, 'grad_norm': 134.85873413085938, 'learning_rate': 2.4920441878632273e-06, 'epoch': 2.34}\n",
      "{'loss': 3.3296, 'grad_norm': 116.94224548339844, 'learning_rate': 2.469285339963892e-06, 'epoch': 2.34}\n",
      "{'loss': 3.4683, 'grad_norm': 126.49826049804688, 'learning_rate': 2.4466162458391364e-06, 'epoch': 2.34}\n",
      "{'loss': 3.6215, 'grad_norm': 140.31781005859375, 'learning_rate': 2.4240371756674063e-06, 'epoch': 2.35}\n",
      "{'loss': 3.85, 'grad_norm': 137.61363220214844, 'learning_rate': 2.401548398554213e-06, 'epoch': 2.35}\n",
      "{'loss': 3.6708, 'grad_norm': 161.03607177734375, 'learning_rate': 2.379150182528909e-06, 'epoch': 2.35}\n",
      "{'loss': 3.5765, 'grad_norm': 149.43553161621094, 'learning_rate': 2.3568427945415163e-06, 'epoch': 2.36}\n",
      "{'loss': 3.7085, 'grad_norm': 132.87484741210938, 'learning_rate': 2.334626500459539e-06, 'epoch': 2.36}\n",
      "{'loss': 3.751, 'grad_norm': 155.69635009765625, 'learning_rate': 2.3125015650647798e-06, 'epoch': 2.36}\n",
      "{'loss': 3.3384, 'grad_norm': 194.07644653320312, 'learning_rate': 2.290468252050204e-06, 'epoch': 2.36}\n",
      "{'loss': 3.6172, 'grad_norm': 184.99256896972656, 'learning_rate': 2.26852682401679e-06, 'epoch': 2.37}\n",
      "{'loss': 3.9211, 'grad_norm': 173.6001739501953, 'learning_rate': 2.246677542470388e-06, 'epoch': 2.37}\n",
      "{'loss': 3.2253, 'grad_norm': 151.7870635986328, 'learning_rate': 2.224920667818622e-06, 'epoch': 2.37}\n",
      "{'loss': 3.7603, 'grad_norm': 165.38555908203125, 'learning_rate': 2.2032564593677773e-06, 'epoch': 2.38}\n",
      "{'loss': 3.7357, 'grad_norm': 200.7650146484375, 'learning_rate': 2.1816851753197023e-06, 'epoch': 2.38}\n",
      "{'loss': 3.214, 'grad_norm': 168.95140075683594, 'learning_rate': 2.1602070727687463e-06, 'epoch': 2.38}\n",
      "{'loss': 3.4541, 'grad_norm': 162.84158325195312, 'learning_rate': 2.1388224076986872e-06, 'epoch': 2.39}\n",
      "{'loss': 3.3448, 'grad_norm': 188.0185546875, 'learning_rate': 2.117531434979675e-06, 'epoch': 2.39}\n",
      "{'loss': 3.2523, 'grad_norm': 142.59625244140625, 'learning_rate': 2.096334408365207e-06, 'epoch': 2.39}\n",
      "{'loss': 3.5529, 'grad_norm': 170.29469299316406, 'learning_rate': 2.075231580489098e-06, 'epoch': 2.4}\n",
      "{'loss': 3.3214, 'grad_norm': 146.7338104248047, 'learning_rate': 2.0542232028624585e-06, 'epoch': 2.4}\n",
      "{'loss': 3.2148, 'grad_norm': 155.45001220703125, 'learning_rate': 2.033309525870717e-06, 'epoch': 2.4}\n",
      "{'loss': 3.1368, 'grad_norm': 146.64935302734375, 'learning_rate': 2.0124907987706243e-06, 'epoch': 2.41}\n",
      "{'loss': 3.3788, 'grad_norm': 151.6657257080078, 'learning_rate': 1.991767269687278e-06, 'epoch': 2.41}\n",
      "{'loss': 3.4316, 'grad_norm': 182.41322326660156, 'learning_rate': 1.971139185611176e-06, 'epoch': 2.41}\n",
      "{'loss': 3.065, 'grad_norm': 175.66441345214844, 'learning_rate': 1.9506067923952676e-06, 'epoch': 2.42}\n",
      "{'loss': 3.1721, 'grad_norm': 190.6668243408203, 'learning_rate': 1.930170334752025e-06, 'epoch': 2.42}\n",
      "{'loss': 3.61, 'grad_norm': 183.56573486328125, 'learning_rate': 1.9098300562505266e-06, 'epoch': 2.42}\n",
      "{'loss': 3.3583, 'grad_norm': 184.9543914794922, 'learning_rate': 1.8895861993135444e-06, 'epoch': 2.43}\n",
      "{'loss': 3.4348, 'grad_norm': 158.57553100585938, 'learning_rate': 1.8694390052146737e-06, 'epoch': 2.43}\n",
      "{'loss': 3.3023, 'grad_norm': 179.77467346191406, 'learning_rate': 1.8493887140754462e-06, 'epoch': 2.43}\n",
      "{'loss': 3.4036, 'grad_norm': 175.69400024414062, 'learning_rate': 1.8294355648624607e-06, 'epoch': 2.44}\n",
      "{'loss': 3.1708, 'grad_norm': 150.00146484375, 'learning_rate': 1.8095797953845507e-06, 'epoch': 2.44}\n",
      "{'loss': 3.3579, 'grad_norm': 157.2950439453125, 'learning_rate': 1.789821642289945e-06, 'epoch': 2.44}\n",
      "{'loss': 3.5535, 'grad_norm': 158.3314208984375, 'learning_rate': 1.7701613410634367e-06, 'epoch': 2.44}\n",
      "{'loss': 3.3496, 'grad_norm': 144.32896423339844, 'learning_rate': 1.750599126023591e-06, 'epoch': 2.45}\n",
      "{'loss': 3.5064, 'grad_norm': 160.57455444335938, 'learning_rate': 1.731135230319948e-06, 'epoch': 2.45}\n",
      "{'loss': 3.2376, 'grad_norm': 171.5819854736328, 'learning_rate': 1.7117698859302357e-06, 'epoch': 2.45}\n",
      "{'loss': 3.1413, 'grad_norm': 167.36305236816406, 'learning_rate': 1.692503323657617e-06, 'epoch': 2.46}\n",
      "{'loss': 3.345, 'grad_norm': 172.0248565673828, 'learning_rate': 1.6733357731279375e-06, 'epoch': 2.46}\n",
      "{'loss': 2.6949, 'grad_norm': 156.54551696777344, 'learning_rate': 1.6542674627869738e-06, 'epoch': 2.46}\n",
      "{'loss': 2.5906, 'grad_norm': 216.55918884277344, 'learning_rate': 1.6352986198977327e-06, 'epoch': 2.47}\n",
      "{'loss': 3.058, 'grad_norm': 186.5666046142578, 'learning_rate': 1.6164294705377292e-06, 'epoch': 2.47}\n",
      "{'loss': 2.6844, 'grad_norm': 173.0972442626953, 'learning_rate': 1.5976602395962892e-06, 'epoch': 2.47}\n",
      "{'loss': 3.022, 'grad_norm': 203.2823944091797, 'learning_rate': 1.5789911507718824e-06, 'epoch': 2.48}\n",
      "{'loss': 3.2369, 'grad_norm': 190.50613403320312, 'learning_rate': 1.560422426569449e-06, 'epoch': 2.48}\n",
      "{'loss': 2.9706, 'grad_norm': 97.04561614990234, 'learning_rate': 1.5419542882977367e-06, 'epoch': 2.48}\n",
      "{'loss': 3.3749, 'grad_norm': 110.53512573242188, 'learning_rate': 1.523586956066686e-06, 'epoch': 2.49}\n",
      "{'loss': 3.1569, 'grad_norm': 120.66654968261719, 'learning_rate': 1.5053206487847916e-06, 'epoch': 2.49}\n",
      "{'loss': 2.8758, 'grad_norm': 123.24276733398438, 'learning_rate': 1.4871555841564889e-06, 'epoch': 2.49}\n",
      "{'loss': 3.395, 'grad_norm': 113.33482360839844, 'learning_rate': 1.4690919786795766e-06, 'epoch': 2.5}\n",
      "{'loss': 3.4342, 'grad_norm': 105.96446990966797, 'learning_rate': 1.4511300476426227e-06, 'epoch': 2.5}\n",
      "{'loss': 3.6158, 'grad_norm': 152.14431762695312, 'learning_rate': 1.433270005122399e-06, 'epoch': 2.5}\n",
      "{'loss': 3.5227, 'grad_norm': 155.73193359375, 'learning_rate': 1.4155120639813392e-06, 'epoch': 2.51}\n",
      "{'loss': 3.6254, 'grad_norm': 121.5849609375, 'learning_rate': 1.3978564358649926e-06, 'epoch': 2.51}\n",
      "{'loss': 3.5566, 'grad_norm': 166.86656188964844, 'learning_rate': 1.3803033311995072e-06, 'epoch': 2.51}\n",
      "{'loss': 3.6333, 'grad_norm': 159.53436279296875, 'learning_rate': 1.3628529591891181e-06, 'epoch': 2.52}\n",
      "{'loss': 3.4511, 'grad_norm': 147.88922119140625, 'learning_rate': 1.345505527813652e-06, 'epoch': 2.52}\n",
      "{'loss': 3.7027, 'grad_norm': 186.3892822265625, 'learning_rate': 1.3282612438260578e-06, 'epoch': 2.52}\n",
      "{'loss': 3.6681, 'grad_norm': 151.06881713867188, 'learning_rate': 1.311120312749935e-06, 'epoch': 2.52}\n",
      "{'loss': 3.7859, 'grad_norm': 161.1415252685547, 'learning_rate': 1.2940829388770837e-06, 'epoch': 2.53}\n",
      "{'loss': 3.554, 'grad_norm': 134.33993530273438, 'learning_rate': 1.2771493252650723e-06, 'epoch': 2.53}\n",
      "{'loss': 3.606, 'grad_norm': 175.21449279785156, 'learning_rate': 1.2603196737348211e-06, 'epoch': 2.53}\n",
      "{'loss': 3.663, 'grad_norm': 169.64051818847656, 'learning_rate': 1.2435941848681864e-06, 'epoch': 2.54}\n",
      "{'loss': 3.6263, 'grad_norm': 185.65457153320312, 'learning_rate': 1.2269730580055806e-06, 'epoch': 2.54}\n",
      "{'loss': 3.5832, 'grad_norm': 160.08294677734375, 'learning_rate': 1.2104564912435924e-06, 'epoch': 2.54}\n",
      "{'loss': 3.159, 'grad_norm': 186.35614013671875, 'learning_rate': 1.19404468143262e-06, 'epoch': 2.55}\n",
      "{'loss': 3.1728, 'grad_norm': 151.6230010986328, 'learning_rate': 1.1777378241745385e-06, 'epoch': 2.55}\n",
      "{'loss': 3.617, 'grad_norm': 180.9880828857422, 'learning_rate': 1.1615361138203574e-06, 'epoch': 2.55}\n",
      "{'loss': 3.4034, 'grad_norm': 156.1156463623047, 'learning_rate': 1.1454397434679022e-06, 'epoch': 2.56}\n",
      "{'loss': 3.5089, 'grad_norm': 148.11431884765625, 'learning_rate': 1.1294489049595247e-06, 'epoch': 2.56}\n",
      "{'loss': 3.2211, 'grad_norm': 157.55772399902344, 'learning_rate': 1.1135637888798101e-06, 'epoch': 2.56}\n",
      "{'loss': 3.4455, 'grad_norm': 156.17567443847656, 'learning_rate': 1.0977845845533009e-06, 'epoch': 2.57}\n",
      "{'loss': 3.6079, 'grad_norm': 191.2084197998047, 'learning_rate': 1.0821114800422482e-06, 'epoch': 2.57}\n",
      "{'loss': 3.4698, 'grad_norm': 162.01463317871094, 'learning_rate': 1.066544662144371e-06, 'epoch': 2.57}\n",
      "{'loss': 3.503, 'grad_norm': 178.7726593017578, 'learning_rate': 1.0510843163906148e-06, 'epoch': 2.58}\n",
      "{'loss': 3.6305, 'grad_norm': 172.5583038330078, 'learning_rate': 1.0357306270429623e-06, 'epoch': 2.58}\n",
      "{'loss': 3.6004, 'grad_norm': 196.84182739257812, 'learning_rate': 1.020483777092226e-06, 'epoch': 2.58}\n",
      "{'loss': 3.4308, 'grad_norm': 177.5296173095703, 'learning_rate': 1.0053439482558602e-06, 'epoch': 2.59}\n",
      "{'loss': 3.2847, 'grad_norm': 191.3316192626953, 'learning_rate': 9.903113209758098e-07, 'epoch': 2.59}\n",
      "{'loss': 3.1821, 'grad_norm': 140.72897338867188, 'learning_rate': 9.753860744163524e-07, 'epoch': 2.59}\n",
      "{'loss': 3.6034, 'grad_norm': 147.08815002441406, 'learning_rate': 9.605683864619574e-07, 'epoch': 2.6}\n",
      "{'loss': 3.1269, 'grad_norm': 191.79771423339844, 'learning_rate': 9.458584337151811e-07, 'epoch': 2.6}\n",
      "{'loss': 3.4833, 'grad_norm': 174.71055603027344, 'learning_rate': 9.312563914945461e-07, 'epoch': 2.6}\n",
      "{'loss': 3.2741, 'grad_norm': 202.49143981933594, 'learning_rate': 9.167624338324599e-07, 'epoch': 2.6}\n",
      "{'loss': 3.6547, 'grad_norm': 213.37718200683594, 'learning_rate': 9.023767334731426e-07, 'epoch': 2.61}\n",
      "{'loss': 3.0966, 'grad_norm': 135.2401580810547, 'learning_rate': 8.880994618705574e-07, 'epoch': 2.61}\n",
      "{'loss': 3.3413, 'grad_norm': 237.2415008544922, 'learning_rate': 8.739307891863813e-07, 'epoch': 2.61}\n",
      "{'loss': 2.9066, 'grad_norm': 189.54150390625, 'learning_rate': 8.598708842879688e-07, 'epoch': 2.62}\n",
      "{'loss': 2.9362, 'grad_norm': 163.26255798339844, 'learning_rate': 8.459199147463371e-07, 'epoch': 2.62}\n",
      "{'loss': 3.3061, 'grad_norm': 190.08010864257812, 'learning_rate': 8.320780468341761e-07, 'epoch': 2.62}\n",
      "{'loss': 2.5505, 'grad_norm': 161.37803649902344, 'learning_rate': 8.183454455238638e-07, 'epoch': 2.63}\n",
      "{'loss': 3.0163, 'grad_norm': 196.0048828125, 'learning_rate': 8.047222744854943e-07, 'epoch': 2.63}\n",
      "{'loss': 2.7563, 'grad_norm': 193.82730102539062, 'learning_rate': 7.912086960849374e-07, 'epoch': 2.63}\n",
      "{'loss': 3.0909, 'grad_norm': 242.37232971191406, 'learning_rate': 7.778048713818975e-07, 'epoch': 2.64}\n",
      "{'loss': 2.9646, 'grad_norm': 282.48602294921875, 'learning_rate': 7.645109601279921e-07, 'epoch': 2.64}\n",
      "{'loss': 3.0795, 'grad_norm': 107.57283782958984, 'learning_rate': 7.513271207648531e-07, 'epoch': 2.64}\n",
      "{'loss': 3.2516, 'grad_norm': 124.5242919921875, 'learning_rate': 7.382535104222366e-07, 'epoch': 2.65}\n",
      "{'loss': 3.2873, 'grad_norm': 114.07148742675781, 'learning_rate': 7.252902849161436e-07, 'epoch': 2.65}\n",
      "{'loss': 3.2947, 'grad_norm': 127.52951049804688, 'learning_rate': 7.124375987469767e-07, 'epoch': 2.65}\n",
      "{'loss': 3.2002, 'grad_norm': 106.63201141357422, 'learning_rate': 6.996956050976878e-07, 'epoch': 2.66}\n",
      "{'loss': 3.3538, 'grad_norm': 110.71678924560547, 'learning_rate': 6.870644558319528e-07, 'epoch': 2.66}\n",
      "{'loss': 3.4585, 'grad_norm': 117.30211639404297, 'learning_rate': 6.745443014923658e-07, 'epoch': 2.66}\n",
      "{'loss': 3.6848, 'grad_norm': 145.78564453125, 'learning_rate': 6.621352912986468e-07, 'epoch': 2.67}\n",
      "{'loss': 3.6922, 'grad_norm': 176.73497009277344, 'learning_rate': 6.498375731458529e-07, 'epoch': 2.67}\n",
      "{'loss': 3.5959, 'grad_norm': 212.35598754882812, 'learning_rate': 6.37651293602628e-07, 'epoch': 2.67}\n",
      "{'loss': 3.6839, 'grad_norm': 140.2598876953125, 'learning_rate': 6.255765979094519e-07, 'epoch': 2.68}\n",
      "{'loss': 3.6877, 'grad_norm': 145.03494262695312, 'learning_rate': 6.136136299768991e-07, 'epoch': 2.68}\n",
      "{'loss': 3.6426, 'grad_norm': 157.21759033203125, 'learning_rate': 6.017625323839415e-07, 'epoch': 2.68}\n",
      "{'loss': 3.4398, 'grad_norm': 169.4998016357422, 'learning_rate': 5.900234463762367e-07, 'epoch': 2.68}\n",
      "{'loss': 3.7785, 'grad_norm': 154.02139282226562, 'learning_rate': 5.783965118644441e-07, 'epoch': 2.69}\n",
      "{'loss': 3.2964, 'grad_norm': 155.19798278808594, 'learning_rate': 5.668818674225684e-07, 'epoch': 2.69}\n",
      "{'loss': 3.3301, 'grad_norm': 179.57733154296875, 'learning_rate': 5.554796502862958e-07, 'epoch': 2.69}\n",
      "{'loss': 3.8356, 'grad_norm': 168.48602294921875, 'learning_rate': 5.441899963513631e-07, 'epoch': 2.7}\n",
      "{'loss': 3.3952, 'grad_norm': 164.29598999023438, 'learning_rate': 5.330130401719413e-07, 'epoch': 2.7}\n",
      "{'loss': 3.2546, 'grad_norm': 194.31369018554688, 'learning_rate': 5.219489149590251e-07, 'epoch': 2.7}\n",
      "{'loss': 3.2457, 'grad_norm': 155.62889099121094, 'learning_rate': 5.109977525788512e-07, 'epoch': 2.71}\n",
      "{'loss': 3.0778, 'grad_norm': 180.29469299316406, 'learning_rate': 5.001596835513256e-07, 'epoch': 2.71}\n",
      "{'loss': 3.5394, 'grad_norm': 181.73841857910156, 'learning_rate': 4.894348370484648e-07, 'epoch': 2.71}\n",
      "{'loss': 3.2807, 'grad_norm': 183.67974853515625, 'learning_rate': 4.788233408928588e-07, 'epoch': 2.72}\n",
      "{'loss': 3.749, 'grad_norm': 157.52413940429688, 'learning_rate': 4.6832532155614895e-07, 'epoch': 2.72}\n",
      "{'loss': 3.3922, 'grad_norm': 141.0074005126953, 'learning_rate': 4.5794090415751666e-07, 'epoch': 2.72}\n",
      "{'loss': 3.2954, 'grad_norm': 141.5591278076172, 'learning_rate': 4.4767021246219566e-07, 'epoch': 2.73}\n",
      "{'loss': 3.5041, 'grad_norm': 148.4921417236328, 'learning_rate': 4.3751336887999597e-07, 'epoch': 2.73}\n",
      "{'loss': 3.4048, 'grad_norm': 170.1424102783203, 'learning_rate': 4.27470494463843e-07, 'epoch': 2.73}\n",
      "{'loss': 3.7952, 'grad_norm': 220.0626983642578, 'learning_rate': 4.1754170890833777e-07, 'epoch': 2.74}\n",
      "{'loss': 3.5151, 'grad_norm': 154.43165588378906, 'learning_rate': 4.077271305483321e-07, 'epoch': 2.74}\n",
      "{'loss': 3.3799, 'grad_norm': 164.85084533691406, 'learning_rate': 3.980268763575079e-07, 'epoch': 2.74}\n",
      "{'loss': 3.4499, 'grad_norm': 182.76856994628906, 'learning_rate': 3.8844106194699696e-07, 'epoch': 2.75}\n",
      "{'loss': 3.3516, 'grad_norm': 161.34242248535156, 'learning_rate': 3.7896980156399533e-07, 'epoch': 2.75}\n",
      "{'loss': 3.1113, 'grad_norm': 193.349853515625, 'learning_rate': 3.6961320809039914e-07, 'epoch': 2.75}\n",
      "{'loss': 3.4263, 'grad_norm': 178.600341796875, 'learning_rate': 3.603713930414676e-07, 'epoch': 2.76}\n",
      "{'loss': 3.6026, 'grad_norm': 194.48269653320312, 'learning_rate': 3.5124446656448654e-07, 'epoch': 2.76}\n",
      "{'loss': 3.0199, 'grad_norm': 198.34718322753906, 'learning_rate': 3.42232537437458e-07, 'epoch': 2.76}\n",
      "{'loss': 3.1865, 'grad_norm': 160.73681640625, 'learning_rate': 3.33335713067805e-07, 'epoch': 2.76}\n",
      "{'loss': 3.1748, 'grad_norm': 176.5546417236328, 'learning_rate': 3.245540994910934e-07, 'epoch': 2.77}\n",
      "{'loss': 3.1469, 'grad_norm': 139.51373291015625, 'learning_rate': 3.158878013697586e-07, 'epoch': 2.77}\n",
      "{'loss': 3.0119, 'grad_norm': 206.28489685058594, 'learning_rate': 3.073369219918698e-07, 'epoch': 2.77}\n",
      "{'loss': 3.1647, 'grad_norm': 221.73211669921875, 'learning_rate': 2.989015632698944e-07, 'epoch': 2.78}\n",
      "{'loss': 3.4075, 'grad_norm': 156.66856384277344, 'learning_rate': 2.905818257394799e-07, 'epoch': 2.78}\n",
      "{'loss': 2.7619, 'grad_norm': 153.529541015625, 'learning_rate': 2.8237780855825957e-07, 'epoch': 2.78}\n",
      "{'loss': 3.1953, 'grad_norm': 152.48806762695312, 'learning_rate': 2.742896095046732e-07, 'epoch': 2.79}\n",
      "{'loss': 3.1483, 'grad_norm': 225.62071228027344, 'learning_rate': 2.6631732497679363e-07, 'epoch': 2.79}\n",
      "{'loss': 3.1569, 'grad_norm': 181.6493377685547, 'learning_rate': 2.584610499911833e-07, 'epoch': 2.79}\n",
      "{'loss': 2.8866, 'grad_norm': 251.1969757080078, 'learning_rate': 2.507208781817638e-07, 'epoch': 2.8}\n",
      "{'loss': 3.2946, 'grad_norm': 246.34408569335938, 'learning_rate': 2.4309690179869503e-07, 'epoch': 2.8}\n",
      "{'loss': 3.3932, 'grad_norm': 111.19287872314453, 'learning_rate': 2.355892117072789e-07, 'epoch': 2.8}\n",
      "{'loss': 3.1046, 'grad_norm': 123.27452850341797, 'learning_rate': 2.2819789738687482e-07, 'epoch': 2.81}\n",
      "{'loss': 3.0262, 'grad_norm': 94.71764373779297, 'learning_rate': 2.2092304692983402e-07, 'epoch': 2.81}\n",
      "{'loss': 3.0507, 'grad_norm': 113.79676055908203, 'learning_rate': 2.1376474704044693e-07, 'epoch': 2.81}\n",
      "{'loss': 3.2152, 'grad_norm': 123.21540069580078, 'learning_rate': 2.067230830339184e-07, 'epoch': 2.82}\n",
      "{'loss': 3.2375, 'grad_norm': 150.63192749023438, 'learning_rate': 1.9979813883533762e-07, 'epoch': 2.82}\n",
      "{'loss': 3.4462, 'grad_norm': 115.79449462890625, 'learning_rate': 1.929899969786897e-07, 'epoch': 2.82}\n",
      "{'loss': 3.5152, 'grad_norm': 120.9515151977539, 'learning_rate': 1.8629873860586567e-07, 'epoch': 2.83}\n",
      "{'loss': 3.4063, 'grad_norm': 136.05638122558594, 'learning_rate': 1.7972444346569752e-07, 'epoch': 2.83}\n",
      "{'loss': 3.6342, 'grad_norm': 146.0439453125, 'learning_rate': 1.7326718991300563e-07, 'epoch': 2.83}\n",
      "{'loss': 3.6001, 'grad_norm': 156.40548706054688, 'learning_rate': 1.6692705490766958e-07, 'epoch': 2.84}\n",
      "{'loss': 3.7109, 'grad_norm': 158.04042053222656, 'learning_rate': 1.6070411401370335e-07, 'epoch': 2.84}\n",
      "{'loss': 3.6355, 'grad_norm': 178.60296630859375, 'learning_rate': 1.5459844139836476e-07, 'epoch': 2.84}\n",
      "{'loss': 3.3076, 'grad_norm': 153.9482421875, 'learning_rate': 1.4861010983126202e-07, 'epoch': 2.84}\n",
      "{'loss': 3.4372, 'grad_norm': 134.20834350585938, 'learning_rate': 1.4273919068349184e-07, 'epoch': 2.85}\n",
      "{'loss': 3.618, 'grad_norm': 157.82232666015625, 'learning_rate': 1.3698575392678492e-07, 'epoch': 2.85}\n",
      "{'loss': 3.7559, 'grad_norm': 124.70575714111328, 'learning_rate': 1.3134986813267968e-07, 'epoch': 2.85}\n",
      "{'loss': 3.3627, 'grad_norm': 166.40870666503906, 'learning_rate': 1.258316004716953e-07, 'epoch': 2.86}\n",
      "{'loss': 3.4432, 'grad_norm': 149.03555297851562, 'learning_rate': 1.2043101671253553e-07, 'epoch': 2.86}\n",
      "{'loss': 3.3057, 'grad_norm': 134.8327178955078, 'learning_rate': 1.1514818122130844e-07, 'epoch': 2.86}\n",
      "{'loss': 3.4564, 'grad_norm': 148.934814453125, 'learning_rate': 1.0998315696075123e-07, 'epoch': 2.87}\n",
      "{'loss': 3.395, 'grad_norm': 193.41429138183594, 'learning_rate': 1.0493600548948879e-07, 'epoch': 2.87}\n",
      "{'loss': 3.2511, 'grad_norm': 151.963623046875, 'learning_rate': 1.0000678696129307e-07, 'epoch': 2.87}\n",
      "{'loss': 3.1594, 'grad_norm': 174.82603454589844, 'learning_rate': 9.519556012436815e-08, 'epoch': 2.88}\n",
      "{'loss': 3.3023, 'grad_norm': 159.4786376953125, 'learning_rate': 9.0502382320653e-08, 'epoch': 2.88}\n",
      "{'loss': 3.6121, 'grad_norm': 145.20167541503906, 'learning_rate': 8.592730948513205e-08, 'epoch': 2.88}\n",
      "{'loss': 3.1645, 'grad_norm': 139.3633270263672, 'learning_rate': 8.147039614517571e-08, 'epoch': 2.89}\n",
      "{'loss': 3.2119, 'grad_norm': 151.7472381591797, 'learning_rate': 7.71316954198853e-08, 'epoch': 2.89}\n",
      "{'loss': 3.4016, 'grad_norm': 135.70191955566406, 'learning_rate': 7.291125901946027e-08, 'epoch': 2.89}\n",
      "{'loss': 3.4454, 'grad_norm': 159.80062866210938, 'learning_rate': 6.880913724458538e-08, 'epoch': 2.9}\n",
      "{'loss': 3.1237, 'grad_norm': 152.7572784423828, 'learning_rate': 6.482537898582886e-08, 'epoch': 2.9}\n",
      "{'loss': 3.1917, 'grad_norm': 164.93370056152344, 'learning_rate': 6.096003172305742e-08, 'epoch': 2.9}\n",
      "{'loss': 3.1782, 'grad_norm': 148.6466064453125, 'learning_rate': 5.721314152487556e-08, 'epoch': 2.91}\n",
      "{'loss': 3.4427, 'grad_norm': 160.2681121826172, 'learning_rate': 5.3584753048073756e-08, 'epoch': 2.91}\n",
      "{'loss': 3.4299, 'grad_norm': 159.94041442871094, 'learning_rate': 5.007490953709227e-08, 'epoch': 2.91}\n",
      "{'loss': 3.4656, 'grad_norm': 138.4359588623047, 'learning_rate': 4.6683652823513725e-08, 'epoch': 2.92}\n",
      "{'loss': 3.3699, 'grad_norm': 190.84381103515625, 'learning_rate': 4.3411023325560245e-08, 'epoch': 2.92}\n",
      "{'loss': 3.1213, 'grad_norm': 197.93496704101562, 'learning_rate': 4.025706004760932e-08, 'epoch': 2.92}\n",
      "{'loss': 3.3066, 'grad_norm': 202.4763641357422, 'learning_rate': 3.7221800579735346e-08, 'epoch': 2.92}\n",
      "{'loss': 3.2843, 'grad_norm': 179.9412841796875, 'learning_rate': 3.430528109725439e-08, 'epoch': 2.93}\n",
      "{'loss': 3.1561, 'grad_norm': 179.16500854492188, 'learning_rate': 3.150753636029902e-08, 'epoch': 2.93}\n",
      "{'loss': 3.309, 'grad_norm': 199.96902465820312, 'learning_rate': 2.8828599713398575e-08, 'epoch': 2.93}\n",
      "{'loss': 3.0425, 'grad_norm': 199.9078826904297, 'learning_rate': 2.6268503085089547e-08, 'epoch': 2.94}\n",
      "{'loss': 3.2522, 'grad_norm': 170.4645233154297, 'learning_rate': 2.3827276987524738e-08, 'epoch': 2.94}\n",
      "{'loss': 2.7412, 'grad_norm': 163.8351593017578, 'learning_rate': 2.1504950516118007e-08, 'epoch': 2.94}\n",
      "{'loss': 2.8811, 'grad_norm': 196.56536865234375, 'learning_rate': 1.9301551349195648e-08, 'epoch': 2.95}\n",
      "{'loss': 2.7416, 'grad_norm': 174.85775756835938, 'learning_rate': 1.721710574766333e-08, 'epoch': 2.95}\n",
      "{'loss': 2.6885, 'grad_norm': 198.83441162109375, 'learning_rate': 1.5251638554694137e-08, 'epoch': 2.95}\n",
      "{'loss': 3.2719, 'grad_norm': 235.3522491455078, 'learning_rate': 1.340517319543877e-08, 'epoch': 2.96}\n",
      "{'loss': 3.1478, 'grad_norm': 252.21090698242188, 'learning_rate': 1.1677731676733584e-08, 'epoch': 2.96}\n",
      "{'loss': 3.1634, 'grad_norm': 101.0271987915039, 'learning_rate': 1.0069334586854106e-08, 'epoch': 2.96}\n",
      "{'loss': 3.2747, 'grad_norm': 120.38616180419922, 'learning_rate': 8.580001095253032e-09, 'epoch': 2.97}\n",
      "{'loss': 3.837, 'grad_norm': 123.69622039794922, 'learning_rate': 7.209748952347051e-09, 'epoch': 2.97}\n",
      "{'loss': 3.7408, 'grad_norm': 149.98326110839844, 'learning_rate': 5.958594489295921e-09, 'epoch': 2.97}\n",
      "{'loss': 2.9625, 'grad_norm': 150.41212463378906, 'learning_rate': 4.826552617807067e-09, 'epoch': 2.98}\n",
      "{'loss': 3.3235, 'grad_norm': 149.09449768066406, 'learning_rate': 3.8136368299668266e-09, 'epoch': 2.98}\n",
      "{'loss': 2.9646, 'grad_norm': 166.40284729003906, 'learning_rate': 2.9198591980705847e-09, 'epoch': 2.98}\n",
      "{'loss': 3.5449, 'grad_norm': 162.88279724121094, 'learning_rate': 2.145230374481777e-09, 'epoch': 2.99}\n",
      "{'loss': 3.7096, 'grad_norm': 139.9879913330078, 'learning_rate': 1.4897595915053242e-09, 'epoch': 2.99}\n",
      "{'loss': 3.2416, 'grad_norm': 195.13369750976562, 'learning_rate': 9.534546612810502e-10, 'epoch': 2.99}\n",
      "{'loss': 3.5715, 'grad_norm': 177.4425811767578, 'learning_rate': 5.363219756837624e-10, 'epoch': 3.0}\n",
      "{'loss': 2.4321, 'grad_norm': 176.15185546875, 'learning_rate': 2.3836650624997627e-10, 'epoch': 3.0}\n",
      "{'loss': 2.9552, 'grad_norm': 260.69464111328125, 'learning_rate': 5.959180412129506e-11, 'epoch': 3.0}\n",
      "{'train_runtime': 286.313, 'train_samples_per_second': 104.78, 'train_steps_per_second': 3.28, 'train_loss': 3.568755222204775, 'epoch': 3.0}\n",
      "100% 939/939 [04:44<00:00,  3.30it/s]\n",
      "*** Trainer State & Trained Model Saved To --> res/pythia-70m_cirriculum_full/output/ ***\n",
      "*** Trainer State & Trained Model Save-Pretrained To --> res/pythia-70m_cirriculum_full/output//pretrained ***\n",
      "*** Training Done!\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mcelestial-sunset-10\u001b[0m at: \u001b[34m\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251129_014918-1gbsfdew/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train.py --config_file configs/full/pythia-70m_cirriculum_full.yml --wandb_key \"0944191bcf43ea6231189f995e76d66cc523c13d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "467886c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-400\tcheckpoint-700\tconfig.json\t\t tokenizer_config.json\n",
      "checkpoint-450\tcheckpoint-750\tgeneration_config.json\t tokenizer.json\n",
      "checkpoint-500\tcheckpoint-800\tmodel.safetensors\t trainer_state.json\n",
      "checkpoint-550\tcheckpoint-850\tpretrained\t\t training_args.bin\n",
      "checkpoint-600\tcheckpoint-900\truns\n",
      "checkpoint-650\tcheckpoint-939\tspecial_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "!ls res/pythia-70m_cirriculum_full/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9868441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 checkpoints: [400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 939]\n",
      "Found 1 GPUs: [0]\n",
      "Starting checkpoint 400 on GPU 0\n",
      "2025-11-29 02:20:23.908046: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 02:20:23.925737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764382823.946772   16086 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764382823.953181   16086 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764382823.969503   16086 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764382823.969527   16086 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764382823.969530   16086 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764382823.969533   16086 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 02:20:23.974338: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-70m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-70m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "\n",
      "***======================================================================================================\n",
      "***** Checkpoint 400 ======================================================================================================\n",
      "***** Model loaded from res/pythia-70m_cirriculum_full/output/checkpoint-400!\n",
      "***** Tokenizer initilized!\n",
      "***** smart_tokenizer_and_embedding_resize done!\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "  0% 1/10000 [00:00<33:17,  5.01it/s]***** Predict-Progress -- 1 DONE !\n",
      "100% 10000/10000 [01:03<00:00, 158.22it/s]\n",
      "***** Losses saved to res/pythia-70m_cirriculum_full/output/checkpoint-400/losses.pt\n",
      "Checkpoint 400 completed successfully on GPU 0\n",
      "Starting checkpoint 450 on GPU 0\n",
      "2025-11-29 02:22:04.609417: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 02:22:04.626908: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764382924.648342   16543 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764382924.654842   16543 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764382924.670920   16543 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764382924.670949   16543 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764382924.670952   16543 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764382924.670955   16543 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 02:22:04.675694: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-70m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-70m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "\n",
      "***======================================================================================================\n",
      "***** Checkpoint 450 ======================================================================================================\n",
      "***** Model loaded from res/pythia-70m_cirriculum_full/output/checkpoint-450!\n",
      "***** Tokenizer initilized!\n",
      "***** smart_tokenizer_and_embedding_resize done!\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "  0% 1/10000 [00:00<30:34,  5.45it/s]***** Predict-Progress -- 1 DONE !\n",
      "100% 10000/10000 [01:02<00:00, 159.84it/s]\n",
      "***** Losses saved to res/pythia-70m_cirriculum_full/output/checkpoint-450/losses.pt\n",
      "Checkpoint 450 completed successfully on GPU 0\n",
      "Starting checkpoint 500 on GPU 0\n",
      "2025-11-29 02:23:43.626570: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 02:23:43.644249: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764383023.665492   16999 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764383023.672056   16999 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764383023.688417   16999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383023.688443   16999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383023.688446   16999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383023.688449   16999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 02:23:43.693245: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-70m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-70m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "\n",
      "***======================================================================================================\n",
      "***** Checkpoint 500 ======================================================================================================\n",
      "***** Model loaded from res/pythia-70m_cirriculum_full/output/checkpoint-500!\n",
      "***** Tokenizer initilized!\n",
      "***** smart_tokenizer_and_embedding_resize done!\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "  0% 1/10000 [00:00<30:31,  5.46it/s]***** Predict-Progress -- 1 DONE !\n",
      "100% 10000/10000 [01:02<00:00, 158.73it/s]\n",
      "***** Losses saved to res/pythia-70m_cirriculum_full/output/checkpoint-500/losses.pt\n",
      "Checkpoint 500 completed successfully on GPU 0\n",
      "Starting checkpoint 550 on GPU 0\n",
      "2025-11-29 02:25:23.698704: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 02:25:23.717127: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764383123.738721   17456 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764383123.745284   17456 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764383123.762081   17456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383123.762107   17456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383123.762110   17456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383123.762113   17456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 02:25:23.767003: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-70m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-70m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "\n",
      "***======================================================================================================\n",
      "***** Checkpoint 550 ======================================================================================================\n",
      "***** Model loaded from res/pythia-70m_cirriculum_full/output/checkpoint-550!\n",
      "***** Tokenizer initilized!\n",
      "***** smart_tokenizer_and_embedding_resize done!\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "  0% 1/10000 [00:00<31:04,  5.36it/s]***** Predict-Progress -- 1 DONE !\n",
      "100% 10000/10000 [01:03<00:00, 156.30it/s]\n",
      "***** Losses saved to res/pythia-70m_cirriculum_full/output/checkpoint-550/losses.pt\n",
      "Checkpoint 550 completed successfully on GPU 0\n",
      "Starting checkpoint 600 on GPU 0\n",
      "2025-11-29 02:27:04.706713: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 02:27:04.724433: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764383224.745538   17913 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764383224.752040   17913 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764383224.768092   17913 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383224.768118   17913 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383224.768121   17913 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383224.768123   17913 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 02:27:04.772991: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-70m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-70m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "\n",
      "***======================================================================================================\n",
      "***** Checkpoint 600 ======================================================================================================\n",
      "***** Model loaded from res/pythia-70m_cirriculum_full/output/checkpoint-600!\n",
      "***** Tokenizer initilized!\n",
      "***** smart_tokenizer_and_embedding_resize done!\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "  0% 1/10000 [00:00<30:51,  5.40it/s]***** Predict-Progress -- 1 DONE !\n",
      "100% 10000/10000 [01:04<00:00, 156.21it/s]\n",
      "***** Losses saved to res/pythia-70m_cirriculum_full/output/checkpoint-600/losses.pt\n",
      "Checkpoint 600 completed successfully on GPU 0\n",
      "Starting checkpoint 650 on GPU 0\n",
      "2025-11-29 02:28:45.682737: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 02:28:45.699994: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764383325.720892   18374 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764383325.727196   18374 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764383325.742981   18374 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383325.743005   18374 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383325.743008   18374 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383325.743011   18374 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 02:28:45.747622: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-70m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-70m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "\n",
      "***======================================================================================================\n",
      "***** Checkpoint 650 ======================================================================================================\n",
      "***** Model loaded from res/pythia-70m_cirriculum_full/output/checkpoint-650!\n",
      "***** Tokenizer initilized!\n",
      "***** smart_tokenizer_and_embedding_resize done!\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "  0% 1/10000 [00:00<30:43,  5.42it/s]***** Predict-Progress -- 1 DONE !\n",
      "100% 10000/10000 [01:02<00:00, 159.76it/s]\n",
      "***** Losses saved to res/pythia-70m_cirriculum_full/output/checkpoint-650/losses.pt\n",
      "Checkpoint 650 completed successfully on GPU 0\n",
      "Starting checkpoint 700 on GPU 0\n",
      "2025-11-29 02:30:24.703374: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 02:30:24.720622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764383424.741588   18829 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764383424.748032   18829 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764383424.764084   18829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383424.764111   18829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383424.764114   18829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383424.764116   18829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 02:30:24.768923: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-70m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-70m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "\n",
      "***======================================================================================================\n",
      "***** Checkpoint 700 ======================================================================================================\n",
      "***** Model loaded from res/pythia-70m_cirriculum_full/output/checkpoint-700!\n",
      "***** Tokenizer initilized!\n",
      "***** smart_tokenizer_and_embedding_resize done!\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "  0% 1/10000 [00:00<30:46,  5.42it/s]***** Predict-Progress -- 1 DONE !\n",
      "100% 10000/10000 [01:03<00:00, 158.29it/s]\n",
      "***** Losses saved to res/pythia-70m_cirriculum_full/output/checkpoint-700/losses.pt\n",
      "Checkpoint 700 completed successfully on GPU 0\n",
      "Starting checkpoint 750 on GPU 0\n",
      "2025-11-29 02:32:04.748631: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 02:32:04.766592: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764383524.787405   19282 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764383524.793811   19282 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764383524.810002   19282 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383524.810027   19282 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383524.810030   19282 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383524.810033   19282 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 02:32:04.814855: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-70m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-70m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "\n",
      "***======================================================================================================\n",
      "***** Checkpoint 750 ======================================================================================================\n",
      "***** Model loaded from res/pythia-70m_cirriculum_full/output/checkpoint-750!\n",
      "***** Tokenizer initilized!\n",
      "***** smart_tokenizer_and_embedding_resize done!\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "  0% 1/10000 [00:00<30:58,  5.38it/s]***** Predict-Progress -- 1 DONE !\n",
      "100% 10000/10000 [01:04<00:00, 156.13it/s]\n",
      "***** Losses saved to res/pythia-70m_cirriculum_full/output/checkpoint-750/losses.pt\n",
      "Checkpoint 750 completed successfully on GPU 0\n",
      "Starting checkpoint 800 on GPU 0\n",
      "2025-11-29 02:33:45.766172: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 02:33:45.783831: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764383625.804993   19743 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764383625.811483   19743 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764383625.827890   19743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383625.827917   19743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383625.827919   19743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383625.827922   19743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 02:33:45.832673: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-70m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-70m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "\n",
      "***======================================================================================================\n",
      "***** Checkpoint 800 ======================================================================================================\n",
      "***** Model loaded from res/pythia-70m_cirriculum_full/output/checkpoint-800!\n",
      "***** Tokenizer initilized!\n",
      "***** smart_tokenizer_and_embedding_resize done!\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "  0% 1/10000 [00:00<31:47,  5.24it/s]***** Predict-Progress -- 1 DONE !\n",
      "100% 10000/10000 [01:03<00:00, 157.48it/s]\n",
      "***** Losses saved to res/pythia-70m_cirriculum_full/output/checkpoint-800/losses.pt\n",
      "Checkpoint 800 completed successfully on GPU 0\n",
      "Starting checkpoint 850 on GPU 0\n",
      "2025-11-29 02:35:25.774759: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 02:35:25.792200: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764383725.813350   20202 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764383725.819867   20202 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764383725.836259   20202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383725.836284   20202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383725.836287   20202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383725.836290   20202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 02:35:25.841173: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-70m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-70m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "\n",
      "***======================================================================================================\n",
      "***** Checkpoint 850 ======================================================================================================\n",
      "***** Model loaded from res/pythia-70m_cirriculum_full/output/checkpoint-850!\n",
      "***** Tokenizer initilized!\n",
      "***** smart_tokenizer_and_embedding_resize done!\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "  0% 1/10000 [00:00<31:22,  5.31it/s]***** Predict-Progress -- 1 DONE !\n",
      "100% 10000/10000 [01:03<00:00, 156.95it/s]\n",
      "***** Losses saved to res/pythia-70m_cirriculum_full/output/checkpoint-850/losses.pt\n",
      "Checkpoint 850 completed successfully on GPU 0\n",
      "Starting checkpoint 900 on GPU 0\n",
      "2025-11-29 02:37:06.761210: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 02:37:06.778468: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764383826.799516   20659 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764383826.805967   20659 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764383826.822128   20659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383826.822156   20659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383826.822159   20659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383826.822161   20659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 02:37:06.826974: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-70m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-70m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "\n",
      "***======================================================================================================\n",
      "***** Checkpoint 900 ======================================================================================================\n",
      "***** Model loaded from res/pythia-70m_cirriculum_full/output/checkpoint-900!\n",
      "***** Tokenizer initilized!\n",
      "***** smart_tokenizer_and_embedding_resize done!\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "  0% 1/10000 [00:00<30:50,  5.40it/s]***** Predict-Progress -- 1 DONE !\n",
      "100% 10000/10000 [01:03<00:00, 156.29it/s]\n",
      "***** Losses saved to res/pythia-70m_cirriculum_full/output/checkpoint-900/losses.pt\n",
      "Checkpoint 900 completed successfully on GPU 0\n",
      "Starting checkpoint 939 on GPU 0\n",
      "2025-11-29 02:38:47.825701: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 02:38:47.843159: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764383927.864235   21120 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764383927.870696   21120 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764383927.886813   21120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383927.886841   21120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383927.886845   21120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764383927.886850   21120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 02:38:47.891645: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-70m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-70m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "\n",
      "***======================================================================================================\n",
      "***** Checkpoint 939 ======================================================================================================\n",
      "***** Model loaded from res/pythia-70m_cirriculum_full/output/checkpoint-939!\n",
      "***** Tokenizer initilized!\n",
      "***** smart_tokenizer_and_embedding_resize done!\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "  0% 1/10000 [00:00<30:36,  5.45it/s]***** Predict-Progress -- 1 DONE !\n",
      "100% 10000/10000 [01:03<00:00, 157.62it/s]\n",
      "***** Losses saved to res/pythia-70m_cirriculum_full/output/checkpoint-939/losses.pt\n",
      "Checkpoint 939 completed successfully on GPU 0\n"
     ]
    }
   ],
   "source": [
    "!python run_distributed_trajectories.py --model_path res/pythia-70m_cirriculum_full/output --config_file configs/full/pythia-70m_cirriculum_full.yml --checkpoints all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ac3eb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-29 02:41:07.700418: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 02:41:07.717982: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764384067.739695   21777 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764384067.746283   21777 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764384067.762789   21777 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764384067.762824   21777 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764384067.762827   21777 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764384067.762830   21777 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 02:41:07.767569: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhenryliu999\u001b[0m (\u001b[33mhenryliu999-other\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-410m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: S2L\n",
      "result_dir_name: s2l-pythia-410m_cirriculum_s2l\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 2\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "init_label_num: 5000\n",
      "n_round: 0\n",
      "n_query: 131020\n",
      "n_components: 100\n",
      "ref_model_path: res/pythia-70m_cirriculum_full/output\n",
      "num_loss_ckpts: -1\n",
      "distance: euclidean\n",
      "seed: 42\n",
      "\n",
      "*** Model initialized!\n",
      "*** Tokenizer initialized!\n",
      "*** Smart tokenizer and embedding resize done!\n",
      "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n",
      "*** res/pythia-70m_cirriculum_full/output/tokenizer_config.json ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/tokenizer_config.json ** Could not load losses.\n",
      "*** res/pythia-70m_cirriculum_full/output/checkpoint-700 ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/training_args.bin ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/training_args.bin ** Could not load losses.\n",
      "*** res/pythia-70m_cirriculum_full/output/checkpoint-850 ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/trainer_state.json ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/trainer_state.json ** Could not load losses.\n",
      "*** res/pythia-70m_cirriculum_full/output/checkpoint-600 ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/checkpoint-939 ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/checkpoint-800 ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/config.json ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/config.json ** Could not load losses.\n",
      "*** res/pythia-70m_cirriculum_full/output/checkpoint-900 ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/runs ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/runs ** Could not load losses.\n",
      "*** res/pythia-70m_cirriculum_full/output/model.safetensors ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/model.safetensors ** Could not load losses.\n",
      "*** res/pythia-70m_cirriculum_full/output/checkpoint-750 ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/tokenizer.json ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/tokenizer.json ** Could not load losses.\n",
      "*** res/pythia-70m_cirriculum_full/output/checkpoint-450 ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/checkpoint-550 ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/special_tokens_map.json ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/special_tokens_map.json ** Could not load losses.\n",
      "*** res/pythia-70m_cirriculum_full/output/checkpoint-650 ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/checkpoint-400 ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/generation_config.json ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/generation_config.json ** Could not load losses.\n",
      "*** res/pythia-70m_cirriculum_full/output/pretrained ** Loading losses...\n",
      "*** res/pythia-70m_cirriculum_full/output/pretrained ** Could not load losses.\n",
      "*** res/pythia-70m_cirriculum_full/output/checkpoint-500 ** Loading losses...\n",
      "*** Rank = 0, **Using all 12 losses\n",
      "torch.Size([10000, 12])\n",
      "*** Schedule built!\n",
      "Sampled 11 samples from source data/CoT/number_comparison.json\n",
      "Sampled 18 samples from source data/CoT/TheoremQA.json\n",
      "Sampled 32 samples from source data/PoT/TheoremQA.json\n",
      "Sampled 72 samples from source data/CoT/college_math.json\n",
      "Sampled 289 samples from source data/CoT/gsm_train.json\n",
      "Sampled 377 samples from source data/PoT/aqua_rat_filtered.json\n",
      "Sampled 404 samples from source data/PoT/MATH_train.json\n",
      "Sampled 420 samples from source data/CoT/MATH_train.json\n",
      "Sampled 511 samples from source data/PoT/gsm_gpt4.json\n",
      "Sampled 513 samples from source data/PoT/numglue.json\n",
      "WARNING clustering 934 points to 100 centroids: please provide at least 3900 training points\n",
      "Clustering 934 points in 12D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 19 (0.01 s, search 0.01 s): objective=22.5178 imbalance=1.271 nsplit=0       \n",
      "*** Rank = 0, **K-means took 0.015329122543334961 seconds\n",
      "*** Rank = 0, **Sanple from clusters with size > 2\n",
      "Sampled 588 samples from source data/PoT/mathqa.json with max loss trajectory coverage\n",
      "WARNING clustering 1060 points to 100 centroids: please provide at least 3900 training points\n",
      "Clustering 1060 points in 12D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 19 (0.00 s, search 0.00 s): objective=58.729 imbalance=1.315 nsplit=0        \n",
      "*** Rank = 0, **K-means took 0.004410505294799805 seconds\n",
      "*** Rank = 0, **Sanple from clusters with size > 2\n",
      "Sampled 588 samples from source data/CoT/gsm_rft.json with max loss trajectory coverage\n",
      "WARNING clustering 1899 points to 100 centroids: please provide at least 3900 training points\n",
      "Clustering 1899 points in 12D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 19 (0.01 s, search 0.01 s): objective=75.855 imbalance=1.184 nsplit=0        \n",
      "*** Rank = 0, **K-means took 0.007578134536743164 seconds\n",
      "*** Rank = 0, **Sanple from clusters with size > 2\n",
      "Sampled 588 samples from source data/CoT/math50k_camel.json with max loss trajectory coverage\n",
      "WARNING clustering 3460 points to 100 centroids: please provide at least 3900 training points\n",
      "Clustering 3460 points in 12D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 19 (0.01 s, search 0.01 s): objective=285.953 imbalance=1.205 nsplit=0       \n",
      "*** Rank = 0, **K-means took 0.013702392578125 seconds\n",
      "*** Rank = 0, **Sanple from clusters with size > 2\n",
      "Sampled 589 samples from source data/CoT/aqua_rat.json with max loss trajectory coverage\n",
      "*** labeled_idx: tensor([   2,    3,    4,  ..., 9987, 9992, 9997])\n",
      "*** jdump(labeled_data_json_format, labeled_data_path) SUCESSFUL to --> res/s2l-pythia-410m_cirriculum_s2l/data/labeled.json\n",
      "*** jdump(unlabeled_data_json_format, unlabeled_data_path) SUCESSFUL to --> res/s2l-pythia-410m_cirriculum_s2l/data/unlabeled.json\n",
      "*** Training-Data-Size = 5000\n",
      "WARNING:root:Loading data...\n",
      "making supervised_dataset -> jload('res/s2l-pythia-410m_cirriculum_s2l/data/labeled.json') SUCESSFUL\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "*** SANITY-CHECK: Training-Sample#1. - TEXT.:\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "A team of six entered for a shooting competition. The best marks man scored 85 points. If he had scored 92 points, the average scores for. The team would have been 84. How many points altogether did the team score?\n",
      "Answer Choices: (A) 288 (B) 497 (C) 168 (D) 127 (E) 664 Let's program in Python in the response.\n",
      "\n",
      "### Response:answers = ['A', 'B', 'C', 'D', 'E']\n",
      "# If the best marksman had scored 92 points, the total score would have been 84 * 6 = 504\n",
      "# But he actually scored 85 points, so the actual total score is 504 - 92 + 85\n",
      "actual_total_score = 504 - 92 + 85\n",
      "options = [288, 497, 168, 127, 664]\n",
      "index = options.index(actual_total_score)\n",
      "print(answers[index])</s>\n",
      "\n",
      "\n",
      "/content/S2L_Cirriculum/schedule_base.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=self.model,\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 50278, 'bos_token_id': 50279, 'pad_token_id': 50277}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m setting up run du3u9xzq (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run du3u9xzq (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m setting up run du3u9xzq (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/S2L_Cirriculum/wandb/run-20251129_024144-du3u9xzq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdazzling-star-12\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/henryliu999-other/S2L_Cirriculum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/henryliu999-other/S2L_Cirriculum/runs/du3u9xzq\u001b[0m\n",
      "{'loss': 1.3982, 'grad_norm': 24.408349990844727, 'learning_rate': 0.0, 'epoch': 0.01}\n",
      "{'loss': 1.256, 'grad_norm': 22.916765213012695, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.01}\n",
      "{'loss': 1.4541, 'grad_norm': 33.896636962890625, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.02}\n",
      "{'loss': 1.6232, 'grad_norm': 30.86801528930664, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.03}\n",
      "{'loss': 1.5724, 'grad_norm': 30.41029930114746, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.03}\n",
      "{'loss': 1.6461, 'grad_norm': 40.456016540527344, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.04}\n",
      "{'loss': 1.6604, 'grad_norm': 42.547515869140625, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 1.5697, 'grad_norm': 45.658565521240234, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.05}\n",
      "{'loss': 1.6759, 'grad_norm': 41.02204132080078, 'learning_rate': 1.0666666666666667e-05, 'epoch': 0.06}\n",
      "{'loss': 1.4609, 'grad_norm': 42.35494613647461, 'learning_rate': 1.2e-05, 'epoch': 0.06}\n",
      "{'loss': 1.5684, 'grad_norm': 43.49700164794922, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.07}\n",
      "{'loss': 1.6319, 'grad_norm': 36.15986251831055, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.08}\n",
      "{'loss': 1.6785, 'grad_norm': 41.544708251953125, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.08}\n",
      "{'loss': 1.5425, 'grad_norm': 36.80168533325195, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.09}\n",
      "{'loss': 1.7963, 'grad_norm': 38.30656433105469, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.1}\n",
      "{'loss': 1.5467, 'grad_norm': 37.19132614135742, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 1.4714, 'grad_norm': 38.63544845581055, 'learning_rate': 1.999976267781041e-05, 'epoch': 0.11}\n",
      "{'loss': 1.5735, 'grad_norm': 32.79814147949219, 'learning_rate': 1.999905072250599e-05, 'epoch': 0.12}\n",
      "{'loss': 1.5534, 'grad_norm': 36.704978942871094, 'learning_rate': 1.9997864167879313e-05, 'epoch': 0.12}\n",
      "{'loss': 1.3252, 'grad_norm': 35.38678741455078, 'learning_rate': 1.9996203070249516e-05, 'epoch': 0.13}\n",
      "{'loss': 1.7007, 'grad_norm': 55.54450225830078, 'learning_rate': 1.999406750845967e-05, 'epoch': 0.13}\n",
      "{'loss': 1.6186, 'grad_norm': 55.213871002197266, 'learning_rate': 1.999145758387301e-05, 'epoch': 0.14}\n",
      "{'loss': 1.6701, 'grad_norm': 73.12984466552734, 'learning_rate': 1.9988373420368146e-05, 'epoch': 0.15}\n",
      "{'loss': 1.5646, 'grad_norm': 47.38724899291992, 'learning_rate': 1.9984815164333163e-05, 'epoch': 0.15}\n",
      "{'loss': 1.6231, 'grad_norm': 54.3823356628418, 'learning_rate': 1.9980782984658682e-05, 'epoch': 0.16}\n",
      "{'loss': 1.5878, 'grad_norm': 38.94039535522461, 'learning_rate': 1.9976277072729845e-05, 'epoch': 0.17}\n",
      "{'loss': 1.6404, 'grad_norm': 43.92781066894531, 'learning_rate': 1.9971297642417233e-05, 'epoch': 0.17}\n",
      "{'loss': 1.8496, 'grad_norm': 44.140708923339844, 'learning_rate': 1.99658449300667e-05, 'epoch': 0.18}\n",
      "{'loss': 1.7001, 'grad_norm': 32.8816032409668, 'learning_rate': 1.9959919194488177e-05, 'epoch': 0.19}\n",
      "{'loss': 1.6801, 'grad_norm': 29.292362213134766, 'learning_rate': 1.9953520716943373e-05, 'epoch': 0.19}\n",
      "{'loss': 1.594, 'grad_norm': 35.00088882446289, 'learning_rate': 1.994664980113243e-05, 'epoch': 0.2}\n",
      "{'loss': 1.5997, 'grad_norm': 29.50918197631836, 'learning_rate': 1.9939306773179498e-05, 'epoch': 0.2}\n",
      "{'loss': 1.8463, 'grad_norm': 34.73456573486328, 'learning_rate': 1.9931491981617276e-05, 'epoch': 0.21}\n",
      "{'loss': 1.7484, 'grad_norm': 36.3448371887207, 'learning_rate': 1.992320579737045e-05, 'epoch': 0.22}\n",
      "{'loss': 1.7117, 'grad_norm': 31.511871337890625, 'learning_rate': 1.9914448613738107e-05, 'epoch': 0.22}\n",
      "{'loss': 1.6872, 'grad_norm': 40.07587432861328, 'learning_rate': 1.990522084637503e-05, 'epoch': 0.23}\n",
      "{'loss': 1.6325, 'grad_norm': 30.43364906311035, 'learning_rate': 1.9895522933272028e-05, 'epoch': 0.24}\n",
      "{'loss': 1.6869, 'grad_norm': 33.86764907836914, 'learning_rate': 1.9885355334735082e-05, 'epoch': 0.24}\n",
      "{'loss': 1.9874, 'grad_norm': 48.442840576171875, 'learning_rate': 1.987471853336355e-05, 'epoch': 0.25}\n",
      "{'loss': 1.5612, 'grad_norm': 20.9752254486084, 'learning_rate': 1.9863613034027224e-05, 'epoch': 0.26}\n",
      "{'loss': 1.4707, 'grad_norm': 20.015724182128906, 'learning_rate': 1.985203936384239e-05, 'epoch': 0.26}\n",
      "{'loss': 1.5138, 'grad_norm': 20.326284408569336, 'learning_rate': 1.98399980721468e-05, 'epoch': 0.27}\n",
      "{'loss': 1.5019, 'grad_norm': 36.04430389404297, 'learning_rate': 1.9827489730473597e-05, 'epoch': 0.28}\n",
      "{'loss': 1.7012, 'grad_norm': 27.079591751098633, 'learning_rate': 1.981451493252418e-05, 'epoch': 0.28}\n",
      "{'loss': 1.7909, 'grad_norm': 37.54152297973633, 'learning_rate': 1.980107429414005e-05, 'epoch': 0.29}\n",
      "{'loss': 1.4972, 'grad_norm': 35.42276382446289, 'learning_rate': 1.9787168453273546e-05, 'epoch': 0.29}\n",
      "{'loss': 1.798, 'grad_norm': 31.805160522460938, 'learning_rate': 1.9772798069957596e-05, 'epoch': 0.3}\n",
      "{'loss': 1.5554, 'grad_norm': 31.338220596313477, 'learning_rate': 1.9757963826274357e-05, 'epoch': 0.31}\n",
      "{'loss': 1.5263, 'grad_norm': 29.94700050354004, 'learning_rate': 1.9742666426322877e-05, 'epoch': 0.31}\n",
      "{'loss': 1.4312, 'grad_norm': 27.14005470275879, 'learning_rate': 1.972690659618564e-05, 'epoch': 0.32}\n",
      "{'loss': 1.6645, 'grad_norm': 26.52009391784668, 'learning_rate': 1.971068508389413e-05, 'epoch': 0.33}\n",
      "{'loss': 1.489, 'grad_norm': 33.824867248535156, 'learning_rate': 1.9694002659393306e-05, 'epoch': 0.33}\n",
      "{'loss': 1.5022, 'grad_norm': 26.89224624633789, 'learning_rate': 1.9676860114505073e-05, 'epoch': 0.34}\n",
      "{'loss': 1.4019, 'grad_norm': 23.36993408203125, 'learning_rate': 1.9659258262890683e-05, 'epoch': 0.35}\n",
      "{'loss': 1.609, 'grad_norm': 21.44124412536621, 'learning_rate': 1.9641197940012136e-05, 'epoch': 0.35}\n",
      "{'loss': 1.3439, 'grad_norm': 23.79900550842285, 'learning_rate': 1.9622680003092503e-05, 'epoch': 0.36}\n",
      "{'loss': 1.4784, 'grad_norm': 28.70575714111328, 'learning_rate': 1.9603705331075255e-05, 'epoch': 0.36}\n",
      "{'loss': 1.6133, 'grad_norm': 21.750019073486328, 'learning_rate': 1.958427482458253e-05, 'epoch': 0.37}\n",
      "{'loss': 1.3866, 'grad_norm': 20.127147674560547, 'learning_rate': 1.9564389405872396e-05, 'epoch': 0.38}\n",
      "{'loss': 1.1808, 'grad_norm': 15.824875831604004, 'learning_rate': 1.9544050018795076e-05, 'epoch': 0.38}\n",
      "{'loss': 1.4277, 'grad_norm': 16.052581787109375, 'learning_rate': 1.9523257628748148e-05, 'epoch': 0.39}\n",
      "{'loss': 1.5314, 'grad_norm': 19.794523239135742, 'learning_rate': 1.9502013222630714e-05, 'epoch': 0.4}\n",
      "{'loss': 1.4378, 'grad_norm': 16.430709838867188, 'learning_rate': 1.9480317808796573e-05, 'epoch': 0.4}\n",
      "{'loss': 1.7316, 'grad_norm': 16.94613265991211, 'learning_rate': 1.9458172417006347e-05, 'epoch': 0.41}\n",
      "{'loss': 1.4086, 'grad_norm': 18.673521041870117, 'learning_rate': 1.9435578098378612e-05, 'epoch': 0.42}\n",
      "{'loss': 1.5555, 'grad_norm': 24.53000259399414, 'learning_rate': 1.9412535925339998e-05, 'epoch': 0.42}\n",
      "{'loss': 1.5252, 'grad_norm': 24.240026473999023, 'learning_rate': 1.9389046991574298e-05, 'epoch': 0.43}\n",
      "{'loss': 1.5628, 'grad_norm': 20.77273178100586, 'learning_rate': 1.936511241197055e-05, 'epoch': 0.44}\n",
      "{'loss': 1.4709, 'grad_norm': 17.67043113708496, 'learning_rate': 1.9340733322570122e-05, 'epoch': 0.44}\n",
      "{'loss': 1.5887, 'grad_norm': 23.141504287719727, 'learning_rate': 1.9315910880512792e-05, 'epoch': 0.45}\n",
      "{'loss': 1.6247, 'grad_norm': 16.581369400024414, 'learning_rate': 1.9290646263981816e-05, 'epoch': 0.45}\n",
      "{'loss': 1.5752, 'grad_norm': 16.413949966430664, 'learning_rate': 1.9264940672148018e-05, 'epoch': 0.46}\n",
      "{'loss': 1.2564, 'grad_norm': 13.99431037902832, 'learning_rate': 1.9238795325112867e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3148, 'grad_norm': 23.474502563476562, 'learning_rate': 1.921221146385057e-05, 'epoch': 0.47}\n",
      "{'loss': 1.4409, 'grad_norm': 22.978118896484375, 'learning_rate': 1.9185190350149146e-05, 'epoch': 0.48}\n",
      "{'loss': 1.284, 'grad_norm': 17.32380485534668, 'learning_rate': 1.9157733266550577e-05, 'epoch': 0.49}\n",
      "{'loss': 1.258, 'grad_norm': 19.038694381713867, 'learning_rate': 1.9129841516289902e-05, 'epoch': 0.49}\n",
      "{'loss': 1.5404, 'grad_norm': 21.55421257019043, 'learning_rate': 1.910151642323337e-05, 'epoch': 0.5}\n",
      "{'loss': 1.4972, 'grad_norm': 14.058523178100586, 'learning_rate': 1.9072759331815602e-05, 'epoch': 0.51}\n",
      "{'loss': 1.431, 'grad_norm': 12.033449172973633, 'learning_rate': 1.9043571606975776e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3074, 'grad_norm': 12.296177864074707, 'learning_rate': 1.9013954634092847e-05, 'epoch': 0.52}\n",
      "{'loss': 1.5083, 'grad_norm': 12.238429069519043, 'learning_rate': 1.898390981891979e-05, 'epoch': 0.52}\n",
      "{'loss': 1.4996, 'grad_norm': 12.240432739257812, 'learning_rate': 1.8953438587516863e-05, 'epoch': 0.53}\n",
      "{'loss': 1.4716, 'grad_norm': 11.35621452331543, 'learning_rate': 1.8922542386183942e-05, 'epoch': 0.54}\n",
      "{'loss': 1.5907, 'grad_norm': 9.892946243286133, 'learning_rate': 1.8891222681391853e-05, 'epoch': 0.54}\n",
      "{'loss': 1.6435, 'grad_norm': 10.221834182739258, 'learning_rate': 1.885948095971278e-05, 'epoch': 0.55}\n",
      "{'loss': 1.4786, 'grad_norm': 10.662603378295898, 'learning_rate': 1.882731872774971e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4504, 'grad_norm': 11.759683609008789, 'learning_rate': 1.879473751206489e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4369, 'grad_norm': 9.838471412658691, 'learning_rate': 1.8761738859107423e-05, 'epoch': 0.57}\n",
      "{'loss': 1.5545, 'grad_norm': 11.174433708190918, 'learning_rate': 1.8728324335139814e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1383, 'grad_norm': 9.473451614379883, 'learning_rate': 1.869449552616367e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2434, 'grad_norm': 11.871150016784668, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2764, 'grad_norm': 11.78988265991211, 'learning_rate': 1.8625601495434968e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3264, 'grad_norm': 10.581476211547852, 'learning_rate': 1.8590539543698852e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2289, 'grad_norm': 11.131567001342773, 'learning_rate': 1.8555069846831882e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3837, 'grad_norm': 10.80842399597168, 'learning_rate': 1.851919408838327e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2045, 'grad_norm': 12.039278984069824, 'learning_rate': 1.8482913971175737e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3804, 'grad_norm': 11.9049654006958, 'learning_rate': 1.844623121722465e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3091, 'grad_norm': 12.171456336975098, 'learning_rate': 1.8409147567656306e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2786, 'grad_norm': 15.961479187011719, 'learning_rate': 1.8371664782625287e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3841, 'grad_norm': 12.639388084411621, 'learning_rate': 1.8333784641230917e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3504, 'grad_norm': 14.875731468200684, 'learning_rate': 1.8295508941432814e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2417, 'grad_norm': 14.266242980957031, 'learning_rate': 1.825683949996556e-05, 'epoch': 0.66}\n",
      "{'loss': 1.295, 'grad_norm': 19.993501663208008, 'learning_rate': 1.821777815225245e-05, 'epoch': 0.67}\n",
      "{'loss': 1.5669, 'grad_norm': 15.920372009277344, 'learning_rate': 1.817832675231841e-05, 'epoch': 0.67}\n",
      "{'loss': 1.512, 'grad_norm': 17.378847122192383, 'learning_rate': 1.813848717270195e-05, 'epoch': 0.68}\n",
      "{'loss': 1.4303, 'grad_norm': 16.3581485748291, 'learning_rate': 1.8098261304366334e-05, 'epoch': 0.68}\n",
      "{'loss': 1.4833, 'grad_norm': 18.4722957611084, 'learning_rate': 1.8057651056609784e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3657, 'grad_norm': 15.974922180175781, 'learning_rate': 1.8016658356974885e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1464, 'grad_norm': 16.26300811767578, 'learning_rate': 1.797528515115709e-05, 'epoch': 0.7}\n",
      "{'loss': 1.7422, 'grad_norm': 18.42116355895996, 'learning_rate': 1.7933533402912354e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2958, 'grad_norm': 21.51064682006836, 'learning_rate': 1.789140509396394e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2607, 'grad_norm': 16.37275505065918, 'learning_rate': 1.7848902223908345e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0472, 'grad_norm': 18.97760009765625, 'learning_rate': 1.7806026810120423e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2564, 'grad_norm': 19.386390686035156, 'learning_rate': 1.7762780887657576e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2006, 'grad_norm': 19.807598114013672, 'learning_rate': 1.771916650916321e-05, 'epoch': 0.74}\n",
      "{'loss': 1.4025, 'grad_norm': 27.180233001708984, 'learning_rate': 1.7675185744769286e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3704, 'grad_norm': 11.393983840942383, 'learning_rate': 1.7630840681998068e-05, 'epoch': 0.76}\n",
      "{'loss': 1.4302, 'grad_norm': 9.781767845153809, 'learning_rate': 1.758613342566303e-05, 'epoch': 0.76}\n",
      "{'loss': 1.4484, 'grad_norm': 10.882691383361816, 'learning_rate': 1.7541066097768965e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3276, 'grad_norm': 14.078420639038086, 'learning_rate': 1.7495640837411265e-05, 'epoch': 0.77}\n",
      "{'loss': 1.4626, 'grad_norm': 11.567641258239746, 'learning_rate': 1.744985980067437e-05, 'epoch': 0.78}\n",
      "{'loss': 1.4332, 'grad_norm': 11.86656665802002, 'learning_rate': 1.740372516052947e-05, 'epoch': 0.79}\n",
      "{'loss': 1.5641, 'grad_norm': 11.600678443908691, 'learning_rate': 1.735723910673132e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2416, 'grad_norm': 11.812721252441406, 'learning_rate': 1.731040384571433e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1276, 'grad_norm': 10.591867446899414, 'learning_rate': 1.7263221600487852e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1424, 'grad_norm': 14.019607543945312, 'learning_rate': 1.7215694610530624e-05, 'epoch': 0.81}\n",
      "{'loss': 1.32, 'grad_norm': 14.69047737121582, 'learning_rate': 1.7167825131684516e-05, 'epoch': 0.82}\n",
      "{'loss': 1.281, 'grad_norm': 11.836824417114258, 'learning_rate': 1.711961543604743e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2102, 'grad_norm': 10.74246597290039, 'learning_rate': 1.7071067811865477e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2219, 'grad_norm': 10.872801780700684, 'learning_rate': 1.702218456342435e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2943, 'grad_norm': 11.282923698425293, 'learning_rate': 1.6972968010939953e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2444, 'grad_norm': 12.638501167297363, 'learning_rate': 1.6923420490448298e-05, 'epoch': 0.85}\n",
      "{'loss': 1.4258, 'grad_norm': 11.393985748291016, 'learning_rate': 1.687354435369459e-05, 'epoch': 0.86}\n",
      "{'loss': 0.963, 'grad_norm': 11.712224960327148, 'learning_rate': 1.682334196802162e-05, 'epoch': 0.86}\n",
      "{'loss': 1.0998, 'grad_norm': 15.056710243225098, 'learning_rate': 1.6772815716257414e-05, 'epoch': 0.87}\n",
      "{'loss': 1.223, 'grad_norm': 15.235174179077148, 'learning_rate': 1.6721967996602106e-05, 'epoch': 0.88}\n",
      "{'loss': 1.2266, 'grad_norm': 17.975496292114258, 'learning_rate': 1.6670801222514135e-05, 'epoch': 0.88}\n",
      "{'loss': 1.2228, 'grad_norm': 15.268192291259766, 'learning_rate': 1.6619317822595666e-05, 'epoch': 0.89}\n",
      "{'loss': 1.3182, 'grad_norm': 17.698627471923828, 'learning_rate': 1.6567520240477344e-05, 'epoch': 0.9}\n",
      "{'loss': 1.2043, 'grad_norm': 17.3526611328125, 'learning_rate': 1.651541093470229e-05, 'epoch': 0.9}\n",
      "{'loss': 1.2382, 'grad_norm': 14.525668144226074, 'learning_rate': 1.646299237860941e-05, 'epoch': 0.91}\n",
      "{'loss': 1.1789, 'grad_norm': 18.132028579711914, 'learning_rate': 1.6410267060216007e-05, 'epoch': 0.92}\n",
      "{'loss': 1.1791, 'grad_norm': 19.35938835144043, 'learning_rate': 1.6357237482099682e-05, 'epoch': 0.92}\n",
      "{'loss': 1.2272, 'grad_norm': 18.5498046875, 'learning_rate': 1.6303906161279554e-05, 'epoch': 0.93}\n",
      "{'loss': 1.2653, 'grad_norm': 22.116384506225586, 'learning_rate': 1.625027562909679e-05, 'epoch': 0.93}\n",
      "{'loss': 1.3824, 'grad_norm': 21.497329711914062, 'learning_rate': 1.6196348431094448e-05, 'epoch': 0.94}\n",
      "{'loss': 1.3918, 'grad_norm': 33.79251480102539, 'learning_rate': 1.6142127126896682e-05, 'epoch': 0.95}\n",
      "{'loss': 1.3692, 'grad_norm': 28.293594360351562, 'learning_rate': 1.608761429008721e-05, 'epoch': 0.95}\n",
      "{'loss': 1.2178, 'grad_norm': 40.587406158447266, 'learning_rate': 1.603281250808719e-05, 'epoch': 0.96}\n",
      "{'loss': 1.2519, 'grad_norm': 29.237579345703125, 'learning_rate': 1.597772438203241e-05, 'epoch': 0.97}\n",
      "{'loss': 1.284, 'grad_norm': 23.149288177490234, 'learning_rate': 1.5922352526649803e-05, 'epoch': 0.97}\n",
      "{'loss': 1.3406, 'grad_norm': 23.92845344543457, 'learning_rate': 1.586669957013336e-05, 'epoch': 0.98}\n",
      "{'loss': 1.395, 'grad_norm': 24.167240142822266, 'learning_rate': 1.5810768154019386e-05, 'epoch': 0.99}\n",
      "{'loss': 1.3664, 'grad_norm': 27.107391357421875, 'learning_rate': 1.5754560933061104e-05, 'epoch': 0.99}\n",
      "{'loss': 1.1681, 'grad_norm': 31.622995376586914, 'learning_rate': 1.5698080575102662e-05, 'epoch': 1.0}\n",
      "{'loss': 1.0481, 'grad_norm': 33.09958267211914, 'learning_rate': 1.5641329760952514e-05, 'epoch': 1.0}\n",
      "{'loss': 1.4252, 'grad_norm': 12.528316497802734, 'learning_rate': 1.5584311184256144e-05, 'epoch': 1.01}\n",
      "{'loss': 1.2074, 'grad_norm': 10.82841968536377, 'learning_rate': 1.552702755136825e-05, 'epoch': 1.01}\n",
      "{'loss': 1.2562, 'grad_norm': 18.445728302001953, 'learning_rate': 1.5469481581224274e-05, 'epoch': 1.02}\n",
      "{'loss': 1.3414, 'grad_norm': 11.687483787536621, 'learning_rate': 1.541167600521133e-05, 'epoch': 1.03}\n",
      "{'loss': 1.2936, 'grad_norm': 11.897448539733887, 'learning_rate': 1.5353613567038607e-05, 'epoch': 1.03}\n",
      "{'loss': 1.368, 'grad_norm': 13.273011207580566, 'learning_rate': 1.529529702260709e-05, 'epoch': 1.04}\n",
      "{'loss': 1.3542, 'grad_norm': 12.038161277770996, 'learning_rate': 1.523672913987878e-05, 'epoch': 1.04}\n",
      "{'loss': 1.232, 'grad_norm': 8.55251693725586, 'learning_rate': 1.5177912698745314e-05, 'epoch': 1.05}\n",
      "{'loss': 1.234, 'grad_norm': 9.164432525634766, 'learning_rate': 1.5118850490896012e-05, 'epoch': 1.06}\n",
      "{'loss': 1.1312, 'grad_norm': 12.099474906921387, 'learning_rate': 1.505954531968537e-05, 'epoch': 1.06}\n",
      "{'loss': 1.1985, 'grad_norm': 9.19717788696289, 'learning_rate': 1.5000000000000002e-05, 'epoch': 1.07}\n",
      "{'loss': 1.2583, 'grad_norm': 9.233131408691406, 'learning_rate': 1.4940217358125042e-05, 'epoch': 1.08}\n",
      "{'loss': 1.1393, 'grad_norm': 10.654316902160645, 'learning_rate': 1.4880200231609982e-05, 'epoch': 1.08}\n",
      "{'loss': 1.078, 'grad_norm': 10.3340482711792, 'learning_rate': 1.4819951469133997e-05, 'epoch': 1.09}\n",
      "{'loss': 1.0611, 'grad_norm': 9.290942192077637, 'learning_rate': 1.4759473930370738e-05, 'epoch': 1.1}\n",
      "{'loss': 1.0686, 'grad_norm': 10.570327758789062, 'learning_rate': 1.4698770485852581e-05, 'epoch': 1.1}\n",
      "{'loss': 1.2329, 'grad_norm': 11.158761978149414, 'learning_rate': 1.4637844016834407e-05, 'epoch': 1.11}\n",
      "{'loss': 0.9974, 'grad_norm': 9.865175247192383, 'learning_rate': 1.4576697415156818e-05, 'epoch': 1.12}\n",
      "{'loss': 1.0369, 'grad_norm': 9.8379487991333, 'learning_rate': 1.4515333583108896e-05, 'epoch': 1.12}\n",
      "{'loss': 1.0784, 'grad_norm': 9.04789924621582, 'learning_rate': 1.4453755433290435e-05, 'epoch': 1.13}\n",
      "{'loss': 0.9821, 'grad_norm': 9.913074493408203, 'learning_rate': 1.4391965888473705e-05, 'epoch': 1.13}\n",
      "{'loss': 1.1126, 'grad_norm': 9.668536186218262, 'learning_rate': 1.4329967881464721e-05, 'epoch': 1.14}\n",
      "{'loss': 1.1194, 'grad_norm': 13.190214157104492, 'learning_rate': 1.4267764354964038e-05, 'epoch': 1.15}\n",
      "{'loss': 1.063, 'grad_norm': 10.13416862487793, 'learning_rate': 1.4205358261427076e-05, 'epoch': 1.15}\n",
      "{'loss': 0.9924, 'grad_norm': 12.653938293457031, 'learning_rate': 1.4142752562923988e-05, 'epoch': 1.16}\n",
      "{'loss': 1.1527, 'grad_norm': 14.285662651062012, 'learning_rate': 1.4079950230999069e-05, 'epoch': 1.17}\n",
      "{'loss': 1.008, 'grad_norm': 14.273873329162598, 'learning_rate': 1.4016954246529697e-05, 'epoch': 1.17}\n",
      "{'loss': 1.0632, 'grad_norm': 16.34848403930664, 'learning_rate': 1.3953767599584869e-05, 'epoch': 1.18}\n",
      "{'loss': 1.1702, 'grad_norm': 13.873598098754883, 'learning_rate': 1.3890393289283262e-05, 'epoch': 1.19}\n",
      "{'loss': 1.1149, 'grad_norm': 15.506750106811523, 'learning_rate': 1.3826834323650899e-05, 'epoch': 1.19}\n",
      "{'loss': 1.0586, 'grad_norm': 14.194217681884766, 'learning_rate': 1.3763093719478357e-05, 'epoch': 1.2}\n",
      "{'loss': 1.1129, 'grad_norm': 14.963292121887207, 'learning_rate': 1.369917450217758e-05, 'epoch': 1.2}\n",
      "{'loss': 1.1929, 'grad_norm': 18.486520767211914, 'learning_rate': 1.3635079705638298e-05, 'epoch': 1.21}\n",
      "{'loss': 0.9849, 'grad_norm': 19.88605308532715, 'learning_rate': 1.3570812372084e-05, 'epoch': 1.22}\n",
      "{'loss': 0.9386, 'grad_norm': 18.579599380493164, 'learning_rate': 1.3506375551927546e-05, 'epoch': 1.22}\n",
      "{'loss': 0.9317, 'grad_norm': 22.277671813964844, 'learning_rate': 1.3441772303626387e-05, 'epoch': 1.23}\n",
      "{'loss': 0.9815, 'grad_norm': 21.09149169921875, 'learning_rate': 1.3377005693537393e-05, 'epoch': 1.24}\n",
      "{'loss': 0.974, 'grad_norm': 19.988126754760742, 'learning_rate': 1.3312078795771307e-05, 'epoch': 1.24}\n",
      "{'loss': 1.0152, 'grad_norm': 27.539701461791992, 'learning_rate': 1.3246994692046837e-05, 'epoch': 1.25}\n",
      "{'loss': 1.327, 'grad_norm': 12.069005012512207, 'learning_rate': 1.3181756471544383e-05, 'epoch': 1.26}\n",
      "{'loss': 1.2106, 'grad_norm': 10.14518928527832, 'learning_rate': 1.3116367230759415e-05, 'epoch': 1.26}\n",
      "{'loss': 1.3453, 'grad_norm': 9.716297149658203, 'learning_rate': 1.305083007335549e-05, 'epoch': 1.27}\n",
      "{'loss': 1.3638, 'grad_norm': 12.569435119628906, 'learning_rate': 1.2985148110016947e-05, 'epoch': 1.28}\n",
      "{'loss': 1.3772, 'grad_norm': 12.257939338684082, 'learning_rate': 1.291932445830126e-05, 'epoch': 1.28}\n",
      "{'loss': 1.2786, 'grad_norm': 14.329164505004883, 'learning_rate': 1.2853362242491054e-05, 'epoch': 1.29}\n",
      "{'loss': 1.2344, 'grad_norm': 12.68545913696289, 'learning_rate': 1.2787264593445832e-05, 'epoch': 1.29}\n",
      "{'loss': 1.2798, 'grad_norm': 13.033432006835938, 'learning_rate': 1.2721034648453353e-05, 'epoch': 1.3}\n",
      "{'loss': 1.137, 'grad_norm': 12.295762062072754, 'learning_rate': 1.2654675551080724e-05, 'epoch': 1.31}\n",
      "{'loss': 1.2358, 'grad_norm': 11.95840835571289, 'learning_rate': 1.2588190451025209e-05, 'epoch': 1.31}\n",
      "{'loss': 1.2527, 'grad_norm': 13.847124099731445, 'learning_rate': 1.2521582503964708e-05, 'epoch': 1.32}\n",
      "{'loss': 1.1829, 'grad_norm': 11.073756217956543, 'learning_rate': 1.2454854871407993e-05, 'epoch': 1.33}\n",
      "{'loss': 0.9281, 'grad_norm': 10.54102897644043, 'learning_rate': 1.2388010720544634e-05, 'epoch': 1.33}\n",
      "{'loss': 1.1787, 'grad_norm': 11.228888511657715, 'learning_rate': 1.2321053224094678e-05, 'epoch': 1.34}\n",
      "{'loss': 1.0809, 'grad_norm': 10.32691764831543, 'learning_rate': 1.2253985560158064e-05, 'epoch': 1.35}\n",
      "{'loss': 1.2186, 'grad_norm': 13.292019844055176, 'learning_rate': 1.218681091206376e-05, 'epoch': 1.35}\n",
      "{'loss': 0.8053, 'grad_norm': 11.17857551574707, 'learning_rate': 1.2119532468218677e-05, 'epoch': 1.36}\n",
      "{'loss': 1.1038, 'grad_norm': 13.054438591003418, 'learning_rate': 1.2052153421956343e-05, 'epoch': 1.36}\n",
      "{'loss': 0.9633, 'grad_norm': 10.70275592803955, 'learning_rate': 1.1984676971385314e-05, 'epoch': 1.37}\n",
      "{'loss': 0.8846, 'grad_norm': 11.219626426696777, 'learning_rate': 1.1917106319237386e-05, 'epoch': 1.38}\n",
      "{'loss': 0.9244, 'grad_norm': 10.314664840698242, 'learning_rate': 1.1849444672715587e-05, 'epoch': 1.38}\n",
      "{'loss': 1.0215, 'grad_norm': 11.126609802246094, 'learning_rate': 1.1781695243341933e-05, 'epoch': 1.39}\n",
      "{'loss': 1.0357, 'grad_norm': 10.958603858947754, 'learning_rate': 1.1713861246805013e-05, 'epoch': 1.4}\n",
      "{'loss': 1.178, 'grad_norm': 9.549440383911133, 'learning_rate': 1.164594590280734e-05, 'epoch': 1.4}\n",
      "{'loss': 0.9216, 'grad_norm': 8.686511993408203, 'learning_rate': 1.157795243491255e-05, 'epoch': 1.41}\n",
      "{'loss': 1.0839, 'grad_norm': 9.8788423538208, 'learning_rate': 1.1509884070392369e-05, 'epoch': 1.42}\n",
      "{'loss': 1.0318, 'grad_norm': 11.653080940246582, 'learning_rate': 1.1441744040073469e-05, 'epoch': 1.42}\n",
      "{'loss': 1.1831, 'grad_norm': 9.926691055297852, 'learning_rate': 1.1373535578184083e-05, 'epoch': 1.43}\n",
      "{'loss': 1.0949, 'grad_norm': 9.400982856750488, 'learning_rate': 1.130526192220052e-05, 'epoch': 1.44}\n",
      "{'loss': 1.0793, 'grad_norm': 11.566290855407715, 'learning_rate': 1.123692631269348e-05, 'epoch': 1.44}\n",
      "{'loss': 1.2164, 'grad_norm': 10.693511009216309, 'learning_rate': 1.1168531993174259e-05, 'epoch': 1.45}\n",
      "{'loss': 0.9684, 'grad_norm': 9.647339820861816, 'learning_rate': 1.1100082209940795e-05, 'epoch': 1.45}\n",
      "{'loss': 0.9598, 'grad_norm': 8.984148979187012, 'learning_rate': 1.103158021192357e-05, 'epoch': 1.46}\n",
      "{'loss': 1.0148, 'grad_norm': 9.963336944580078, 'learning_rate': 1.0963029250531418e-05, 'epoch': 1.47}\n",
      "{'loss': 0.9672, 'grad_norm': 12.878464698791504, 'learning_rate': 1.0894432579497192e-05, 'epoch': 1.47}\n",
      "{'loss': 0.8646, 'grad_norm': 10.247647285461426, 'learning_rate': 1.0825793454723325e-05, 'epoch': 1.48}\n",
      "{'loss': 0.9751, 'grad_norm': 11.596410751342773, 'learning_rate': 1.0757115134127291e-05, 'epoch': 1.49}\n",
      "{'loss': 1.1332, 'grad_norm': 12.758667945861816, 'learning_rate': 1.0688400877486978e-05, 'epoch': 1.49}\n",
      "{'loss': 1.0063, 'grad_norm': 14.432766914367676, 'learning_rate': 1.0619653946285948e-05, 'epoch': 1.5}\n",
      "{'loss': 1.1683, 'grad_norm': 6.191705226898193, 'learning_rate': 1.0550877603558656e-05, 'epoch': 1.51}\n",
      "{'loss': 1.1833, 'grad_norm': 7.174776554107666, 'learning_rate': 1.048207511373555e-05, 'epoch': 1.51}\n",
      "{'loss': 1.1791, 'grad_norm': 5.986924648284912, 'learning_rate': 1.0413249742488132e-05, 'epoch': 1.52}\n",
      "{'loss': 1.2051, 'grad_norm': 6.759235858917236, 'learning_rate': 1.0344404756573972e-05, 'epoch': 1.52}\n",
      "{'loss': 1.3799, 'grad_norm': 8.560338973999023, 'learning_rate': 1.0275543423681622e-05, 'epoch': 1.53}\n",
      "{'loss': 1.337, 'grad_norm': 8.265101432800293, 'learning_rate': 1.0206669012275546e-05, 'epoch': 1.54}\n",
      "{'loss': 1.2413, 'grad_norm': 12.467022895812988, 'learning_rate': 1.0137784791440965e-05, 'epoch': 1.54}\n",
      "{'loss': 1.2281, 'grad_norm': 7.903509616851807, 'learning_rate': 1.0068894030728705e-05, 'epoch': 1.55}\n",
      "{'loss': 1.1309, 'grad_norm': 7.240969181060791, 'learning_rate': 1e-05, 'epoch': 1.56}\n",
      "{'loss': 0.9289, 'grad_norm': 6.752858638763428, 'learning_rate': 9.931105969271298e-06, 'epoch': 1.56}\n",
      "{'loss': 0.9583, 'grad_norm': 7.502236366271973, 'learning_rate': 9.862215208559037e-06, 'epoch': 1.57}\n",
      "{'loss': 1.0761, 'grad_norm': 8.573447227478027, 'learning_rate': 9.79333098772446e-06, 'epoch': 1.58}\n",
      "{'loss': 0.9957, 'grad_norm': 7.576855182647705, 'learning_rate': 9.724456576318383e-06, 'epoch': 1.58}\n",
      "{'loss': 1.016, 'grad_norm': 6.9324774742126465, 'learning_rate': 9.655595243426033e-06, 'epoch': 1.59}\n",
      "{'loss': 0.839, 'grad_norm': 7.342552661895752, 'learning_rate': 9.586750257511868e-06, 'epoch': 1.6}\n",
      "{'loss': 1.2038, 'grad_norm': 8.437997817993164, 'learning_rate': 9.517924886264454e-06, 'epoch': 1.6}\n",
      "{'loss': 0.7798, 'grad_norm': 7.854689121246338, 'learning_rate': 9.449122396441344e-06, 'epoch': 1.61}\n",
      "{'loss': 0.9785, 'grad_norm': 6.897814750671387, 'learning_rate': 9.380346053714055e-06, 'epoch': 1.61}\n",
      "{'loss': 1.0746, 'grad_norm': 7.898758411407471, 'learning_rate': 9.311599122513029e-06, 'epoch': 1.62}\n",
      "{'loss': 0.9687, 'grad_norm': 6.699366092681885, 'learning_rate': 9.24288486587271e-06, 'epoch': 1.63}\n",
      "{'loss': 0.8845, 'grad_norm': 8.08432674407959, 'learning_rate': 9.174206545276678e-06, 'epoch': 1.63}\n",
      "{'loss': 1.0784, 'grad_norm': 8.308361053466797, 'learning_rate': 9.105567420502808e-06, 'epoch': 1.64}\n",
      "{'loss': 1.2035, 'grad_norm': 7.357717037200928, 'learning_rate': 9.036970749468585e-06, 'epoch': 1.65}\n",
      "{'loss': 1.0688, 'grad_norm': 7.731811046600342, 'learning_rate': 8.968419788076431e-06, 'epoch': 1.65}\n",
      "{'loss': 1.0354, 'grad_norm': 7.126890182495117, 'learning_rate': 8.899917790059208e-06, 'epoch': 1.66}\n",
      "{'loss': 0.922, 'grad_norm': 7.420022964477539, 'learning_rate': 8.831468006825746e-06, 'epoch': 1.67}\n",
      "{'loss': 1.0278, 'grad_norm': 7.914211273193359, 'learning_rate': 8.763073687306523e-06, 'epoch': 1.67}\n",
      "{'loss': 1.2028, 'grad_norm': 8.996089935302734, 'learning_rate': 8.694738077799487e-06, 'epoch': 1.68}\n",
      "{'loss': 1.0671, 'grad_norm': 9.357922554016113, 'learning_rate': 8.626464421815919e-06, 'epoch': 1.68}\n",
      "{'loss': 0.9936, 'grad_norm': 8.223133087158203, 'learning_rate': 8.558255959926533e-06, 'epoch': 1.69}\n",
      "{'loss': 1.1928, 'grad_norm': 8.786767959594727, 'learning_rate': 8.49011592960763e-06, 'epoch': 1.7}\n",
      "{'loss': 0.9515, 'grad_norm': 8.494449615478516, 'learning_rate': 8.422047565087455e-06, 'epoch': 1.7}\n",
      "{'loss': 1.0364, 'grad_norm': 7.921321868896484, 'learning_rate': 8.35405409719266e-06, 'epoch': 1.71}\n",
      "{'loss': 1.2147, 'grad_norm': 9.056968688964844, 'learning_rate': 8.286138753194993e-06, 'epoch': 1.72}\n",
      "{'loss': 1.0997, 'grad_norm': 8.963553428649902, 'learning_rate': 8.218304756658072e-06, 'epoch': 1.72}\n",
      "{'loss': 1.0163, 'grad_norm': 8.884286880493164, 'learning_rate': 8.150555327284417e-06, 'epoch': 1.73}\n",
      "{'loss': 0.8349, 'grad_norm': 8.250275611877441, 'learning_rate': 8.082893680762619e-06, 'epoch': 1.74}\n",
      "{'loss': 0.7827, 'grad_norm': 11.423345565795898, 'learning_rate': 8.015323028614688e-06, 'epoch': 1.74}\n",
      "{'loss': 0.8889, 'grad_norm': 9.911523818969727, 'learning_rate': 7.947846578043658e-06, 'epoch': 1.75}\n",
      "{'loss': 1.1357, 'grad_norm': 5.514070510864258, 'learning_rate': 7.880467531781323e-06, 'epoch': 1.76}\n",
      "{'loss': 1.2457, 'grad_norm': 5.475132465362549, 'learning_rate': 7.813189087936243e-06, 'epoch': 1.76}\n",
      "{'loss': 1.2714, 'grad_norm': 7.459404468536377, 'learning_rate': 7.746014439841941e-06, 'epoch': 1.77}\n",
      "{'loss': 1.0894, 'grad_norm': 5.777510643005371, 'learning_rate': 7.678946775905323e-06, 'epoch': 1.77}\n",
      "{'loss': 1.3459, 'grad_norm': 5.575949668884277, 'learning_rate': 7.611989279455371e-06, 'epoch': 1.78}\n",
      "{'loss': 1.4084, 'grad_norm': 5.928121566772461, 'learning_rate': 7.545145128592009e-06, 'epoch': 1.79}\n",
      "{'loss': 1.2537, 'grad_norm': 7.073980808258057, 'learning_rate': 7.478417496035294e-06, 'epoch': 1.79}\n",
      "{'loss': 1.2094, 'grad_norm': 5.709934711456299, 'learning_rate': 7.411809548974792e-06, 'epoch': 1.8}\n",
      "{'loss': 1.1913, 'grad_norm': 5.809081554412842, 'learning_rate': 7.34532444891928e-06, 'epoch': 1.81}\n",
      "{'loss': 0.938, 'grad_norm': 5.357234001159668, 'learning_rate': 7.278965351546648e-06, 'epoch': 1.81}\n",
      "{'loss': 1.1081, 'grad_norm': 5.721870422363281, 'learning_rate': 7.21273540655417e-06, 'epoch': 1.82}\n",
      "{'loss': 1.0613, 'grad_norm': 6.174818992614746, 'learning_rate': 7.14663775750895e-06, 'epoch': 1.83}\n",
      "{'loss': 1.009, 'grad_norm': 5.900502681732178, 'learning_rate': 7.080675541698743e-06, 'epoch': 1.83}\n",
      "{'loss': 1.0414, 'grad_norm': 6.038222789764404, 'learning_rate': 7.014851889983058e-06, 'epoch': 1.84}\n",
      "{'loss': 0.8323, 'grad_norm': 5.359280586242676, 'learning_rate': 6.949169926644513e-06, 'epoch': 1.84}\n",
      "{'loss': 0.8582, 'grad_norm': 5.5254974365234375, 'learning_rate': 6.883632769240589e-06, 'epoch': 1.85}\n",
      "{'loss': 0.944, 'grad_norm': 5.951173782348633, 'learning_rate': 6.818243528455618e-06, 'epoch': 1.86}\n",
      "{'loss': 0.8671, 'grad_norm': 5.732396125793457, 'learning_rate': 6.7530053079531664e-06, 'epoch': 1.86}\n",
      "{'loss': 1.0142, 'grad_norm': 6.542916774749756, 'learning_rate': 6.687921204228698e-06, 'epoch': 1.87}\n",
      "{'loss': 0.9418, 'grad_norm': 5.559123992919922, 'learning_rate': 6.6229943064626115e-06, 'epoch': 1.88}\n",
      "{'loss': 0.9062, 'grad_norm': 6.627279281616211, 'learning_rate': 6.558227696373617e-06, 'epoch': 1.88}\n",
      "{'loss': 1.107, 'grad_norm': 6.098381519317627, 'learning_rate': 6.4936244480724575e-06, 'epoch': 1.89}\n",
      "{'loss': 0.7924, 'grad_norm': 6.258964538574219, 'learning_rate': 6.429187627916003e-06, 'epoch': 1.9}\n",
      "{'loss': 0.9479, 'grad_norm': 6.217166900634766, 'learning_rate': 6.364920294361701e-06, 'epoch': 1.9}\n",
      "{'loss': 0.8399, 'grad_norm': 6.169191837310791, 'learning_rate': 6.300825497822421e-06, 'epoch': 1.91}\n",
      "{'loss': 0.9086, 'grad_norm': 6.549243450164795, 'learning_rate': 6.236906280521646e-06, 'epoch': 1.92}\n",
      "{'loss': 0.9927, 'grad_norm': 6.909409999847412, 'learning_rate': 6.173165676349103e-06, 'epoch': 1.92}\n",
      "{'loss': 1.0412, 'grad_norm': 6.155458450317383, 'learning_rate': 6.109606710716741e-06, 'epoch': 1.93}\n",
      "{'loss': 1.125, 'grad_norm': 7.745985507965088, 'learning_rate': 6.0462324004151355e-06, 'epoch': 1.93}\n",
      "{'loss': 1.2309, 'grad_norm': 8.943183898925781, 'learning_rate': 5.983045753470308e-06, 'epoch': 1.94}\n",
      "{'loss': 1.0363, 'grad_norm': 7.751873970031738, 'learning_rate': 5.920049769000934e-06, 'epoch': 1.95}\n",
      "{'loss': 0.9079, 'grad_norm': 7.381524085998535, 'learning_rate': 5.857247437076012e-06, 'epoch': 1.95}\n",
      "{'loss': 0.9048, 'grad_norm': 7.294864654541016, 'learning_rate': 5.794641738572925e-06, 'epoch': 1.96}\n",
      "{'loss': 0.916, 'grad_norm': 8.255647659301758, 'learning_rate': 5.732235645035964e-06, 'epoch': 1.97}\n",
      "{'loss': 1.0389, 'grad_norm': 8.927201271057129, 'learning_rate': 5.670032118535281e-06, 'epoch': 1.97}\n",
      "{'loss': 0.844, 'grad_norm': 7.658039093017578, 'learning_rate': 5.608034111526298e-06, 'epoch': 1.98}\n",
      "{'loss': 0.9636, 'grad_norm': 9.106586456298828, 'learning_rate': 5.5462445667095684e-06, 'epoch': 1.99}\n",
      "{'loss': 0.9714, 'grad_norm': 13.860318183898926, 'learning_rate': 5.484666416891109e-06, 'epoch': 1.99}\n",
      "{'loss': 0.8574, 'grad_norm': 10.96987247467041, 'learning_rate': 5.423302584843186e-06, 'epoch': 2.0}\n",
      "{'loss': 0.6891, 'grad_norm': 12.580041885375977, 'learning_rate': 5.362155983165594e-06, 'epoch': 2.0}\n",
      "{'loss': 1.011, 'grad_norm': 4.5657734870910645, 'learning_rate': 5.301229514147419e-06, 'epoch': 2.01}\n",
      "{'loss': 1.2023, 'grad_norm': 4.501542091369629, 'learning_rate': 5.240526069629265e-06, 'epoch': 2.01}\n",
      "{'loss': 1.0183, 'grad_norm': 4.0881500244140625, 'learning_rate': 5.180048530866004e-06, 'epoch': 2.02}\n",
      "{'loss': 1.1293, 'grad_norm': 4.588848114013672, 'learning_rate': 5.119799768390021e-06, 'epoch': 2.03}\n",
      "{'loss': 1.2303, 'grad_norm': 5.142581939697266, 'learning_rate': 5.059782641874962e-06, 'epoch': 2.03}\n",
      "{'loss': 1.0437, 'grad_norm': 5.237027645111084, 'learning_rate': 5.000000000000003e-06, 'epoch': 2.04}\n",
      "{'loss': 1.146, 'grad_norm': 5.902374744415283, 'learning_rate': 4.940454680314632e-06, 'epoch': 2.04}\n",
      "{'loss': 1.09, 'grad_norm': 7.082878589630127, 'learning_rate': 4.881149509103993e-06, 'epoch': 2.05}\n",
      "{'loss': 0.9369, 'grad_norm': 5.85634708404541, 'learning_rate': 4.822087301254686e-06, 'epoch': 2.06}\n",
      "{'loss': 0.9152, 'grad_norm': 6.201871871948242, 'learning_rate': 4.763270860121222e-06, 'epoch': 2.06}\n",
      "{'loss': 0.9384, 'grad_norm': 5.53693151473999, 'learning_rate': 4.704702977392914e-06, 'epoch': 2.07}\n",
      "{'loss': 0.8671, 'grad_norm': 5.206831455230713, 'learning_rate': 4.646386432961396e-06, 'epoch': 2.08}\n",
      "{'loss': 0.8776, 'grad_norm': 5.838264465332031, 'learning_rate': 4.5883239947886726e-06, 'epoch': 2.08}\n",
      "{'loss': 0.8837, 'grad_norm': 5.270049095153809, 'learning_rate': 4.530518418775734e-06, 'epoch': 2.09}\n",
      "{'loss': 0.9481, 'grad_norm': 5.531768798828125, 'learning_rate': 4.472972448631754e-06, 'epoch': 2.1}\n",
      "{'loss': 0.9703, 'grad_norm': 6.803117752075195, 'learning_rate': 4.415688815743858e-06, 'epoch': 2.1}\n",
      "{'loss': 0.8377, 'grad_norm': 6.302883148193359, 'learning_rate': 4.35867023904749e-06, 'epoch': 2.11}\n",
      "{'loss': 0.8352, 'grad_norm': 7.168339252471924, 'learning_rate': 4.301919424897339e-06, 'epoch': 2.12}\n",
      "{'loss': 0.9601, 'grad_norm': 6.775455474853516, 'learning_rate': 4.2454390669389e-06, 'epoch': 2.12}\n",
      "{'loss': 0.8915, 'grad_norm': 6.763454914093018, 'learning_rate': 4.189231845980618e-06, 'epoch': 2.13}\n",
      "{'loss': 0.8372, 'grad_norm': 6.469552040100098, 'learning_rate': 4.133300429866643e-06, 'epoch': 2.13}\n",
      "{'loss': 0.9464, 'grad_norm': 6.8530120849609375, 'learning_rate': 4.077647473350201e-06, 'epoch': 2.14}\n",
      "{'loss': 0.8178, 'grad_norm': 6.695575714111328, 'learning_rate': 4.0222756179675915e-06, 'epoch': 2.15}\n",
      "{'loss': 1.0116, 'grad_norm': 7.091066837310791, 'learning_rate': 3.967187491912813e-06, 'epoch': 2.15}\n",
      "{'loss': 0.6646, 'grad_norm': 6.862252235412598, 'learning_rate': 3.912385709912794e-06, 'epoch': 2.16}\n",
      "{'loss': 0.8137, 'grad_norm': 8.520278930664062, 'learning_rate': 3.857872873103322e-06, 'epoch': 2.17}\n",
      "{'loss': 0.8077, 'grad_norm': 7.299831867218018, 'learning_rate': 3.803651568905554e-06, 'epoch': 2.17}\n",
      "{'loss': 0.9082, 'grad_norm': 7.625579357147217, 'learning_rate': 3.749724370903216e-06, 'epoch': 2.18}\n",
      "{'loss': 1.0277, 'grad_norm': 7.6350202560424805, 'learning_rate': 3.69609383872045e-06, 'epoch': 2.19}\n",
      "{'loss': 0.8106, 'grad_norm': 7.6630706787109375, 'learning_rate': 3.6427625179003223e-06, 'epoch': 2.19}\n",
      "{'loss': 0.8579, 'grad_norm': 7.621020317077637, 'learning_rate': 3.5897329397839975e-06, 'epoch': 2.2}\n",
      "{'loss': 0.8513, 'grad_norm': 12.20942497253418, 'learning_rate': 3.5370076213905904e-06, 'epoch': 2.2}\n",
      "{'loss': 0.8921, 'grad_norm': 8.13758373260498, 'learning_rate': 3.484589065297711e-06, 'epoch': 2.21}\n",
      "{'loss': 0.8085, 'grad_norm': 9.283821105957031, 'learning_rate': 3.4324797595226567e-06, 'epoch': 2.22}\n",
      "{'loss': 0.8412, 'grad_norm': 9.237272262573242, 'learning_rate': 3.380682177404335e-06, 'epoch': 2.22}\n",
      "{'loss': 0.7391, 'grad_norm': 8.258414268493652, 'learning_rate': 3.329198777485869e-06, 'epoch': 2.23}\n",
      "{'loss': 0.7266, 'grad_norm': 9.26369857788086, 'learning_rate': 3.2780320033978975e-06, 'epoch': 2.24}\n",
      "{'loss': 0.8714, 'grad_norm': 11.710552215576172, 'learning_rate': 3.2271842837425917e-06, 'epoch': 2.24}\n",
      "{'loss': 0.7699, 'grad_norm': 10.745806694030762, 'learning_rate': 3.1766580319783814e-06, 'epoch': 2.25}\n",
      "{'loss': 1.1613, 'grad_norm': 5.281564712524414, 'learning_rate': 3.1264556463054162e-06, 'epoch': 2.26}\n",
      "{'loss': 0.9844, 'grad_norm': 4.8506622314453125, 'learning_rate': 3.0765795095517026e-06, 'epoch': 2.26}\n",
      "{'loss': 1.0928, 'grad_norm': 4.955047130584717, 'learning_rate': 3.0270319890600465e-06, 'epoch': 2.27}\n",
      "{'loss': 1.1045, 'grad_norm': 5.914402961730957, 'learning_rate': 2.977815436575654e-06, 'epoch': 2.28}\n",
      "{'loss': 1.1684, 'grad_norm': 6.474936485290527, 'learning_rate': 2.9289321881345257e-06, 'epoch': 2.28}\n",
      "{'loss': 1.0946, 'grad_norm': 6.837672710418701, 'learning_rate': 2.880384563952573e-06, 'epoch': 2.29}\n",
      "{'loss': 1.1089, 'grad_norm': 6.338348865509033, 'learning_rate': 2.8321748683154893e-06, 'epoch': 2.29}\n",
      "{'loss': 0.9579, 'grad_norm': 7.17185115814209, 'learning_rate': 2.7843053894693805e-06, 'epoch': 2.3}\n",
      "{'loss': 0.9247, 'grad_norm': 6.327229976654053, 'learning_rate': 2.73677839951215e-06, 'epoch': 2.31}\n",
      "{'loss': 0.9377, 'grad_norm': 6.981347560882568, 'learning_rate': 2.689596154285672e-06, 'epoch': 2.31}\n",
      "{'loss': 0.9721, 'grad_norm': 9.923701286315918, 'learning_rate': 2.642760893268684e-06, 'epoch': 2.32}\n",
      "{'loss': 0.9115, 'grad_norm': 7.643708229064941, 'learning_rate': 2.5962748394705326e-06, 'epoch': 2.33}\n",
      "{'loss': 0.8331, 'grad_norm': 7.627870082855225, 'learning_rate': 2.55014019932563e-06, 'epoch': 2.33}\n",
      "{'loss': 0.8574, 'grad_norm': 7.240132808685303, 'learning_rate': 2.504359162588741e-06, 'epoch': 2.34}\n",
      "{'loss': 0.8374, 'grad_norm': 7.21933650970459, 'learning_rate': 2.4589339022310386e-06, 'epoch': 2.35}\n",
      "{'loss': 0.9414, 'grad_norm': 6.663401126861572, 'learning_rate': 2.413866574336975e-06, 'epoch': 2.35}\n",
      "{'loss': 0.7818, 'grad_norm': 8.07718276977539, 'learning_rate': 2.369159318001937e-06, 'epoch': 2.36}\n",
      "{'loss': 0.832, 'grad_norm': 6.970658302307129, 'learning_rate': 2.324814255230714e-06, 'epoch': 2.36}\n",
      "{'loss': 0.9162, 'grad_norm': 8.007091522216797, 'learning_rate': 2.2808334908367914e-06, 'epoch': 2.37}\n",
      "{'loss': 0.5956, 'grad_norm': 6.800124168395996, 'learning_rate': 2.237219112342426e-06, 'epoch': 2.38}\n",
      "{'loss': 0.9177, 'grad_norm': 7.155671119689941, 'learning_rate': 2.1939731898795803e-06, 'epoch': 2.38}\n",
      "{'loss': 0.7794, 'grad_norm': 9.643218040466309, 'learning_rate': 2.151097776091654e-06, 'epoch': 2.39}\n",
      "{'loss': 0.9467, 'grad_norm': 7.329101085662842, 'learning_rate': 2.1085949060360654e-06, 'epoch': 2.4}\n",
      "{'loss': 0.9039, 'grad_norm': 8.607955932617188, 'learning_rate': 2.0664665970876496e-06, 'epoch': 2.4}\n",
      "{'loss': 0.8936, 'grad_norm': 7.827223300933838, 'learning_rate': 2.0247148488429104e-06, 'epoch': 2.41}\n",
      "{'loss': 0.9769, 'grad_norm': 7.553165912628174, 'learning_rate': 1.983341643025117e-06, 'epoch': 2.42}\n",
      "{'loss': 0.9668, 'grad_norm': 8.314334869384766, 'learning_rate': 1.9423489433902186e-06, 'epoch': 2.42}\n",
      "{'loss': 0.9144, 'grad_norm': 25.706249237060547, 'learning_rate': 1.9017386956336692e-06, 'epoch': 2.43}\n",
      "{'loss': 1.0019, 'grad_norm': 8.65068244934082, 'learning_rate': 1.861512827298051e-06, 'epoch': 2.44}\n",
      "{'loss': 0.904, 'grad_norm': 10.19472885131836, 'learning_rate': 1.8216732476815935e-06, 'epoch': 2.44}\n",
      "{'loss': 0.9795, 'grad_norm': 9.84058666229248, 'learning_rate': 1.7822218477475496e-06, 'epoch': 2.45}\n",
      "{'loss': 0.8136, 'grad_norm': 9.922245979309082, 'learning_rate': 1.743160500034443e-06, 'epoch': 2.45}\n",
      "{'loss': 0.8215, 'grad_norm': 11.976814270019531, 'learning_rate': 1.704491058567187e-06, 'epoch': 2.46}\n",
      "{'loss': 0.9016, 'grad_norm': 9.832677841186523, 'learning_rate': 1.6662153587690844e-06, 'epoch': 2.47}\n",
      "{'loss': 0.6614, 'grad_norm': 8.339445114135742, 'learning_rate': 1.6283352173747148e-06, 'epoch': 2.47}\n",
      "{'loss': 0.8309, 'grad_norm': 10.718027114868164, 'learning_rate': 1.5908524323436958e-06, 'epoch': 2.48}\n",
      "{'loss': 0.7984, 'grad_norm': 10.645310401916504, 'learning_rate': 1.5537687827753512e-06, 'epoch': 2.49}\n",
      "{'loss': 0.7261, 'grad_norm': 12.954313278198242, 'learning_rate': 1.5170860288242638e-06, 'epoch': 2.49}\n",
      "{'loss': 0.6034, 'grad_norm': 11.726852416992188, 'learning_rate': 1.4808059116167306e-06, 'epoch': 2.5}\n",
      "{'loss': 1.0618, 'grad_norm': 5.666452407836914, 'learning_rate': 1.4449301531681226e-06, 'epoch': 2.51}\n",
      "{'loss': 1.0582, 'grad_norm': 5.406843185424805, 'learning_rate': 1.409460456301147e-06, 'epoch': 2.51}\n",
      "{'loss': 0.9051, 'grad_norm': 4.458008766174316, 'learning_rate': 1.3743985045650366e-06, 'epoch': 2.52}\n",
      "{'loss': 1.0689, 'grad_norm': 6.046264171600342, 'learning_rate': 1.339745962155613e-06, 'epoch': 2.52}\n",
      "{'loss': 1.1426, 'grad_norm': 6.571690559387207, 'learning_rate': 1.305504473836331e-06, 'epoch': 2.53}\n",
      "{'loss': 1.2354, 'grad_norm': 7.4629716873168945, 'learning_rate': 1.2716756648601857e-06, 'epoch': 2.54}\n",
      "{'loss': 1.1809, 'grad_norm': 7.25618839263916, 'learning_rate': 1.2382611408925793e-06, 'epoch': 2.54}\n",
      "{'loss': 1.0842, 'grad_norm': 7.973904609680176, 'learning_rate': 1.2052624879351105e-06, 'epoch': 2.55}\n",
      "{'loss': 0.8519, 'grad_norm': 7.424454212188721, 'learning_rate': 1.1726812722502945e-06, 'epoch': 2.56}\n",
      "{'loss': 0.9533, 'grad_norm': 15.503576278686523, 'learning_rate': 1.1405190402872201e-06, 'epoch': 2.56}\n",
      "{'loss': 0.9203, 'grad_norm': 7.594271659851074, 'learning_rate': 1.1087773186081474e-06, 'epoch': 2.57}\n",
      "{'loss': 0.9845, 'grad_norm': 7.156661033630371, 'learning_rate': 1.0774576138160596e-06, 'epoch': 2.58}\n",
      "{'loss': 0.7912, 'grad_norm': 6.816806793212891, 'learning_rate': 1.0465614124831381e-06, 'epoch': 2.58}\n",
      "{'loss': 0.9014, 'grad_norm': 7.793581008911133, 'learning_rate': 1.0160901810802114e-06, 'epoch': 2.59}\n",
      "{'loss': 0.875, 'grad_norm': 8.475143432617188, 'learning_rate': 9.860453659071533e-07, 'epoch': 2.6}\n",
      "{'loss': 0.7245, 'grad_norm': 6.042415618896484, 'learning_rate': 9.564283930242258e-07, 'epoch': 2.6}\n",
      "{'loss': 0.825, 'grad_norm': 8.034272193908691, 'learning_rate': 9.272406681844015e-07, 'epoch': 2.61}\n",
      "{'loss': 0.6646, 'grad_norm': 8.682022094726562, 'learning_rate': 8.984835767666311e-07, 'epoch': 2.61}\n",
      "{'loss': 0.7677, 'grad_norm': 7.516270160675049, 'learning_rate': 8.701584837101018e-07, 'epoch': 2.62}\n",
      "{'loss': 0.8054, 'grad_norm': 7.55564546585083, 'learning_rate': 8.42266733449425e-07, 'epoch': 2.63}\n",
      "{'loss': 0.7839, 'grad_norm': 7.954898834228516, 'learning_rate': 8.148096498508573e-07, 'epoch': 2.63}\n",
      "{'loss': 0.6722, 'grad_norm': 7.118888854980469, 'learning_rate': 7.877885361494353e-07, 'epoch': 2.64}\n",
      "{'loss': 0.7764, 'grad_norm': 7.420288562774658, 'learning_rate': 7.612046748871327e-07, 'epoch': 2.65}\n",
      "{'loss': 0.8948, 'grad_norm': 14.83134651184082, 'learning_rate': 7.350593278519824e-07, 'epoch': 2.65}\n",
      "{'loss': 0.7272, 'grad_norm': 7.6879401206970215, 'learning_rate': 7.093537360181868e-07, 'epoch': 2.66}\n",
      "{'loss': 0.8415, 'grad_norm': 8.068614959716797, 'learning_rate': 6.840891194872112e-07, 'epoch': 2.67}\n",
      "{'loss': 0.8897, 'grad_norm': 7.589296340942383, 'learning_rate': 6.592666774298783e-07, 'epoch': 2.67}\n",
      "{'loss': 0.8963, 'grad_norm': 8.689851760864258, 'learning_rate': 6.348875880294536e-07, 'epoch': 2.68}\n",
      "{'loss': 0.9381, 'grad_norm': 8.193085670471191, 'learning_rate': 6.109530084257043e-07, 'epoch': 2.68}\n",
      "{'loss': 0.8064, 'grad_norm': 8.02147388458252, 'learning_rate': 5.874640746600047e-07, 'epoch': 2.69}\n",
      "{'loss': 0.858, 'grad_norm': 9.212872505187988, 'learning_rate': 5.644219016213903e-07, 'epoch': 2.7}\n",
      "{'loss': 0.7896, 'grad_norm': 12.351015090942383, 'learning_rate': 5.418275829936537e-07, 'epoch': 2.7}\n",
      "{'loss': 0.9588, 'grad_norm': 10.973462104797363, 'learning_rate': 5.196821912034277e-07, 'epoch': 2.71}\n",
      "{'loss': 0.808, 'grad_norm': 12.699771881103516, 'learning_rate': 4.979867773692881e-07, 'epoch': 2.72}\n",
      "{'loss': 0.8922, 'grad_norm': 11.611869812011719, 'learning_rate': 4.7674237125185597e-07, 'epoch': 2.72}\n",
      "{'loss': 0.7154, 'grad_norm': 8.7600679397583, 'learning_rate': 4.5594998120492505e-07, 'epoch': 2.73}\n",
      "{'loss': 0.7222, 'grad_norm': 12.57277774810791, 'learning_rate': 4.3561059412760674e-07, 'epoch': 2.74}\n",
      "{'loss': 0.8158, 'grad_norm': 10.514628410339355, 'learning_rate': 4.1572517541747294e-07, 'epoch': 2.74}\n",
      "{'loss': 0.7695, 'grad_norm': 12.464926719665527, 'learning_rate': 3.962946689247471e-07, 'epoch': 2.75}\n",
      "{'loss': 1.1257, 'grad_norm': 5.395938396453857, 'learning_rate': 3.773199969074959e-07, 'epoch': 2.76}\n",
      "{'loss': 1.051, 'grad_norm': 6.184567928314209, 'learning_rate': 3.588020599878639e-07, 'epoch': 2.76}\n",
      "{'loss': 1.1702, 'grad_norm': 5.432119369506836, 'learning_rate': 3.4074173710931804e-07, 'epoch': 2.77}\n",
      "{'loss': 1.1024, 'grad_norm': 6.333852767944336, 'learning_rate': 3.231398854949297e-07, 'epoch': 2.77}\n",
      "{'loss': 1.0342, 'grad_norm': 5.743387222290039, 'learning_rate': 3.059973406066963e-07, 'epoch': 2.78}\n",
      "{'loss': 1.0137, 'grad_norm': 5.922595024108887, 'learning_rate': 2.893149161058717e-07, 'epoch': 2.79}\n",
      "{'loss': 1.2901, 'grad_norm': 8.231369972229004, 'learning_rate': 2.730934038143607e-07, 'epoch': 2.79}\n",
      "{'loss': 1.0011, 'grad_norm': 6.729804515838623, 'learning_rate': 2.573335736771254e-07, 'epoch': 2.8}\n",
      "{'loss': 1.2073, 'grad_norm': 6.58375883102417, 'learning_rate': 2.420361737256438e-07, 'epoch': 2.81}\n",
      "{'loss': 0.7675, 'grad_norm': 7.3036885261535645, 'learning_rate': 2.2720193004240775e-07, 'epoch': 2.81}\n",
      "{'loss': 1.0139, 'grad_norm': 7.129098415374756, 'learning_rate': 2.1283154672645522e-07, 'epoch': 2.82}\n",
      "{'loss': 0.9124, 'grad_norm': 6.061060905456543, 'learning_rate': 1.9892570585995362e-07, 'epoch': 2.83}\n",
      "{'loss': 0.9436, 'grad_norm': 7.057581901550293, 'learning_rate': 1.854850674758213e-07, 'epoch': 2.83}\n",
      "{'loss': 0.8423, 'grad_norm': 6.367761611938477, 'learning_rate': 1.7251026952640583e-07, 'epoch': 2.84}\n",
      "{'loss': 0.8391, 'grad_norm': 6.949166774749756, 'learning_rate': 1.6000192785320057e-07, 'epoch': 2.84}\n",
      "{'loss': 0.8323, 'grad_norm': 6.627246856689453, 'learning_rate': 1.4796063615760913e-07, 'epoch': 2.85}\n",
      "{'loss': 0.7842, 'grad_norm': 10.154516220092773, 'learning_rate': 1.3638696597277678e-07, 'epoch': 2.86}\n",
      "{'loss': 0.7366, 'grad_norm': 6.833240509033203, 'learning_rate': 1.2528146663645102e-07, 'epoch': 2.86}\n",
      "{'loss': 0.7594, 'grad_norm': 7.539350509643555, 'learning_rate': 1.1464466526491691e-07, 'epoch': 2.87}\n",
      "{'loss': 0.8424, 'grad_norm': 7.551751136779785, 'learning_rate': 1.0447706672797264e-07, 'epoch': 2.88}\n",
      "{'loss': 0.8177, 'grad_norm': 7.526346206665039, 'learning_rate': 9.47791536249676e-08, 'epoch': 2.88}\n",
      "{'loss': 0.8378, 'grad_norm': 6.851788520812988, 'learning_rate': 8.555138626189619e-08, 'epoch': 2.89}\n",
      "{'loss': 0.7241, 'grad_norm': 6.90325927734375, 'learning_rate': 7.679420262954984e-08, 'epoch': 2.9}\n",
      "{'loss': 0.8146, 'grad_norm': 7.2509989738464355, 'learning_rate': 6.850801838272692e-08, 'epoch': 2.9}\n",
      "{'loss': 0.8102, 'grad_norm': 7.469784736633301, 'learning_rate': 6.069322682050516e-08, 'epoch': 2.91}\n",
      "{'loss': 0.58, 'grad_norm': 6.813799858093262, 'learning_rate': 5.3350198867574424e-08, 'epoch': 2.92}\n",
      "{'loss': 0.8609, 'grad_norm': 7.565710067749023, 'learning_rate': 4.647928305662852e-08, 'epoch': 2.92}\n",
      "{'loss': 0.7778, 'grad_norm': 9.452301979064941, 'learning_rate': 4.0080805511824027e-08, 'epoch': 2.93}\n",
      "{'loss': 0.9297, 'grad_norm': 9.113537788391113, 'learning_rate': 3.4155069933301535e-08, 'epoch': 2.93}\n",
      "{'loss': 0.8836, 'grad_norm': 8.653704643249512, 'learning_rate': 2.8702357582769404e-08, 'epoch': 2.94}\n",
      "{'loss': 0.9115, 'grad_norm': 9.297487258911133, 'learning_rate': 2.372292727015557e-08, 'epoch': 2.95}\n",
      "{'loss': 0.8518, 'grad_norm': 7.730424404144287, 'learning_rate': 1.9217015341318478e-08, 'epoch': 2.95}\n",
      "{'loss': 0.9726, 'grad_norm': 9.056127548217773, 'learning_rate': 1.518483566683826e-08, 'epoch': 2.96}\n",
      "{'loss': 0.6649, 'grad_norm': 9.437285423278809, 'learning_rate': 1.1626579631853763e-08, 'epoch': 2.97}\n",
      "{'loss': 0.686, 'grad_norm': 8.270322799682617, 'learning_rate': 8.542416126989805e-09, 'epoch': 2.97}\n",
      "{'loss': 0.7293, 'grad_norm': 11.215616226196289, 'learning_rate': 5.9324915403324855e-09, 'epoch': 2.98}\n",
      "{'loss': 0.6814, 'grad_norm': 12.81429672241211, 'learning_rate': 3.7969297504858445e-09, 'epoch': 2.99}\n",
      "{'loss': 0.6792, 'grad_norm': 10.104758262634277, 'learning_rate': 2.1358321206899067e-09, 'epoch': 2.99}\n",
      "{'loss': 0.7204, 'grad_norm': 11.180538177490234, 'learning_rate': 9.49277494008971e-10, 'epoch': 3.0}\n",
      "{'loss': 0.7348, 'grad_norm': 15.469114303588867, 'learning_rate': 2.3732218959349186e-10, 'epoch': 3.0}\n",
      "{'train_runtime': 583.7791, 'train_samples_per_second': 25.695, 'train_steps_per_second': 0.807, 'train_loss': 1.1379450421677526, 'epoch': 3.0}\n",
      "100% 471/471 [09:42<00:00,  1.24s/it]\n",
      "*** Trainer State & Trained Model Saved To --> res/s2l-pythia-410m_cirriculum_s2l/output/ ***\n",
      "*** Trainer State & Trained Model Save-Pretrained To --> res/s2l-pythia-410m_cirriculum_s2l/output//pretrained ***\n",
      "*** Training Done!\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mdazzling-star-12\u001b[0m at: \u001b[34m\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251129_024144-du3u9xzq/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train.py --config_file configs/s2l/pythia-410m_cirriculum_s2l.yml --wandb_key \"0944191bcf43ea6231189f995e76d66cc523c13d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01d13935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-29 01:59:03.523089: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 01:59:03.540483: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764381543.561521   10762 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764381543.567942   10762 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764381543.584288   10762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764381543.584312   10762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764381543.584315   10762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764381543.584318   10762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 01:59:03.589058: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhenryliu999\u001b[0m (\u001b[33mhenryliu999-other\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-410m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-410m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "ref_model_path: null\n",
      "n_components: -1\n",
      "num_loss_ckpts: -1\n",
      "distance: euclidean\n",
      "seed: 42\n",
      "\n",
      "config.json: 100% 570/570 [00:00<00:00, 5.22MB/s]\n",
      "model.safetensors: 100% 911M/911M [00:02<00:00, 305MB/s]     \n",
      "*** Model initialized!\n",
      "tokenizer_config.json: 100% 396/396 [00:00<00:00, 4.11MB/s]\n",
      "tokenizer.json: 2.11MB [00:00, 149MB/s]\n",
      "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 959kB/s]\n",
      "*** Tokenizer initialized!\n",
      "*** Smart tokenizer and embedding resize done!\n",
      "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n",
      "*** Schedule built!\n",
      "*** labeled_idx: tensor([   0,    1,    2,  ..., 9997, 9998, 9999])\n",
      "*** jdump(labeled_data_json_format, labeled_data_path) SUCESSFUL to --> res/pythia-410m_cirriculum_full/data/labeled.json\n",
      "*** jdump(unlabeled_data_json_format, unlabeled_data_path) SUCESSFUL to --> res/pythia-410m_cirriculum_full/data/unlabeled.json\n",
      "*** Training-Data-Size = 10000\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "*** SANITY-CHECK: Training-Sample#1. - TEXT.:\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "The distance between two stars is 6.52 √ó 10^5 light years. What is the distance between the two stars in parsecs? (1 parsec = 3.26 light years)\n",
      "Answer Choices: (A) 2 √ó 10^5 (B) 4 √ó 10^6 (C) 5 √ó 10^7 (D) 7 √ó 10^7 (E) 9 √ó 10^8\n",
      "\n",
      "### Response:Let's think about the multi-choice question.\n",
      "6.52 √ó 10^5 ly / (3.26 ly/parsec) = 2 x 10^5 persec\n",
      "The answer is A.</s>\n",
      "\n",
      "\n",
      "/content/S2L_Cirriculum/schedule_base.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=self.model,\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 50278, 'bos_token_id': 50279, 'pad_token_id': 50277}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m setting up run gqyllnen (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run gqyllnen (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m setting up run gqyllnen (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/S2L_Cirriculum/wandb/run-20251129_015943-gqyllnen\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdesert-grass-11\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/henryliu999-other/S2L_Cirriculum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/henryliu999-other/S2L_Cirriculum/runs/gqyllnen\u001b[0m\n",
      "{'loss': 1.2472, 'grad_norm': 27.093629837036133, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'loss': 1.2081, 'grad_norm': 23.974123001098633, 'learning_rate': 6.896551724137931e-07, 'epoch': 0.01}\n",
      "{'loss': 1.22, 'grad_norm': 24.11684226989746, 'learning_rate': 1.3793103448275862e-06, 'epoch': 0.01}\n",
      "{'loss': 1.1363, 'grad_norm': 28.3900089263916, 'learning_rate': 2.0689655172413796e-06, 'epoch': 0.01}\n",
      "{'loss': 1.2741, 'grad_norm': 25.70219612121582, 'learning_rate': 2.7586206896551725e-06, 'epoch': 0.02}\n",
      "{'loss': 1.4431, 'grad_norm': 24.49167823791504, 'learning_rate': 3.448275862068966e-06, 'epoch': 0.02}\n",
      "{'loss': 1.4507, 'grad_norm': 29.916536331176758, 'learning_rate': 4.137931034482759e-06, 'epoch': 0.02}\n",
      "{'loss': 1.6217, 'grad_norm': 35.59943389892578, 'learning_rate': 4.8275862068965525e-06, 'epoch': 0.03}\n",
      "{'loss': 1.5333, 'grad_norm': 36.50762176513672, 'learning_rate': 5.517241379310345e-06, 'epoch': 0.03}\n",
      "{'loss': 1.7814, 'grad_norm': 36.73701477050781, 'learning_rate': 6.206896551724138e-06, 'epoch': 0.03}\n",
      "{'loss': 1.8317, 'grad_norm': 32.750404357910156, 'learning_rate': 6.896551724137932e-06, 'epoch': 0.04}\n",
      "{'loss': 1.8169, 'grad_norm': 44.61578369140625, 'learning_rate': 7.586206896551724e-06, 'epoch': 0.04}\n",
      "{'loss': 1.9803, 'grad_norm': 48.533226013183594, 'learning_rate': 8.275862068965518e-06, 'epoch': 0.04}\n",
      "{'loss': 1.7376, 'grad_norm': 46.38938903808594, 'learning_rate': 8.965517241379312e-06, 'epoch': 0.04}\n",
      "{'loss': 2.0411, 'grad_norm': 44.195213317871094, 'learning_rate': 9.655172413793105e-06, 'epoch': 0.05}\n",
      "{'loss': 1.7772, 'grad_norm': 45.16423416137695, 'learning_rate': 1.0344827586206898e-05, 'epoch': 0.05}\n",
      "{'loss': 1.7579, 'grad_norm': 50.288307189941406, 'learning_rate': 1.103448275862069e-05, 'epoch': 0.05}\n",
      "{'loss': 1.9129, 'grad_norm': 57.00651931762695, 'learning_rate': 1.1724137931034483e-05, 'epoch': 0.06}\n",
      "{'loss': 2.0604, 'grad_norm': 45.57039260864258, 'learning_rate': 1.2413793103448277e-05, 'epoch': 0.06}\n",
      "{'loss': 1.8913, 'grad_norm': 45.67634963989258, 'learning_rate': 1.310344827586207e-05, 'epoch': 0.06}\n",
      "{'loss': 2.0612, 'grad_norm': 45.58883285522461, 'learning_rate': 1.3793103448275863e-05, 'epoch': 0.07}\n",
      "{'loss': 1.8672, 'grad_norm': 59.32292556762695, 'learning_rate': 1.4482758620689657e-05, 'epoch': 0.07}\n",
      "{'loss': 1.7731, 'grad_norm': 49.42411422729492, 'learning_rate': 1.5172413793103448e-05, 'epoch': 0.07}\n",
      "{'loss': 2.0168, 'grad_norm': 43.94362258911133, 'learning_rate': 1.586206896551724e-05, 'epoch': 0.08}\n",
      "{'loss': 1.8793, 'grad_norm': 70.2939682006836, 'learning_rate': 1.6551724137931037e-05, 'epoch': 0.08}\n",
      "{'loss': 1.8965, 'grad_norm': 40.69740676879883, 'learning_rate': 1.7241379310344828e-05, 'epoch': 0.08}\n",
      "{'loss': 1.8837, 'grad_norm': 38.41814422607422, 'learning_rate': 1.7931034482758623e-05, 'epoch': 0.09}\n",
      "{'loss': 1.7947, 'grad_norm': 42.52568817138672, 'learning_rate': 1.8620689655172415e-05, 'epoch': 0.09}\n",
      "{'loss': 1.9409, 'grad_norm': 35.755374908447266, 'learning_rate': 1.931034482758621e-05, 'epoch': 0.09}\n",
      "{'loss': 1.9241, 'grad_norm': 36.008453369140625, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 1.867, 'grad_norm': 36.36589813232422, 'learning_rate': 1.9999940408195878e-05, 'epoch': 0.1}\n",
      "{'loss': 1.9899, 'grad_norm': 43.089115142822266, 'learning_rate': 1.9999761633493754e-05, 'epoch': 0.1}\n",
      "{'loss': 1.9351, 'grad_norm': 40.21225357055664, 'learning_rate': 1.9999463678024317e-05, 'epoch': 0.11}\n",
      "{'loss': 2.0359, 'grad_norm': 37.954627990722656, 'learning_rate': 1.999904654533872e-05, 'epoch': 0.11}\n",
      "{'loss': 2.0459, 'grad_norm': 50.3984375, 'learning_rate': 1.9998510240408495e-05, 'epoch': 0.11}\n",
      "{'loss': 1.8648, 'grad_norm': 33.39844512939453, 'learning_rate': 1.999785476962552e-05, 'epoch': 0.12}\n",
      "{'loss': 2.0091, 'grad_norm': 24.34857177734375, 'learning_rate': 1.9997080140801932e-05, 'epoch': 0.12}\n",
      "{'loss': 1.9127, 'grad_norm': 25.62027359008789, 'learning_rate': 1.9996186363170037e-05, 'epoch': 0.12}\n",
      "{'loss': 1.7761, 'grad_norm': 24.467479705810547, 'learning_rate': 1.9995173447382193e-05, 'epoch': 0.12}\n",
      "{'loss': 1.947, 'grad_norm': 27.450965881347656, 'learning_rate': 1.9994041405510705e-05, 'epoch': 0.13}\n",
      "{'loss': 1.9163, 'grad_norm': 30.006792068481445, 'learning_rate': 1.9992790251047655e-05, 'epoch': 0.13}\n",
      "{'loss': 1.9147, 'grad_norm': 26.8416748046875, 'learning_rate': 1.999141999890475e-05, 'epoch': 0.13}\n",
      "{'loss': 2.0408, 'grad_norm': 21.908161163330078, 'learning_rate': 1.9989930665413148e-05, 'epoch': 0.14}\n",
      "{'loss': 1.7333, 'grad_norm': 20.014816284179688, 'learning_rate': 1.998832226832327e-05, 'epoch': 0.14}\n",
      "{'loss': 1.5443, 'grad_norm': 24.270498275756836, 'learning_rate': 1.9986594826804563e-05, 'epoch': 0.14}\n",
      "{'loss': 1.5451, 'grad_norm': 21.0932559967041, 'learning_rate': 1.9984748361445306e-05, 'epoch': 0.15}\n",
      "{'loss': 1.6946, 'grad_norm': 20.9068660736084, 'learning_rate': 1.998278289425234e-05, 'epoch': 0.15}\n",
      "{'loss': 1.601, 'grad_norm': 21.56127166748047, 'learning_rate': 1.9980698448650805e-05, 'epoch': 0.15}\n",
      "{'loss': 1.689, 'grad_norm': 24.062137603759766, 'learning_rate': 1.9978495049483883e-05, 'epoch': 0.16}\n",
      "{'loss': 1.8061, 'grad_norm': 26.863723754882812, 'learning_rate': 1.997617272301248e-05, 'epoch': 0.16}\n",
      "{'loss': 1.443, 'grad_norm': 16.612043380737305, 'learning_rate': 1.9973731496914914e-05, 'epoch': 0.16}\n",
      "{'loss': 1.4136, 'grad_norm': 20.846939086914062, 'learning_rate': 1.9971171400286602e-05, 'epoch': 0.17}\n",
      "{'loss': 1.4132, 'grad_norm': 20.072912216186523, 'learning_rate': 1.9968492463639704e-05, 'epoch': 0.17}\n",
      "{'loss': 1.4423, 'grad_norm': 16.229339599609375, 'learning_rate': 1.9965694718902745e-05, 'epoch': 0.17}\n",
      "{'loss': 1.3944, 'grad_norm': 20.39926528930664, 'learning_rate': 1.9962778199420265e-05, 'epoch': 0.18}\n",
      "{'loss': 1.4645, 'grad_norm': 21.067167282104492, 'learning_rate': 1.9959742939952393e-05, 'epoch': 0.18}\n",
      "{'loss': 1.3919, 'grad_norm': 26.00871467590332, 'learning_rate': 1.9956588976674442e-05, 'epoch': 0.18}\n",
      "{'loss': 1.6805, 'grad_norm': 24.2702693939209, 'learning_rate': 1.995331634717649e-05, 'epoch': 0.19}\n",
      "{'loss': 1.6751, 'grad_norm': 20.65357780456543, 'learning_rate': 1.994992509046291e-05, 'epoch': 0.19}\n",
      "{'loss': 1.6316, 'grad_norm': 20.87973403930664, 'learning_rate': 1.9946415246951928e-05, 'epoch': 0.19}\n",
      "{'loss': 1.9241, 'grad_norm': 25.716005325317383, 'learning_rate': 1.9942786858475126e-05, 'epoch': 0.2}\n",
      "{'loss': 1.801, 'grad_norm': 20.62413787841797, 'learning_rate': 1.9939039968276942e-05, 'epoch': 0.2}\n",
      "{'loss': 1.765, 'grad_norm': 18.11965560913086, 'learning_rate': 1.9935174621014173e-05, 'epoch': 0.2}\n",
      "{'loss': 1.6294, 'grad_norm': 17.262256622314453, 'learning_rate': 1.9931190862755416e-05, 'epoch': 0.2}\n",
      "{'loss': 1.6383, 'grad_norm': 21.299945831298828, 'learning_rate': 1.992708874098054e-05, 'epoch': 0.21}\n",
      "{'loss': 1.705, 'grad_norm': 15.038808822631836, 'learning_rate': 1.992286830458012e-05, 'epoch': 0.21}\n",
      "{'loss': 1.7374, 'grad_norm': 13.73176383972168, 'learning_rate': 1.9918529603854825e-05, 'epoch': 0.21}\n",
      "{'loss': 1.6257, 'grad_norm': 15.316874504089355, 'learning_rate': 1.991407269051487e-05, 'epoch': 0.22}\n",
      "{'loss': 1.8311, 'grad_norm': 14.757955551147461, 'learning_rate': 1.990949761767935e-05, 'epoch': 0.22}\n",
      "{'loss': 1.804, 'grad_norm': 14.52001667022705, 'learning_rate': 1.9904804439875635e-05, 'epoch': 0.22}\n",
      "{'loss': 1.8053, 'grad_norm': 12.967301368713379, 'learning_rate': 1.989999321303871e-05, 'epoch': 0.23}\n",
      "{'loss': 1.6424, 'grad_norm': 13.093436241149902, 'learning_rate': 1.9895063994510512e-05, 'epoch': 0.23}\n",
      "{'loss': 1.7537, 'grad_norm': 11.529438018798828, 'learning_rate': 1.989001684303925e-05, 'epoch': 0.23}\n",
      "{'loss': 1.3976, 'grad_norm': 11.578262329101562, 'learning_rate': 1.9884851818778695e-05, 'epoch': 0.24}\n",
      "{'loss': 1.5692, 'grad_norm': 12.580682754516602, 'learning_rate': 1.9879568983287468e-05, 'epoch': 0.24}\n",
      "{'loss': 1.8562, 'grad_norm': 16.578916549682617, 'learning_rate': 1.9874168399528307e-05, 'epoch': 0.24}\n",
      "{'loss': 1.4611, 'grad_norm': 12.755864143371582, 'learning_rate': 1.986865013186732e-05, 'epoch': 0.25}\n",
      "{'loss': 1.7044, 'grad_norm': 12.787575721740723, 'learning_rate': 1.9863014246073216e-05, 'epoch': 0.25}\n",
      "{'loss': 1.7242, 'grad_norm': 13.635358810424805, 'learning_rate': 1.985726080931651e-05, 'epoch': 0.25}\n",
      "{'loss': 1.4876, 'grad_norm': 13.482460975646973, 'learning_rate': 1.9851389890168738e-05, 'epoch': 0.26}\n",
      "{'loss': 1.7547, 'grad_norm': 14.414863586425781, 'learning_rate': 1.9845401558601634e-05, 'epoch': 0.26}\n",
      "{'loss': 1.6772, 'grad_norm': 12.274845123291016, 'learning_rate': 1.98392958859863e-05, 'epoch': 0.26}\n",
      "{'loss': 1.8035, 'grad_norm': 13.015154838562012, 'learning_rate': 1.9833072945092334e-05, 'epoch': 0.27}\n",
      "{'loss': 1.6745, 'grad_norm': 14.658114433288574, 'learning_rate': 1.9826732810087e-05, 'epoch': 0.27}\n",
      "{'loss': 1.5794, 'grad_norm': 19.643415451049805, 'learning_rate': 1.9820275556534306e-05, 'epoch': 0.27}\n",
      "{'loss': 1.5746, 'grad_norm': 15.9939546585083, 'learning_rate': 1.9813701261394136e-05, 'epoch': 0.28}\n",
      "{'loss': 1.6763, 'grad_norm': 17.175024032592773, 'learning_rate': 1.980701000302131e-05, 'epoch': 0.28}\n",
      "{'loss': 1.4039, 'grad_norm': 15.698506355285645, 'learning_rate': 1.9800201861164665e-05, 'epoch': 0.28}\n",
      "{'loss': 1.8296, 'grad_norm': 21.75229263305664, 'learning_rate': 1.979327691696608e-05, 'epoch': 0.28}\n",
      "{'loss': 1.8628, 'grad_norm': 18.570068359375, 'learning_rate': 1.9786235252959555e-05, 'epoch': 0.29}\n",
      "{'loss': 1.6897, 'grad_norm': 18.816940307617188, 'learning_rate': 1.977907695307017e-05, 'epoch': 0.29}\n",
      "{'loss': 1.4192, 'grad_norm': 20.119678497314453, 'learning_rate': 1.9771802102613127e-05, 'epoch': 0.29}\n",
      "{'loss': 1.4279, 'grad_norm': 16.95323371887207, 'learning_rate': 1.9764410788292724e-05, 'epoch': 0.3}\n",
      "{'loss': 1.8306, 'grad_norm': 18.39238739013672, 'learning_rate': 1.975690309820131e-05, 'epoch': 0.3}\n",
      "{'loss': 1.4472, 'grad_norm': 17.113460540771484, 'learning_rate': 1.9749279121818235e-05, 'epoch': 0.3}\n",
      "{'loss': 1.5866, 'grad_norm': 20.86079978942871, 'learning_rate': 1.9741538950008817e-05, 'epoch': 0.31}\n",
      "{'loss': 1.2503, 'grad_norm': 21.535938262939453, 'learning_rate': 1.9733682675023207e-05, 'epoch': 0.31}\n",
      "{'loss': 1.3255, 'grad_norm': 22.10923194885254, 'learning_rate': 1.972571039049533e-05, 'epoch': 0.31}\n",
      "{'loss': 1.378, 'grad_norm': 23.876312255859375, 'learning_rate': 1.971762219144174e-05, 'epoch': 0.32}\n",
      "{'loss': 1.3477, 'grad_norm': 25.30373764038086, 'learning_rate': 1.9709418174260523e-05, 'epoch': 0.32}\n",
      "{'loss': 1.3934, 'grad_norm': 17.125038146972656, 'learning_rate': 1.9701098436730108e-05, 'epoch': 0.32}\n",
      "{'loss': 1.4295, 'grad_norm': 13.062219619750977, 'learning_rate': 1.969266307800813e-05, 'epoch': 0.33}\n",
      "{'loss': 1.5503, 'grad_norm': 13.880707740783691, 'learning_rate': 1.9684112198630246e-05, 'epoch': 0.33}\n",
      "{'loss': 1.2913, 'grad_norm': 12.007887840270996, 'learning_rate': 1.967544590050891e-05, 'epoch': 0.33}\n",
      "{'loss': 1.3127, 'grad_norm': 18.465871810913086, 'learning_rate': 1.9666664286932198e-05, 'epoch': 0.34}\n",
      "{'loss': 1.4689, 'grad_norm': 12.096962928771973, 'learning_rate': 1.9657767462562544e-05, 'epoch': 0.34}\n",
      "{'loss': 1.4099, 'grad_norm': 13.357730865478516, 'learning_rate': 1.9648755533435517e-05, 'epoch': 0.34}\n",
      "{'loss': 1.5011, 'grad_norm': 15.162436485290527, 'learning_rate': 1.9639628606958535e-05, 'epoch': 0.35}\n",
      "{'loss': 1.5833, 'grad_norm': 19.13787841796875, 'learning_rate': 1.96303867919096e-05, 'epoch': 0.35}\n",
      "{'loss': 1.5551, 'grad_norm': 17.04464340209961, 'learning_rate': 1.9621030198436007e-05, 'epoch': 0.35}\n",
      "{'loss': 1.6566, 'grad_norm': 18.140151977539062, 'learning_rate': 1.9611558938053003e-05, 'epoch': 0.36}\n",
      "{'loss': 1.5886, 'grad_norm': 21.80881690979004, 'learning_rate': 1.9601973123642493e-05, 'epoch': 0.36}\n",
      "{'loss': 1.5334, 'grad_norm': 19.12769317626953, 'learning_rate': 1.9592272869451672e-05, 'epoch': 0.36}\n",
      "{'loss': 1.5815, 'grad_norm': 18.047870635986328, 'learning_rate': 1.9582458291091664e-05, 'epoch': 0.36}\n",
      "{'loss': 1.6387, 'grad_norm': 18.302736282348633, 'learning_rate': 1.957252950553616e-05, 'epoch': 0.37}\n",
      "{'loss': 1.8374, 'grad_norm': 18.79608917236328, 'learning_rate': 1.9562486631120007e-05, 'epoch': 0.37}\n",
      "{'loss': 1.8008, 'grad_norm': 20.335559844970703, 'learning_rate': 1.9552329787537805e-05, 'epoch': 0.37}\n",
      "{'loss': 1.7614, 'grad_norm': 19.745561599731445, 'learning_rate': 1.9542059095842484e-05, 'epoch': 0.38}\n",
      "{'loss': 1.4225, 'grad_norm': 15.808358192443848, 'learning_rate': 1.9531674678443853e-05, 'epoch': 0.38}\n",
      "{'loss': 1.5417, 'grad_norm': 22.7427921295166, 'learning_rate': 1.952117665910714e-05, 'epoch': 0.38}\n",
      "{'loss': 1.733, 'grad_norm': 13.003190040588379, 'learning_rate': 1.9510565162951538e-05, 'epoch': 0.39}\n",
      "{'loss': 1.6665, 'grad_norm': 12.241786003112793, 'learning_rate': 1.9499840316448675e-05, 'epoch': 0.39}\n",
      "{'loss': 1.6168, 'grad_norm': 20.661705017089844, 'learning_rate': 1.948900224742115e-05, 'epoch': 0.39}\n",
      "{'loss': 1.58, 'grad_norm': 15.433815956115723, 'learning_rate': 1.9478051085040978e-05, 'epoch': 0.4}\n",
      "{'loss': 1.4736, 'grad_norm': 13.177715301513672, 'learning_rate': 1.9466986959828063e-05, 'epoch': 0.4}\n",
      "{'loss': 1.6309, 'grad_norm': 25.670074462890625, 'learning_rate': 1.945581000364864e-05, 'epoch': 0.4}\n",
      "{'loss': 1.2995, 'grad_norm': 12.844149589538574, 'learning_rate': 1.9444520349713705e-05, 'epoch': 0.41}\n",
      "{'loss': 1.5245, 'grad_norm': 14.913558006286621, 'learning_rate': 1.9433118132577432e-05, 'epoch': 0.41}\n",
      "{'loss': 1.5237, 'grad_norm': 15.739096641540527, 'learning_rate': 1.942160348813556e-05, 'epoch': 0.41}\n",
      "{'loss': 1.5904, 'grad_norm': 20.873119354248047, 'learning_rate': 1.9409976553623767e-05, 'epoch': 0.42}\n",
      "{'loss': 1.4056, 'grad_norm': 16.71555519104004, 'learning_rate': 1.9398237467616063e-05, 'epoch': 0.42}\n",
      "{'loss': 1.4371, 'grad_norm': 39.48033142089844, 'learning_rate': 1.9386386370023104e-05, 'epoch': 0.42}\n",
      "{'loss': 1.4593, 'grad_norm': 17.456567764282227, 'learning_rate': 1.9374423402090553e-05, 'epoch': 0.43}\n",
      "{'loss': 1.8715, 'grad_norm': 24.319623947143555, 'learning_rate': 1.9362348706397374e-05, 'epoch': 0.43}\n",
      "{'loss': 1.6453, 'grad_norm': 16.798873901367188, 'learning_rate': 1.9350162426854152e-05, 'epoch': 0.43}\n",
      "{'loss': 1.5518, 'grad_norm': 15.375784873962402, 'learning_rate': 1.933786470870136e-05, 'epoch': 0.44}\n",
      "{'loss': 1.5859, 'grad_norm': 16.91322135925293, 'learning_rate': 1.9325455698507638e-05, 'epoch': 0.44}\n",
      "{'loss': 1.808, 'grad_norm': 16.045696258544922, 'learning_rate': 1.931293554416805e-05, 'epoch': 0.44}\n",
      "{'loss': 1.4652, 'grad_norm': 14.647836685180664, 'learning_rate': 1.9300304394902315e-05, 'epoch': 0.44}\n",
      "{'loss': 1.4546, 'grad_norm': 13.989152908325195, 'learning_rate': 1.9287562401253023e-05, 'epoch': 0.45}\n",
      "{'loss': 1.3995, 'grad_norm': 14.446857452392578, 'learning_rate': 1.927470971508386e-05, 'epoch': 0.45}\n",
      "{'loss': 1.733, 'grad_norm': 16.395973205566406, 'learning_rate': 1.9261746489577767e-05, 'epoch': 0.45}\n",
      "{'loss': 1.5823, 'grad_norm': 19.482967376708984, 'learning_rate': 1.924867287923515e-05, 'epoch': 0.46}\n",
      "{'loss': 1.3561, 'grad_norm': 15.238972663879395, 'learning_rate': 1.923548903987201e-05, 'epoch': 0.46}\n",
      "{'loss': 1.3925, 'grad_norm': 15.660013198852539, 'learning_rate': 1.9222195128618108e-05, 'epoch': 0.46}\n",
      "{'loss': 1.4908, 'grad_norm': 17.950510025024414, 'learning_rate': 1.9208791303915063e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2559, 'grad_norm': 14.640763282775879, 'learning_rate': 1.919527772551451e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3033, 'grad_norm': 15.499478340148926, 'learning_rate': 1.918165455447614e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3515, 'grad_norm': 21.03371238708496, 'learning_rate': 1.9167921953165827e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4448, 'grad_norm': 19.76853370666504, 'learning_rate': 1.9154080085253665e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3855, 'grad_norm': 12.757416725158691, 'learning_rate': 1.9140129115712035e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3154, 'grad_norm': 11.6941499710083, 'learning_rate': 1.912606921081362e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3494, 'grad_norm': 7.861086845397949, 'learning_rate': 1.9111900538129443e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3664, 'grad_norm': 8.90381908416748, 'learning_rate': 1.909762326652686e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3136, 'grad_norm': 9.790143966674805, 'learning_rate': 1.908323756616754e-05, 'epoch': 0.5}\n",
      "{'loss': 1.4056, 'grad_norm': 9.91052532196045, 'learning_rate': 1.9068743608505454e-05, 'epoch': 0.5}\n",
      "{'loss': 1.4383, 'grad_norm': 9.199407577514648, 'learning_rate': 1.9054141566284822e-05, 'epoch': 0.5}\n",
      "{'loss': 1.5289, 'grad_norm': 9.841320991516113, 'learning_rate': 1.9039431613538047e-05, 'epoch': 0.51}\n",
      "{'loss': 1.7608, 'grad_norm': 9.316141128540039, 'learning_rate': 1.9024613925583652e-05, 'epoch': 0.51}\n",
      "{'loss': 1.5188, 'grad_norm': 9.939764976501465, 'learning_rate': 1.900968867902419e-05, 'epoch': 0.51}\n",
      "{'loss': 1.5971, 'grad_norm': 14.839086532592773, 'learning_rate': 1.899465605174414e-05, 'epoch': 0.52}\n",
      "{'loss': 1.5611, 'grad_norm': 11.374881744384766, 'learning_rate': 1.8979516222907776e-05, 'epoch': 0.52}\n",
      "{'loss': 1.4557, 'grad_norm': 11.129461288452148, 'learning_rate': 1.896426937295704e-05, 'epoch': 0.52}\n",
      "{'loss': 1.7324, 'grad_norm': 10.835678100585938, 'learning_rate': 1.8948915683609387e-05, 'epoch': 0.52}\n",
      "{'loss': 1.47, 'grad_norm': 11.885248184204102, 'learning_rate': 1.8933455337855633e-05, 'epoch': 0.53}\n",
      "{'loss': 1.6899, 'grad_norm': 14.869141578674316, 'learning_rate': 1.8917888519957756e-05, 'epoch': 0.53}\n",
      "{'loss': 1.4228, 'grad_norm': 13.889668464660645, 'learning_rate': 1.89022154154467e-05, 'epoch': 0.53}\n",
      "{'loss': 1.4155, 'grad_norm': 13.067256927490234, 'learning_rate': 1.8886436211120195e-05, 'epoch': 0.54}\n",
      "{'loss': 1.6763, 'grad_norm': 24.105684280395508, 'learning_rate': 1.8870551095040476e-05, 'epoch': 0.54}\n",
      "{'loss': 1.4015, 'grad_norm': 12.468908309936523, 'learning_rate': 1.8854560256532098e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3702, 'grad_norm': 13.502534866333008, 'learning_rate': 1.8838463886179647e-05, 'epoch': 0.55}\n",
      "{'loss': 1.5564, 'grad_norm': 12.89148998260498, 'learning_rate': 1.8822262175825463e-05, 'epoch': 0.55}\n",
      "{'loss': 1.6622, 'grad_norm': 14.484284400939941, 'learning_rate': 1.880595531856738e-05, 'epoch': 0.55}\n",
      "{'loss': 1.6374, 'grad_norm': 15.697340965270996, 'learning_rate': 1.878954350875641e-05, 'epoch': 0.56}\n",
      "{'loss': 1.5836, 'grad_norm': 12.995500564575195, 'learning_rate': 1.877302694199442e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4699, 'grad_norm': 13.729212760925293, 'learning_rate': 1.8756405815131815e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3725, 'grad_norm': 12.488597869873047, 'learning_rate': 1.873968032626518e-05, 'epoch': 0.57}\n",
      "{'loss': 1.5874, 'grad_norm': 14.96603012084961, 'learning_rate': 1.872285067473493e-05, 'epoch': 0.57}\n",
      "{'loss': 1.6212, 'grad_norm': 14.572916984558105, 'learning_rate': 1.8705917061122917e-05, 'epoch': 0.57}\n",
      "{'loss': 1.4576, 'grad_norm': 22.28921127319336, 'learning_rate': 1.8688879687250067e-05, 'epoch': 0.58}\n",
      "{'loss': 1.5906, 'grad_norm': 16.159669876098633, 'learning_rate': 1.8671738756173946e-05, 'epoch': 0.58}\n",
      "{'loss': 1.4841, 'grad_norm': 16.19417381286621, 'learning_rate': 1.8654494472186352e-05, 'epoch': 0.58}\n",
      "{'loss': 1.6276, 'grad_norm': 16.44759178161621, 'learning_rate': 1.8637147040810884e-05, 'epoch': 0.59}\n",
      "{'loss': 1.4358, 'grad_norm': 19.77128791809082, 'learning_rate': 1.8619696668800494e-05, 'epoch': 0.59}\n",
      "{'loss': 1.5383, 'grad_norm': 14.448455810546875, 'learning_rate': 1.860214356413501e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3162, 'grad_norm': 23.489238739013672, 'learning_rate': 1.8584487936018663e-05, 'epoch': 0.6}\n",
      "{'loss': 1.448, 'grad_norm': 19.420024871826172, 'learning_rate': 1.8566729994877604e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3784, 'grad_norm': 25.242063522338867, 'learning_rate': 1.854886995235738e-05, 'epoch': 0.6}\n",
      "{'loss': 1.4644, 'grad_norm': 19.619203567504883, 'learning_rate': 1.8530908021320427e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3717, 'grad_norm': 23.427133560180664, 'learning_rate': 1.8512844415843514e-05, 'epoch': 0.61}\n",
      "{'loss': 1.4323, 'grad_norm': 16.14582633972168, 'learning_rate': 1.8494679351215212e-05, 'epoch': 0.61}\n",
      "{'loss': 1.4501, 'grad_norm': 22.47583770751953, 'learning_rate': 1.8476413043933316e-05, 'epoch': 0.61}\n",
      "{'loss': 1.6888, 'grad_norm': 27.363021850585938, 'learning_rate': 1.8458045711702264e-05, 'epoch': 0.62}\n",
      "{'loss': 1.4837, 'grad_norm': 21.020875930786133, 'learning_rate': 1.8439577573430557e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1698, 'grad_norm': 20.638654708862305, 'learning_rate': 1.842100884922812e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3692, 'grad_norm': 34.46812057495117, 'learning_rate': 1.8402339760403715e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1618, 'grad_norm': 15.98904037475586, 'learning_rate': 1.8383570529462273e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3084, 'grad_norm': 15.090605735778809, 'learning_rate': 1.8364701380102267e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2729, 'grad_norm': 17.183277130126953, 'learning_rate': 1.834573253721303e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1129, 'grad_norm': 15.149637222290039, 'learning_rate': 1.8326664226872063e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3039, 'grad_norm': 18.097959518432617, 'learning_rate': 1.8307496676342384e-05, 'epoch': 0.64}\n",
      "{'loss': 1.299, 'grad_norm': 14.153168678283691, 'learning_rate': 1.828823011406977e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3257, 'grad_norm': 14.677268981933594, 'learning_rate': 1.8268864769680054e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2536, 'grad_norm': 15.212383270263672, 'learning_rate': 1.824940087397641e-05, 'epoch': 0.65}\n",
      "{'loss': 1.4408, 'grad_norm': 11.10699462890625, 'learning_rate': 1.8229838658936566e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3487, 'grad_norm': 9.653395652770996, 'learning_rate': 1.8210178357710057e-05, 'epoch': 0.66}\n",
      "{'loss': 1.338, 'grad_norm': 13.581986427307129, 'learning_rate': 1.819042020461545e-05, 'epoch': 0.66}\n",
      "{'loss': 1.5449, 'grad_norm': 12.763666152954102, 'learning_rate': 1.8170564435137542e-05, 'epoch': 0.67}\n",
      "{'loss': 1.4968, 'grad_norm': 10.951178550720215, 'learning_rate': 1.8150611285924556e-05, 'epoch': 0.67}\n",
      "{'loss': 1.6705, 'grad_norm': 13.469205856323242, 'learning_rate': 1.8130560994785325e-05, 'epoch': 0.67}\n",
      "{'loss': 1.5185, 'grad_norm': 13.843772888183594, 'learning_rate': 1.8110413800686456e-05, 'epoch': 0.68}\n",
      "{'loss': 1.6476, 'grad_norm': 11.296405792236328, 'learning_rate': 1.8090169943749477e-05, 'epoch': 0.68}\n",
      "{'loss': 1.6305, 'grad_norm': 11.516517639160156, 'learning_rate': 1.8069829665247975e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3657, 'grad_norm': 14.630508422851562, 'learning_rate': 1.8049393207604734e-05, 'epoch': 0.68}\n",
      "{'loss': 1.6646, 'grad_norm': 12.887083053588867, 'learning_rate': 1.8028860814388826e-05, 'epoch': 0.69}\n",
      "{'loss': 1.4254, 'grad_norm': 11.800593376159668, 'learning_rate': 1.8008232730312724e-05, 'epoch': 0.69}\n",
      "{'loss': 1.6334, 'grad_norm': 15.57284927368164, 'learning_rate': 1.7987509201229378e-05, 'epoch': 0.69}\n",
      "{'loss': 1.5336, 'grad_norm': 10.444375038146973, 'learning_rate': 1.7966690474129285e-05, 'epoch': 0.7}\n",
      "{'loss': 1.5631, 'grad_norm': 11.982242584228516, 'learning_rate': 1.7945776797137544e-05, 'epoch': 0.7}\n",
      "{'loss': 1.4204, 'grad_norm': 14.047536849975586, 'learning_rate': 1.7924768419510906e-05, 'epoch': 0.7}\n",
      "{'loss': 1.664, 'grad_norm': 25.90170669555664, 'learning_rate': 1.7903665591634794e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3708, 'grad_norm': 14.024728775024414, 'learning_rate': 1.7882468565020327e-05, 'epoch': 0.71}\n",
      "{'loss': 1.4303, 'grad_norm': 25.065940856933594, 'learning_rate': 1.786117759230132e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3754, 'grad_norm': 19.078094482421875, 'learning_rate': 1.7839792927231253e-05, 'epoch': 0.72}\n",
      "{'loss': 1.5722, 'grad_norm': 12.507901191711426, 'learning_rate': 1.78183148246803e-05, 'epoch': 0.72}\n",
      "{'loss': 1.4518, 'grad_norm': 15.800950050354004, 'learning_rate': 1.7796743540632226e-05, 'epoch': 0.72}\n",
      "{'loss': 1.5784, 'grad_norm': 14.778787612915039, 'learning_rate': 1.777507933218138e-05, 'epoch': 0.73}\n",
      "{'loss': 1.5843, 'grad_norm': 13.895177841186523, 'learning_rate': 1.7753322457529615e-05, 'epoch': 0.73}\n",
      "{'loss': 1.5641, 'grad_norm': 14.531061172485352, 'learning_rate': 1.7731473175983215e-05, 'epoch': 0.73}\n",
      "{'loss': 1.497, 'grad_norm': 13.433439254760742, 'learning_rate': 1.7709531747949796e-05, 'epoch': 0.74}\n",
      "{'loss': 1.6045, 'grad_norm': 16.840972900390625, 'learning_rate': 1.7687498434935224e-05, 'epoch': 0.74}\n",
      "{'loss': 1.5657, 'grad_norm': 14.033491134643555, 'learning_rate': 1.7665373499540464e-05, 'epoch': 0.74}\n",
      "{'loss': 1.4406, 'grad_norm': 20.074464797973633, 'learning_rate': 1.7643157205458483e-05, 'epoch': 0.75}\n",
      "{'loss': 1.5131, 'grad_norm': 18.809389114379883, 'learning_rate': 1.7620849817471094e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3533, 'grad_norm': 14.023041725158691, 'learning_rate': 1.759845160144579e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3875, 'grad_norm': 15.505875587463379, 'learning_rate': 1.7575962824332595e-05, 'epoch': 0.76}\n",
      "{'loss': 1.5431, 'grad_norm': 14.030781745910645, 'learning_rate': 1.7553383754160864e-05, 'epoch': 0.76}\n",
      "{'loss': 1.5698, 'grad_norm': 15.144686698913574, 'learning_rate': 1.7530714660036112e-05, 'epoch': 0.76}\n",
      "{'loss': 1.515, 'grad_norm': 14.702296257019043, 'learning_rate': 1.7507955812136775e-05, 'epoch': 0.76}\n",
      "{'loss': 1.5432, 'grad_norm': 16.486549377441406, 'learning_rate': 1.7485107481711014e-05, 'epoch': 0.77}\n",
      "{'loss': 1.385, 'grad_norm': 16.026409149169922, 'learning_rate': 1.7462169941073478e-05, 'epoch': 0.77}\n",
      "{'loss': 1.429, 'grad_norm': 18.965906143188477, 'learning_rate': 1.7439143463602052e-05, 'epoch': 0.77}\n",
      "{'loss': 1.5796, 'grad_norm': 14.934606552124023, 'learning_rate': 1.74160283237346e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3945, 'grad_norm': 14.110798835754395, 'learning_rate': 1.7392824796965703e-05, 'epoch': 0.78}\n",
      "{'loss': 1.4096, 'grad_norm': 15.447569847106934, 'learning_rate': 1.7369533159843368e-05, 'epoch': 0.78}\n",
      "{'loss': 1.134, 'grad_norm': 14.921366691589355, 'learning_rate': 1.734615368996573e-05, 'epoch': 0.79}\n",
      "{'loss': 0.922, 'grad_norm': 13.118118286132812, 'learning_rate': 1.7322686665977738e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0899, 'grad_norm': 18.471302032470703, 'learning_rate': 1.7299132367567856e-05, 'epoch': 0.79}\n",
      "{'loss': 1.4151, 'grad_norm': 16.118446350097656, 'learning_rate': 1.7275491075464716e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1171, 'grad_norm': 16.520950317382812, 'learning_rate': 1.7251763071433767e-05, 'epoch': 0.8}\n",
      "{'loss': 1.212, 'grad_norm': 13.021488189697266, 'learning_rate': 1.7227948638273918e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3081, 'grad_norm': 11.8041410446167, 'learning_rate': 1.7204048059814175e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3387, 'grad_norm': 13.15970230102539, 'learning_rate': 1.7180061620910263e-05, 'epoch': 0.81}\n",
      "{'loss': 1.323, 'grad_norm': 10.41515064239502, 'learning_rate': 1.715598960744121e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2234, 'grad_norm': 9.853618621826172, 'learning_rate': 1.7131832306305964e-05, 'epoch': 0.82}\n",
      "{'loss': 1.4514, 'grad_norm': 9.310538291931152, 'learning_rate': 1.710759000541995e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3789, 'grad_norm': 11.344127655029297, 'learning_rate': 1.7083262993711663e-05, 'epoch': 0.82}\n",
      "{'loss': 1.5364, 'grad_norm': 11.245309829711914, 'learning_rate': 1.7058851561119198e-05, 'epoch': 0.83}\n",
      "{'loss': 1.4383, 'grad_norm': 9.39879035949707, 'learning_rate': 1.7034355998586828e-05, 'epoch': 0.83}\n",
      "{'loss': 1.7468, 'grad_norm': 12.51472282409668, 'learning_rate': 1.7009776598061496e-05, 'epoch': 0.83}\n",
      "{'loss': 1.6179, 'grad_norm': 12.114042282104492, 'learning_rate': 1.6985113652489374e-05, 'epoch': 0.84}\n",
      "{'loss': 1.5117, 'grad_norm': 10.515917778015137, 'learning_rate': 1.6960367455812336e-05, 'epoch': 0.84}\n",
      "{'loss': 1.525, 'grad_norm': 11.968033790588379, 'learning_rate': 1.6935538302964496e-05, 'epoch': 0.84}\n",
      "{'loss': 1.7276, 'grad_norm': 12.021330833435059, 'learning_rate': 1.691062648986865e-05, 'epoch': 0.84}\n",
      "{'loss': 1.5519, 'grad_norm': 9.761343002319336, 'learning_rate': 1.6885632313432772e-05, 'epoch': 0.85}\n",
      "{'loss': 1.4311, 'grad_norm': 8.260954856872559, 'learning_rate': 1.686055607154648e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3742, 'grad_norm': 9.706308364868164, 'learning_rate': 1.6835398063077476e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3683, 'grad_norm': 10.924155235290527, 'learning_rate': 1.6810158587867973e-05, 'epoch': 0.86}\n",
      "{'loss': 1.4716, 'grad_norm': 8.711442947387695, 'learning_rate': 1.6784837946731148e-05, 'epoch': 0.86}\n",
      "{'loss': 1.3311, 'grad_norm': 9.054081916809082, 'learning_rate': 1.6759436441447544e-05, 'epoch': 0.86}\n",
      "{'loss': 1.4088, 'grad_norm': 8.115363121032715, 'learning_rate': 1.673395437476146e-05, 'epoch': 0.87}\n",
      "{'loss': 1.4006, 'grad_norm': 7.920323848724365, 'learning_rate': 1.6708392050377365e-05, 'epoch': 0.87}\n",
      "{'loss': 1.4901, 'grad_norm': 9.216085433959961, 'learning_rate': 1.668274977295626e-05, 'epoch': 0.87}\n",
      "{'loss': 1.5261, 'grad_norm': 8.459778785705566, 'learning_rate': 1.6657027848112064e-05, 'epoch': 0.88}\n",
      "{'loss': 1.6878, 'grad_norm': 11.647859573364258, 'learning_rate': 1.6631226582407954e-05, 'epoch': 0.88}\n",
      "{'loss': 1.5199, 'grad_norm': 8.74267292022705, 'learning_rate': 1.660534628335273e-05, 'epoch': 0.88}\n",
      "{'loss': 1.4343, 'grad_norm': 8.777178764343262, 'learning_rate': 1.657938725939713e-05, 'epoch': 0.89}\n",
      "{'loss': 1.4198, 'grad_norm': 9.178218841552734, 'learning_rate': 1.6553349819930167e-05, 'epoch': 0.89}\n",
      "{'loss': 1.4956, 'grad_norm': 12.805329322814941, 'learning_rate': 1.6527234275275445e-05, 'epoch': 0.89}\n",
      "{'loss': 1.5623, 'grad_norm': 10.104896545410156, 'learning_rate': 1.6501040936687444e-05, 'epoch': 0.9}\n",
      "{'loss': 1.1492, 'grad_norm': 12.543598175048828, 'learning_rate': 1.6474770116347824e-05, 'epoch': 0.9}\n",
      "{'loss': 1.5138, 'grad_norm': 14.538607597351074, 'learning_rate': 1.6448422127361707e-05, 'epoch': 0.9}\n",
      "{'loss': 1.3998, 'grad_norm': 12.659088134765625, 'learning_rate': 1.6421997283753928e-05, 'epoch': 0.91}\n",
      "{'loss': 1.4049, 'grad_norm': 12.31688117980957, 'learning_rate': 1.6395495900465306e-05, 'epoch': 0.91}\n",
      "{'loss': 1.3477, 'grad_norm': 13.11269760131836, 'learning_rate': 1.6368918293348893e-05, 'epoch': 0.91}\n",
      "{'loss': 1.4638, 'grad_norm': 12.163769721984863, 'learning_rate': 1.63422647791662e-05, 'epoch': 0.92}\n",
      "{'loss': 1.4309, 'grad_norm': 12.62402057647705, 'learning_rate': 1.6315535675583425e-05, 'epoch': 0.92}\n",
      "{'loss': 1.6254, 'grad_norm': 20.119239807128906, 'learning_rate': 1.6288731301167667e-05, 'epoch': 0.92}\n",
      "{'loss': 1.5361, 'grad_norm': 13.396856307983398, 'learning_rate': 1.626185197538314e-05, 'epoch': 0.92}\n",
      "{'loss': 1.4523, 'grad_norm': 13.494990348815918, 'learning_rate': 1.6234898018587336e-05, 'epoch': 0.93}\n",
      "{'loss': 1.5662, 'grad_norm': 41.90048599243164, 'learning_rate': 1.6207869752027248e-05, 'epoch': 0.93}\n",
      "{'loss': 1.3858, 'grad_norm': 13.188246726989746, 'learning_rate': 1.6180767497835503e-05, 'epoch': 0.93}\n",
      "{'loss': 1.4246, 'grad_norm': 14.31867790222168, 'learning_rate': 1.6153591579026545e-05, 'epoch': 0.94}\n",
      "{'loss': 1.2095, 'grad_norm': 11.867061614990234, 'learning_rate': 1.6126342319492784e-05, 'epoch': 0.94}\n",
      "{'loss': 1.1529, 'grad_norm': 12.799060821533203, 'learning_rate': 1.609902004400073e-05, 'epoch': 0.94}\n",
      "{'loss': 1.1721, 'grad_norm': 12.004386901855469, 'learning_rate': 1.6071625078187113e-05, 'epoch': 0.95}\n",
      "{'loss': 1.1834, 'grad_norm': 12.778221130371094, 'learning_rate': 1.6044157748555024e-05, 'epoch': 0.95}\n",
      "{'loss': 1.1186, 'grad_norm': 12.402630805969238, 'learning_rate': 1.6016618382470014e-05, 'epoch': 0.95}\n",
      "{'loss': 1.1817, 'grad_norm': 20.267826080322266, 'learning_rate': 1.598900730815617e-05, 'epoch': 0.96}\n",
      "{'loss': 1.0746, 'grad_norm': 16.62312126159668, 'learning_rate': 1.5961324854692254e-05, 'epoch': 0.96}\n",
      "{'loss': 1.2841, 'grad_norm': 10.751227378845215, 'learning_rate': 1.593357135200773e-05, 'epoch': 0.96}\n",
      "{'loss': 1.227, 'grad_norm': 12.313261985778809, 'learning_rate': 1.5905747130878853e-05, 'epoch': 0.97}\n",
      "{'loss': 1.5614, 'grad_norm': 19.77381134033203, 'learning_rate': 1.5877852522924733e-05, 'epoch': 0.97}\n",
      "{'loss': 1.5323, 'grad_norm': 25.188514709472656, 'learning_rate': 1.5849887860603374e-05, 'epoch': 0.97}\n",
      "{'loss': 1.5883, 'grad_norm': 22.460222244262695, 'learning_rate': 1.582185347720771e-05, 'epoch': 0.98}\n",
      "{'loss': 1.4128, 'grad_norm': 24.05792808532715, 'learning_rate': 1.5793749706861637e-05, 'epoch': 0.98}\n",
      "{'loss': 1.6695, 'grad_norm': 31.85933494567871, 'learning_rate': 1.576557688451603e-05, 'epoch': 0.98}\n",
      "{'loss': 1.5912, 'grad_norm': 19.34246253967285, 'learning_rate': 1.5737335345944758e-05, 'epoch': 0.99}\n",
      "{'loss': 1.5244, 'grad_norm': 16.579862594604492, 'learning_rate': 1.570902542774066e-05, 'epoch': 0.99}\n",
      "{'loss': 1.3709, 'grad_norm': 11.783015251159668, 'learning_rate': 1.568064746731156e-05, 'epoch': 0.99}\n",
      "{'loss': 1.3702, 'grad_norm': 20.339000701904297, 'learning_rate': 1.5652201802876227e-05, 'epoch': 1.0}\n",
      "{'loss': 1.1047, 'grad_norm': 14.112297058105469, 'learning_rate': 1.5623688773460358e-05, 'epoch': 1.0}\n",
      "{'loss': 1.0643, 'grad_norm': 16.274188995361328, 'learning_rate': 1.559510871889252e-05, 'epoch': 1.0}\n",
      "{'loss': 1.2182, 'grad_norm': 7.61199951171875, 'learning_rate': 1.556646197980012e-05, 'epoch': 1.0}\n",
      "{'loss': 1.0922, 'grad_norm': 7.5894389152526855, 'learning_rate': 1.553774889760533e-05, 'epoch': 1.01}\n",
      "{'loss': 1.1668, 'grad_norm': 9.74318790435791, 'learning_rate': 1.5508969814521026e-05, 'epoch': 1.01}\n",
      "{'loss': 1.0503, 'grad_norm': 9.199241638183594, 'learning_rate': 1.5480125073546705e-05, 'epoch': 1.01}\n",
      "{'loss': 1.2043, 'grad_norm': 7.709866523742676, 'learning_rate': 1.5451215018464386e-05, 'epoch': 1.02}\n",
      "{'loss': 1.2754, 'grad_norm': 8.209304809570312, 'learning_rate': 1.542223999383455e-05, 'epoch': 1.02}\n",
      "{'loss': 1.2577, 'grad_norm': 8.304532051086426, 'learning_rate': 1.5393200344991993e-05, 'epoch': 1.02}\n",
      "{'loss': 1.4476, 'grad_norm': 8.544384002685547, 'learning_rate': 1.5364096418041723e-05, 'epoch': 1.03}\n",
      "{'loss': 1.303, 'grad_norm': 10.356827735900879, 'learning_rate': 1.533492855985485e-05, 'epoch': 1.03}\n",
      "{'loss': 1.3206, 'grad_norm': 14.91120433807373, 'learning_rate': 1.530569711806443e-05, 'epoch': 1.03}\n",
      "{'loss': 1.4495, 'grad_norm': 9.669265747070312, 'learning_rate': 1.527640244106133e-05, 'epoch': 1.04}\n",
      "{'loss': 1.4284, 'grad_norm': 9.952008247375488, 'learning_rate': 1.524704487799008e-05, 'epoch': 1.04}\n",
      "{'loss': 1.4504, 'grad_norm': 10.954413414001465, 'learning_rate': 1.5217624778744718e-05, 'epoch': 1.04}\n",
      "{'loss': 1.5067, 'grad_norm': 10.544814109802246, 'learning_rate': 1.5188142493964595e-05, 'epoch': 1.04}\n",
      "{'loss': 1.3995, 'grad_norm': 12.03367805480957, 'learning_rate': 1.5158598375030218e-05, 'epoch': 1.05}\n",
      "{'loss': 1.4606, 'grad_norm': 12.154812812805176, 'learning_rate': 1.5128992774059063e-05, 'epoch': 1.05}\n",
      "{'loss': 1.2837, 'grad_norm': 12.08406925201416, 'learning_rate': 1.5099326043901361e-05, 'epoch': 1.05}\n",
      "{'loss': 1.3243, 'grad_norm': 10.995247840881348, 'learning_rate': 1.5069598538135905e-05, 'epoch': 1.06}\n",
      "{'loss': 1.3855, 'grad_norm': 16.828201293945312, 'learning_rate': 1.503981061106584e-05, 'epoch': 1.06}\n",
      "{'loss': 1.5399, 'grad_norm': 10.459692001342773, 'learning_rate': 1.5009962617714425e-05, 'epoch': 1.06}\n",
      "{'loss': 1.4649, 'grad_norm': 9.972837448120117, 'learning_rate': 1.4980054913820814e-05, 'epoch': 1.07}\n",
      "{'loss': 1.1688, 'grad_norm': 9.041287422180176, 'learning_rate': 1.4950087855835816e-05, 'epoch': 1.07}\n",
      "{'loss': 1.4444, 'grad_norm': 10.1941499710083, 'learning_rate': 1.4920061800917637e-05, 'epoch': 1.07}\n",
      "{'loss': 1.4065, 'grad_norm': 9.851873397827148, 'learning_rate': 1.4889977106927642e-05, 'epoch': 1.08}\n",
      "{'loss': 1.3024, 'grad_norm': 11.218029022216797, 'learning_rate': 1.485983413242606e-05, 'epoch': 1.08}\n",
      "{'loss': 1.2935, 'grad_norm': 9.48108959197998, 'learning_rate': 1.4829633236667746e-05, 'epoch': 1.08}\n",
      "{'loss': 1.1935, 'grad_norm': 9.5401611328125, 'learning_rate': 1.4799374779597866e-05, 'epoch': 1.09}\n",
      "{'loss': 1.3852, 'grad_norm': 11.432683944702148, 'learning_rate': 1.476905912184763e-05, 'epoch': 1.09}\n",
      "{'loss': 1.278, 'grad_norm': 9.366909980773926, 'learning_rate': 1.4738686624729987e-05, 'epoch': 1.09}\n",
      "{'loss': 1.2469, 'grad_norm': 9.061297416687012, 'learning_rate': 1.470825765023532e-05, 'epoch': 1.1}\n",
      "{'loss': 1.2648, 'grad_norm': 10.712254524230957, 'learning_rate': 1.4677772561027121e-05, 'epoch': 1.1}\n",
      "{'loss': 1.2466, 'grad_norm': 11.310608863830566, 'learning_rate': 1.4647231720437687e-05, 'epoch': 1.1}\n",
      "{'loss': 1.3396, 'grad_norm': 10.77717113494873, 'learning_rate': 1.4616635492463775e-05, 'epoch': 1.11}\n",
      "{'loss': 1.3189, 'grad_norm': 9.162737846374512, 'learning_rate': 1.4585984241762268e-05, 'epoch': 1.11}\n",
      "{'loss': 1.196, 'grad_norm': 9.222952842712402, 'learning_rate': 1.4555278333645833e-05, 'epoch': 1.11}\n",
      "{'loss': 1.2393, 'grad_norm': 11.916872024536133, 'learning_rate': 1.4524518134078565e-05, 'epoch': 1.12}\n",
      "{'loss': 1.3649, 'grad_norm': 9.992544174194336, 'learning_rate': 1.4493704009671614e-05, 'epoch': 1.12}\n",
      "{'loss': 1.1663, 'grad_norm': 10.642590522766113, 'learning_rate': 1.446283632767884e-05, 'epoch': 1.12}\n",
      "{'loss': 1.2321, 'grad_norm': 11.037673950195312, 'learning_rate': 1.4431915455992416e-05, 'epoch': 1.12}\n",
      "{'loss': 1.2411, 'grad_norm': 15.622066497802734, 'learning_rate': 1.440094176313844e-05, 'epoch': 1.13}\n",
      "{'loss': 1.3823, 'grad_norm': 9.930530548095703, 'learning_rate': 1.4369915618272568e-05, 'epoch': 1.13}\n",
      "{'loss': 1.2555, 'grad_norm': 12.002904891967773, 'learning_rate': 1.4338837391175582e-05, 'epoch': 1.13}\n",
      "{'loss': 1.1117, 'grad_norm': 9.689949035644531, 'learning_rate': 1.4307707452249013e-05, 'epoch': 1.14}\n",
      "{'loss': 1.488, 'grad_norm': 13.118701934814453, 'learning_rate': 1.42765261725107e-05, 'epoch': 1.14}\n",
      "{'loss': 1.1103, 'grad_norm': 10.894780158996582, 'learning_rate': 1.424529392359039e-05, 'epoch': 1.14}\n",
      "{'loss': 1.0701, 'grad_norm': 12.785613059997559, 'learning_rate': 1.4214011077725293e-05, 'epoch': 1.15}\n",
      "{'loss': 0.8146, 'grad_norm': 10.852293014526367, 'learning_rate': 1.4182678007755653e-05, 'epoch': 1.15}\n",
      "{'loss': 0.8439, 'grad_norm': 9.195640563964844, 'learning_rate': 1.4151295087120307e-05, 'epoch': 1.15}\n",
      "{'loss': 0.9909, 'grad_norm': 11.420082092285156, 'learning_rate': 1.4119862689852224e-05, 'epoch': 1.16}\n",
      "{'loss': 0.8504, 'grad_norm': 13.379630088806152, 'learning_rate': 1.4088381190574051e-05, 'epoch': 1.16}\n",
      "{'loss': 1.1264, 'grad_norm': 12.556077003479004, 'learning_rate': 1.4056850964493668e-05, 'epoch': 1.16}\n",
      "{'loss': 1.1868, 'grad_norm': 14.0816068649292, 'learning_rate': 1.4025272387399676e-05, 'epoch': 1.17}\n",
      "{'loss': 1.1368, 'grad_norm': 12.4837646484375, 'learning_rate': 1.3993645835656955e-05, 'epoch': 1.17}\n",
      "{'loss': 1.1087, 'grad_norm': 12.343517303466797, 'learning_rate': 1.3961971686202163e-05, 'epoch': 1.17}\n",
      "{'loss': 1.1466, 'grad_norm': 10.545587539672852, 'learning_rate': 1.3930250316539237e-05, 'epoch': 1.18}\n",
      "{'loss': 1.1743, 'grad_norm': 14.160867691040039, 'learning_rate': 1.3898482104734909e-05, 'epoch': 1.18}\n",
      "{'loss': 1.2788, 'grad_norm': 13.634122848510742, 'learning_rate': 1.3866667429414188e-05, 'epoch': 1.18}\n",
      "{'loss': 1.5562, 'grad_norm': 14.445489883422852, 'learning_rate': 1.383480666975586e-05, 'epoch': 1.19}\n",
      "{'loss': 1.4486, 'grad_norm': 14.235284805297852, 'learning_rate': 1.3802900205487948e-05, 'epoch': 1.19}\n",
      "{'loss': 1.4751, 'grad_norm': 16.473262786865234, 'learning_rate': 1.3770948416883205e-05, 'epoch': 1.19}\n",
      "{'loss': 1.4491, 'grad_norm': 17.090364456176758, 'learning_rate': 1.3738951684754585e-05, 'epoch': 1.2}\n",
      "{'loss': 1.3061, 'grad_norm': 13.307912826538086, 'learning_rate': 1.3706910390450679e-05, 'epoch': 1.2}\n",
      "{'loss': 1.3896, 'grad_norm': 16.41256332397461, 'learning_rate': 1.3674824915851193e-05, 'epoch': 1.2}\n",
      "{'loss': 1.3419, 'grad_norm': 15.22188663482666, 'learning_rate': 1.3642695643362398e-05, 'epoch': 1.2}\n",
      "{'loss': 1.3539, 'grad_norm': 17.6226806640625, 'learning_rate': 1.3610522955912551e-05, 'epoch': 1.21}\n",
      "{'loss': 1.2366, 'grad_norm': 16.094545364379883, 'learning_rate': 1.3578307236947348e-05, 'epoch': 1.21}\n",
      "{'loss': 1.3279, 'grad_norm': 15.384180068969727, 'learning_rate': 1.3546048870425356e-05, 'epoch': 1.21}\n",
      "{'loss': 1.3258, 'grad_norm': 16.623462677001953, 'learning_rate': 1.3513748240813429e-05, 'epoch': 1.22}\n",
      "{'loss': 1.2104, 'grad_norm': 14.99543285369873, 'learning_rate': 1.3481405733082118e-05, 'epoch': 1.22}\n",
      "{'loss': 1.3268, 'grad_norm': 13.097556114196777, 'learning_rate': 1.3449021732701106e-05, 'epoch': 1.22}\n",
      "{'loss': 1.406, 'grad_norm': 17.829519271850586, 'learning_rate': 1.3416596625634595e-05, 'epoch': 1.23}\n",
      "{'loss': 1.5185, 'grad_norm': 49.406455993652344, 'learning_rate': 1.3384130798336705e-05, 'epoch': 1.23}\n",
      "{'loss': 1.603, 'grad_norm': 17.0820255279541, 'learning_rate': 1.3351624637746885e-05, 'epoch': 1.23}\n",
      "{'loss': 1.2715, 'grad_norm': 13.957545280456543, 'learning_rate': 1.3319078531285286e-05, 'epoch': 1.24}\n",
      "{'loss': 1.1873, 'grad_norm': 12.46259593963623, 'learning_rate': 1.3286492866848143e-05, 'epoch': 1.24}\n",
      "{'loss': 1.3953, 'grad_norm': 18.454790115356445, 'learning_rate': 1.3253868032803171e-05, 'epoch': 1.24}\n",
      "{'loss': 1.1006, 'grad_norm': 15.181138038635254, 'learning_rate': 1.3221204417984907e-05, 'epoch': 1.25}\n",
      "{'loss': 1.318, 'grad_norm': 13.143539428710938, 'learning_rate': 1.3188502411690101e-05, 'epoch': 1.25}\n",
      "{'loss': 1.3911, 'grad_norm': 14.534926414489746, 'learning_rate': 1.3155762403673065e-05, 'epoch': 1.25}\n",
      "{'loss': 1.3803, 'grad_norm': 14.104043960571289, 'learning_rate': 1.3122984784141021e-05, 'epoch': 1.26}\n",
      "{'loss': 1.1273, 'grad_norm': 12.989500045776367, 'learning_rate': 1.3090169943749475e-05, 'epoch': 1.26}\n",
      "{'loss': 1.316, 'grad_norm': 11.784252166748047, 'learning_rate': 1.3057318273597531e-05, 'epoch': 1.26}\n",
      "{'loss': 1.146, 'grad_norm': 11.8733491897583, 'learning_rate': 1.3024430165223245e-05, 'epoch': 1.27}\n",
      "{'loss': 1.2104, 'grad_norm': 25.954923629760742, 'learning_rate': 1.2991506010598965e-05, 'epoch': 1.27}\n",
      "{'loss': 1.223, 'grad_norm': 15.401305198669434, 'learning_rate': 1.2958546202126638e-05, 'epoch': 1.27}\n",
      "{'loss': 1.1265, 'grad_norm': 13.327609062194824, 'learning_rate': 1.2925551132633164e-05, 'epoch': 1.28}\n",
      "{'loss': 1.2769, 'grad_norm': 14.923540115356445, 'learning_rate': 1.2892521195365679e-05, 'epoch': 1.28}\n",
      "{'loss': 1.5452, 'grad_norm': 13.069478988647461, 'learning_rate': 1.2859456783986892e-05, 'epoch': 1.28}\n",
      "{'loss': 1.2184, 'grad_norm': 15.754485130310059, 'learning_rate': 1.2826358292570398e-05, 'epoch': 1.28}\n",
      "{'loss': 1.2473, 'grad_norm': 12.47900104522705, 'learning_rate': 1.2793226115595951e-05, 'epoch': 1.29}\n",
      "{'loss': 1.2338, 'grad_norm': 13.960577964782715, 'learning_rate': 1.2760060647944794e-05, 'epoch': 1.29}\n",
      "{'loss': 1.1883, 'grad_norm': 12.39403247833252, 'learning_rate': 1.2726862284894939e-05, 'epoch': 1.29}\n",
      "{'loss': 1.4236, 'grad_norm': 12.412757873535156, 'learning_rate': 1.2693631422116455e-05, 'epoch': 1.3}\n",
      "{'loss': 1.037, 'grad_norm': 11.16620922088623, 'learning_rate': 1.2660368455666752e-05, 'epoch': 1.3}\n",
      "{'loss': 1.2123, 'grad_norm': 19.550981521606445, 'learning_rate': 1.262707378198587e-05, 'epoch': 1.3}\n",
      "{'loss': 1.0436, 'grad_norm': 11.73232650756836, 'learning_rate': 1.2593747797891743e-05, 'epoch': 1.31}\n",
      "{'loss': 0.8635, 'grad_norm': 9.977005958557129, 'learning_rate': 1.2560390900575472e-05, 'epoch': 1.31}\n",
      "{'loss': 1.0779, 'grad_norm': 10.409286499023438, 'learning_rate': 1.2527003487596598e-05, 'epoch': 1.31}\n",
      "{'loss': 0.9132, 'grad_norm': 13.053997993469238, 'learning_rate': 1.2493585956878354e-05, 'epoch': 1.32}\n",
      "{'loss': 0.8572, 'grad_norm': 13.548495292663574, 'learning_rate': 1.2460138706702929e-05, 'epoch': 1.32}\n",
      "{'loss': 1.223, 'grad_norm': 9.29475212097168, 'learning_rate': 1.242666213570672e-05, 'epoch': 1.32}\n",
      "{'loss': 1.1485, 'grad_norm': 8.599078178405762, 'learning_rate': 1.2393156642875579e-05, 'epoch': 1.33}\n",
      "{'loss': 1.1236, 'grad_norm': 8.518572807312012, 'learning_rate': 1.2359622627540059e-05, 'epoch': 1.33}\n",
      "{'loss': 1.1388, 'grad_norm': 8.880722999572754, 'learning_rate': 1.2326060489370655e-05, 'epoch': 1.33}\n",
      "{'loss': 1.1373, 'grad_norm': 9.40961742401123, 'learning_rate': 1.229247062837304e-05, 'epoch': 1.34}\n",
      "{'loss': 1.1707, 'grad_norm': 8.474485397338867, 'learning_rate': 1.2258853444883297e-05, 'epoch': 1.34}\n",
      "{'loss': 1.2509, 'grad_norm': 10.57576847076416, 'learning_rate': 1.2225209339563144e-05, 'epoch': 1.34}\n",
      "{'loss': 1.2882, 'grad_norm': 12.600860595703125, 'learning_rate': 1.219153871339518e-05, 'epoch': 1.35}\n",
      "{'loss': 1.4607, 'grad_norm': 12.18893051147461, 'learning_rate': 1.2157841967678064e-05, 'epoch': 1.35}\n",
      "{'loss': 1.2277, 'grad_norm': 14.763238906860352, 'learning_rate': 1.2124119504021776e-05, 'epoch': 1.35}\n",
      "{'loss': 1.3796, 'grad_norm': 13.099287986755371, 'learning_rate': 1.2090371724342804e-05, 'epoch': 1.36}\n",
      "{'loss': 1.5715, 'grad_norm': 15.285503387451172, 'learning_rate': 1.2056599030859367e-05, 'epoch': 1.36}\n",
      "{'loss': 1.244, 'grad_norm': 14.043230056762695, 'learning_rate': 1.2022801826086609e-05, 'epoch': 1.36}\n",
      "{'loss': 1.3012, 'grad_norm': 11.35576343536377, 'learning_rate': 1.1988980512831809e-05, 'epoch': 1.36}\n",
      "{'loss': 1.4989, 'grad_norm': 12.621065139770508, 'learning_rate': 1.195513549418959e-05, 'epoch': 1.37}\n",
      "{'loss': 1.273, 'grad_norm': 12.288043975830078, 'learning_rate': 1.1921267173537085e-05, 'epoch': 1.37}\n",
      "{'loss': 1.1774, 'grad_norm': 11.573018074035645, 'learning_rate': 1.1887375954529167e-05, 'epoch': 1.37}\n",
      "{'loss': 1.5839, 'grad_norm': 16.174631118774414, 'learning_rate': 1.1853462241093614e-05, 'epoch': 1.38}\n",
      "{'loss': 1.2818, 'grad_norm': 15.146202087402344, 'learning_rate': 1.1819526437426298e-05, 'epoch': 1.38}\n",
      "{'loss': 1.2568, 'grad_norm': 19.82988166809082, 'learning_rate': 1.1785568947986368e-05, 'epoch': 1.38}\n",
      "{'loss': 1.3707, 'grad_norm': 13.446076393127441, 'learning_rate': 1.1751590177491441e-05, 'epoch': 1.39}\n",
      "{'loss': 1.3491, 'grad_norm': 11.468859672546387, 'learning_rate': 1.1717590530912764e-05, 'epoch': 1.39}\n",
      "{'loss': 1.2114, 'grad_norm': 12.681286811828613, 'learning_rate': 1.1683570413470384e-05, 'epoch': 1.39}\n",
      "{'loss': 1.1142, 'grad_norm': 10.947165489196777, 'learning_rate': 1.164953023062835e-05, 'epoch': 1.4}\n",
      "{'loss': 1.2448, 'grad_norm': 11.895483016967773, 'learning_rate': 1.1615470388089836e-05, 'epoch': 1.4}\n",
      "{'loss': 1.3299, 'grad_norm': 12.564374923706055, 'learning_rate': 1.1581391291792336e-05, 'epoch': 1.4}\n",
      "{'loss': 1.3339, 'grad_norm': 11.36915111541748, 'learning_rate': 1.1547293347902813e-05, 'epoch': 1.41}\n",
      "{'loss': 1.2896, 'grad_norm': 9.77184772491455, 'learning_rate': 1.151317696281287e-05, 'epoch': 1.41}\n",
      "{'loss': 1.1812, 'grad_norm': 10.19845199584961, 'learning_rate': 1.1479042543133895e-05, 'epoch': 1.41}\n",
      "{'loss': 1.0198, 'grad_norm': 10.84755802154541, 'learning_rate': 1.1444890495692214e-05, 'epoch': 1.42}\n",
      "{'loss': 1.5316, 'grad_norm': 12.288375854492188, 'learning_rate': 1.1410721227524256e-05, 'epoch': 1.42}\n",
      "{'loss': 1.3628, 'grad_norm': 12.093709945678711, 'learning_rate': 1.1376535145871685e-05, 'epoch': 1.42}\n",
      "{'loss': 1.1472, 'grad_norm': 8.779691696166992, 'learning_rate': 1.1342332658176556e-05, 'epoch': 1.43}\n",
      "{'loss': 1.3801, 'grad_norm': 11.820806503295898, 'learning_rate': 1.1308114172076464e-05, 'epoch': 1.43}\n",
      "{'loss': 1.0751, 'grad_norm': 11.215828895568848, 'learning_rate': 1.1273880095399667e-05, 'epoch': 1.43}\n",
      "{'loss': 1.2536, 'grad_norm': 12.569446563720703, 'learning_rate': 1.1239630836160246e-05, 'epoch': 1.44}\n",
      "{'loss': 1.044, 'grad_norm': 11.718851089477539, 'learning_rate': 1.1205366802553231e-05, 'epoch': 1.44}\n",
      "{'loss': 1.2683, 'grad_norm': 12.137988090515137, 'learning_rate': 1.1171088402949739e-05, 'epoch': 1.44}\n",
      "{'loss': 1.2546, 'grad_norm': 11.606438636779785, 'learning_rate': 1.1136796045892102e-05, 'epoch': 1.44}\n",
      "{'loss': 1.2992, 'grad_norm': 10.928889274597168, 'learning_rate': 1.1102490140089009e-05, 'epoch': 1.45}\n",
      "{'loss': 1.3085, 'grad_norm': 10.980263710021973, 'learning_rate': 1.1068171094410618e-05, 'epoch': 1.45}\n",
      "{'loss': 1.1699, 'grad_norm': 12.617344856262207, 'learning_rate': 1.10338393178837e-05, 'epoch': 1.45}\n",
      "{'loss': 1.2908, 'grad_norm': 12.953207015991211, 'learning_rate': 1.0999495219686762e-05, 'epoch': 1.46}\n",
      "{'loss': 1.2591, 'grad_norm': 12.409153938293457, 'learning_rate': 1.0965139209145153e-05, 'epoch': 1.46}\n",
      "{'loss': 1.2499, 'grad_norm': 12.255062103271484, 'learning_rate': 1.0930771695726201e-05, 'epoch': 1.46}\n",
      "{'loss': 1.0251, 'grad_norm': 12.500568389892578, 'learning_rate': 1.0896393089034336e-05, 'epoch': 1.47}\n",
      "{'loss': 0.9517, 'grad_norm': 10.882055282592773, 'learning_rate': 1.0862003798806195e-05, 'epoch': 1.47}\n",
      "{'loss': 0.7289, 'grad_norm': 11.185416221618652, 'learning_rate': 1.0827604234905749e-05, 'epoch': 1.47}\n",
      "{'loss': 0.9362, 'grad_norm': 12.732502937316895, 'learning_rate': 1.079319480731941e-05, 'epoch': 1.48}\n",
      "{'loss': 0.9822, 'grad_norm': 14.405077934265137, 'learning_rate': 1.0758775926151155e-05, 'epoch': 1.48}\n",
      "{'loss': 1.1577, 'grad_norm': 9.376566886901855, 'learning_rate': 1.0724348001617626e-05, 'epoch': 1.48}\n",
      "{'loss': 1.2597, 'grad_norm': 9.016305923461914, 'learning_rate': 1.0689911444043249e-05, 'epoch': 1.49}\n",
      "{'loss': 1.113, 'grad_norm': 7.637360095977783, 'learning_rate': 1.0655466663855349e-05, 'epoch': 1.49}\n",
      "{'loss': 1.1032, 'grad_norm': 7.8353095054626465, 'learning_rate': 1.0621014071579241e-05, 'epoch': 1.49}\n",
      "{'loss': 1.1433, 'grad_norm': 8.10083293914795, 'learning_rate': 1.0586554077833346e-05, 'epoch': 1.5}\n",
      "{'loss': 1.2533, 'grad_norm': 9.012182235717773, 'learning_rate': 1.0552087093324314e-05, 'epoch': 1.5}\n",
      "{'loss': 1.1281, 'grad_norm': 8.551861763000488, 'learning_rate': 1.0517613528842096e-05, 'epoch': 1.5}\n",
      "{'loss': 1.2692, 'grad_norm': 7.48217248916626, 'learning_rate': 1.0483133795255072e-05, 'epoch': 1.51}\n",
      "{'loss': 1.3405, 'grad_norm': 11.505873680114746, 'learning_rate': 1.044864830350515e-05, 'epoch': 1.51}\n",
      "{'loss': 1.232, 'grad_norm': 7.69214391708374, 'learning_rate': 1.0414157464602866e-05, 'epoch': 1.51}\n",
      "{'loss': 1.4087, 'grad_norm': 9.581838607788086, 'learning_rate': 1.0379661689622477e-05, 'epoch': 1.52}\n",
      "{'loss': 1.4832, 'grad_norm': 8.075206756591797, 'learning_rate': 1.0345161389697083e-05, 'epoch': 1.52}\n",
      "{'loss': 1.5156, 'grad_norm': 10.154224395751953, 'learning_rate': 1.0310656976013704e-05, 'epoch': 1.52}\n",
      "{'loss': 1.3188, 'grad_norm': 7.683579444885254, 'learning_rate': 1.027614885980839e-05, 'epoch': 1.52}\n",
      "{'loss': 1.2843, 'grad_norm': 8.176581382751465, 'learning_rate': 1.0241637452361323e-05, 'epoch': 1.53}\n",
      "{'loss': 1.2729, 'grad_norm': 7.1318278312683105, 'learning_rate': 1.0207123164991912e-05, 'epoch': 1.53}\n",
      "{'loss': 1.3942, 'grad_norm': 10.867375373840332, 'learning_rate': 1.0172606409053887e-05, 'epoch': 1.53}\n",
      "{'loss': 1.4767, 'grad_norm': 8.693802833557129, 'learning_rate': 1.0138087595930394e-05, 'epoch': 1.54}\n",
      "{'loss': 1.3594, 'grad_norm': 7.922430038452148, 'learning_rate': 1.0103567137029111e-05, 'epoch': 1.54}\n",
      "{'loss': 1.1736, 'grad_norm': 8.7865571975708, 'learning_rate': 1.0069045443777318e-05, 'epoch': 1.54}\n",
      "{'loss': 1.2287, 'grad_norm': 8.159887313842773, 'learning_rate': 1.0034522927617014e-05, 'epoch': 1.55}\n",
      "{'loss': 1.3807, 'grad_norm': 8.282713890075684, 'learning_rate': 1e-05, 'epoch': 1.55}\n",
      "{'loss': 1.0754, 'grad_norm': 7.653985500335693, 'learning_rate': 9.965477072382989e-06, 'epoch': 1.55}\n",
      "{'loss': 1.1937, 'grad_norm': 8.19426155090332, 'learning_rate': 9.930954556222683e-06, 'epoch': 1.56}\n",
      "{'loss': 1.3317, 'grad_norm': 8.323820114135742, 'learning_rate': 9.896432862970892e-06, 'epoch': 1.56}\n",
      "{'loss': 1.2229, 'grad_norm': 8.50858211517334, 'learning_rate': 9.861912404069608e-06, 'epoch': 1.56}\n",
      "{'loss': 1.4012, 'grad_norm': 8.460016250610352, 'learning_rate': 9.827393590946116e-06, 'epoch': 1.57}\n",
      "{'loss': 1.1533, 'grad_norm': 9.34698486328125, 'learning_rate': 9.79287683500809e-06, 'epoch': 1.57}\n",
      "{'loss': 1.2868, 'grad_norm': 11.304505348205566, 'learning_rate': 9.75836254763868e-06, 'epoch': 1.57}\n",
      "{'loss': 1.2713, 'grad_norm': 9.212244033813477, 'learning_rate': 9.723851140191613e-06, 'epoch': 1.58}\n",
      "{'loss': 1.2737, 'grad_norm': 8.679449081420898, 'learning_rate': 9.689343023986303e-06, 'epoch': 1.58}\n",
      "{'loss': 1.1818, 'grad_norm': 8.672170639038086, 'learning_rate': 9.654838610302922e-06, 'epoch': 1.58}\n",
      "{'loss': 1.3196, 'grad_norm': 9.679801940917969, 'learning_rate': 9.620338310377526e-06, 'epoch': 1.59}\n",
      "{'loss': 1.1597, 'grad_norm': 9.521411895751953, 'learning_rate': 9.58584253539714e-06, 'epoch': 1.59}\n",
      "{'loss': 0.9382, 'grad_norm': 7.134879112243652, 'learning_rate': 9.551351696494854e-06, 'epoch': 1.59}\n",
      "{'loss': 1.3752, 'grad_norm': 10.00033950805664, 'learning_rate': 9.516866204744932e-06, 'epoch': 1.6}\n",
      "{'loss': 1.2705, 'grad_norm': 9.441974639892578, 'learning_rate': 9.482386471157905e-06, 'epoch': 1.6}\n",
      "{'loss': 1.2376, 'grad_norm': 10.583897590637207, 'learning_rate': 9.447912906675687e-06, 'epoch': 1.6}\n",
      "{'loss': 1.1165, 'grad_norm': 9.340893745422363, 'learning_rate': 9.413445922166654e-06, 'epoch': 1.6}\n",
      "{'loss': 0.9609, 'grad_norm': 10.928790092468262, 'learning_rate': 9.378985928420764e-06, 'epoch': 1.61}\n",
      "{'loss': 1.2857, 'grad_norm': 11.480367660522461, 'learning_rate': 9.344533336144653e-06, 'epoch': 1.61}\n",
      "{'loss': 0.9788, 'grad_norm': 10.186474800109863, 'learning_rate': 9.310088555956751e-06, 'epoch': 1.61}\n",
      "{'loss': 0.9063, 'grad_norm': 11.532774925231934, 'learning_rate': 9.275651998382377e-06, 'epoch': 1.62}\n",
      "{'loss': 0.9544, 'grad_norm': 11.324356079101562, 'learning_rate': 9.241224073848848e-06, 'epoch': 1.62}\n",
      "{'loss': 0.9191, 'grad_norm': 15.020618438720703, 'learning_rate': 9.206805192680592e-06, 'epoch': 1.62}\n",
      "{'loss': 1.1094, 'grad_norm': 15.54071044921875, 'learning_rate': 9.172395765094255e-06, 'epoch': 1.63}\n",
      "{'loss': 0.8665, 'grad_norm': 13.814291000366211, 'learning_rate': 9.137996201193807e-06, 'epoch': 1.63}\n",
      "{'loss': 0.9497, 'grad_norm': 12.137360572814941, 'learning_rate': 9.103606910965666e-06, 'epoch': 1.63}\n",
      "{'loss': 0.873, 'grad_norm': 18.46076774597168, 'learning_rate': 9.069228304273802e-06, 'epoch': 1.64}\n",
      "{'loss': 1.0209, 'grad_norm': 15.35365104675293, 'learning_rate': 9.034860790854848e-06, 'epoch': 1.64}\n",
      "{'loss': 1.0481, 'grad_norm': 16.419437408447266, 'learning_rate': 9.00050478031324e-06, 'epoch': 1.64}\n",
      "{'loss': 1.0628, 'grad_norm': 11.144360542297363, 'learning_rate': 8.966160682116301e-06, 'epoch': 1.65}\n",
      "{'loss': 1.1195, 'grad_norm': 10.038145065307617, 'learning_rate': 8.931828905589385e-06, 'epoch': 1.65}\n",
      "{'loss': 1.0799, 'grad_norm': 12.010442733764648, 'learning_rate': 8.897509859910996e-06, 'epoch': 1.65}\n",
      "{'loss': 1.1113, 'grad_norm': 12.675471305847168, 'learning_rate': 8.863203954107902e-06, 'epoch': 1.66}\n",
      "{'loss': 1.1464, 'grad_norm': 10.22140121459961, 'learning_rate': 8.828911597050263e-06, 'epoch': 1.66}\n",
      "{'loss': 1.3199, 'grad_norm': 11.811229705810547, 'learning_rate': 8.79463319744677e-06, 'epoch': 1.66}\n",
      "{'loss': 1.2945, 'grad_norm': 8.870491027832031, 'learning_rate': 8.760369163839759e-06, 'epoch': 1.67}\n",
      "{'loss': 1.4807, 'grad_norm': 12.404671669006348, 'learning_rate': 8.726119904600337e-06, 'epoch': 1.67}\n",
      "{'loss': 1.6355, 'grad_norm': 21.07191276550293, 'learning_rate': 8.691885827923541e-06, 'epoch': 1.67}\n",
      "{'loss': 1.4075, 'grad_norm': 16.662200927734375, 'learning_rate': 8.657667341823449e-06, 'epoch': 1.68}\n",
      "{'loss': 1.3562, 'grad_norm': 11.924481391906738, 'learning_rate': 8.62346485412832e-06, 'epoch': 1.68}\n",
      "{'loss': 1.5037, 'grad_norm': 27.647254943847656, 'learning_rate': 8.58927877247575e-06, 'epoch': 1.68}\n",
      "{'loss': 1.473, 'grad_norm': 14.007772445678711, 'learning_rate': 8.55510950430779e-06, 'epoch': 1.68}\n",
      "{'loss': 1.4613, 'grad_norm': 9.103519439697266, 'learning_rate': 8.520957456866107e-06, 'epoch': 1.69}\n",
      "{'loss': 1.3446, 'grad_norm': 8.46019172668457, 'learning_rate': 8.48682303718713e-06, 'epoch': 1.69}\n",
      "{'loss': 1.3304, 'grad_norm': 17.04161834716797, 'learning_rate': 8.452706652097187e-06, 'epoch': 1.69}\n",
      "{'loss': 1.0564, 'grad_norm': 8.737227439880371, 'learning_rate': 8.418608708207667e-06, 'epoch': 1.7}\n",
      "{'loss': 1.2864, 'grad_norm': 9.248566627502441, 'learning_rate': 8.384529611910164e-06, 'epoch': 1.7}\n",
      "{'loss': 1.1431, 'grad_norm': 9.28956127166748, 'learning_rate': 8.35046976937165e-06, 'epoch': 1.7}\n",
      "{'loss': 1.1424, 'grad_norm': 8.326753616333008, 'learning_rate': 8.316429586529616e-06, 'epoch': 1.71}\n",
      "{'loss': 1.0134, 'grad_norm': 8.102027893066406, 'learning_rate': 8.28240946908724e-06, 'epoch': 1.71}\n",
      "{'loss': 1.232, 'grad_norm': 10.336404800415039, 'learning_rate': 8.24840982250856e-06, 'epoch': 1.71}\n",
      "{'loss': 1.395, 'grad_norm': 10.517169952392578, 'learning_rate': 8.214431052013636e-06, 'epoch': 1.72}\n",
      "{'loss': 1.4685, 'grad_norm': 11.157259941101074, 'learning_rate': 8.180473562573705e-06, 'epoch': 1.72}\n",
      "{'loss': 1.1871, 'grad_norm': 9.809596061706543, 'learning_rate': 8.146537758906388e-06, 'epoch': 1.72}\n",
      "{'loss': 1.1677, 'grad_norm': 8.285682678222656, 'learning_rate': 8.112624045470834e-06, 'epoch': 1.73}\n",
      "{'loss': 1.0703, 'grad_norm': 8.350813865661621, 'learning_rate': 8.078732826462917e-06, 'epoch': 1.73}\n",
      "{'loss': 1.1862, 'grad_norm': 8.20268726348877, 'learning_rate': 8.044864505810415e-06, 'epoch': 1.73}\n",
      "{'loss': 1.1896, 'grad_norm': 8.631599426269531, 'learning_rate': 8.011019487168193e-06, 'epoch': 1.74}\n",
      "{'loss': 1.1475, 'grad_norm': 8.972465515136719, 'learning_rate': 7.977198173913394e-06, 'epoch': 1.74}\n",
      "{'loss': 1.1747, 'grad_norm': 9.158676147460938, 'learning_rate': 7.943400969140635e-06, 'epoch': 1.74}\n",
      "{'loss': 1.1772, 'grad_norm': 8.269600868225098, 'learning_rate': 7.909628275657199e-06, 'epoch': 1.75}\n",
      "{'loss': 1.1791, 'grad_norm': 8.502671241760254, 'learning_rate': 7.875880495978227e-06, 'epoch': 1.75}\n",
      "{'loss': 1.1825, 'grad_norm': 7.876400470733643, 'learning_rate': 7.84215803232194e-06, 'epoch': 1.75}\n",
      "{'loss': 1.2686, 'grad_norm': 14.937850952148438, 'learning_rate': 7.808461286604828e-06, 'epoch': 1.76}\n",
      "{'loss': 1.1288, 'grad_norm': 8.754910469055176, 'learning_rate': 7.774790660436857e-06, 'epoch': 1.76}\n",
      "{'loss': 0.9548, 'grad_norm': 8.698223114013672, 'learning_rate': 7.741146555116708e-06, 'epoch': 1.76}\n",
      "{'loss': 1.0433, 'grad_norm': 9.577380180358887, 'learning_rate': 7.707529371626966e-06, 'epoch': 1.76}\n",
      "{'loss': 1.171, 'grad_norm': 8.71187686920166, 'learning_rate': 7.67393951062935e-06, 'epoch': 1.77}\n",
      "{'loss': 1.2994, 'grad_norm': 10.397917747497559, 'learning_rate': 7.640377372459944e-06, 'epoch': 1.77}\n",
      "{'loss': 1.216, 'grad_norm': 8.506305694580078, 'learning_rate': 7.606843357124426e-06, 'epoch': 1.77}\n",
      "{'loss': 1.3127, 'grad_norm': 10.883910179138184, 'learning_rate': 7.573337864293283e-06, 'epoch': 1.78}\n",
      "{'loss': 1.1389, 'grad_norm': 12.586996078491211, 'learning_rate': 7.539861293297073e-06, 'epoch': 1.78}\n",
      "{'loss': 0.9968, 'grad_norm': 7.906808376312256, 'learning_rate': 7.506414043121647e-06, 'epoch': 1.78}\n",
      "{'loss': 1.0944, 'grad_norm': 9.238296508789062, 'learning_rate': 7.472996512403403e-06, 'epoch': 1.79}\n",
      "{'loss': 0.96, 'grad_norm': 8.32480239868164, 'learning_rate': 7.4396090994245295e-06, 'epoch': 1.79}\n",
      "{'loss': 0.5989, 'grad_norm': 8.581392288208008, 'learning_rate': 7.406252202108258e-06, 'epoch': 1.79}\n",
      "{'loss': 0.9266, 'grad_norm': 11.888463973999023, 'learning_rate': 7.372926218014131e-06, 'epoch': 1.8}\n",
      "{'loss': 0.8507, 'grad_norm': 9.759237289428711, 'learning_rate': 7.33963154433325e-06, 'epoch': 1.8}\n",
      "{'loss': 1.1506, 'grad_norm': 7.300513744354248, 'learning_rate': 7.306368577883547e-06, 'epoch': 1.8}\n",
      "{'loss': 1.1474, 'grad_norm': 7.21986198425293, 'learning_rate': 7.273137715105063e-06, 'epoch': 1.81}\n",
      "{'loss': 1.0494, 'grad_norm': 14.233599662780762, 'learning_rate': 7.239939352055208e-06, 'epoch': 1.81}\n",
      "{'loss': 1.2008, 'grad_norm': 7.819028377532959, 'learning_rate': 7.2067738844040516e-06, 'epoch': 1.81}\n",
      "{'loss': 1.156, 'grad_norm': 9.173150062561035, 'learning_rate': 7.173641707429606e-06, 'epoch': 1.82}\n",
      "{'loss': 1.1182, 'grad_norm': 6.395073413848877, 'learning_rate': 7.140543216013109e-06, 'epoch': 1.82}\n",
      "{'loss': 1.133, 'grad_norm': 9.340950965881348, 'learning_rate': 7.107478804634324e-06, 'epoch': 1.82}\n",
      "{'loss': 1.2833, 'grad_norm': 9.35492992401123, 'learning_rate': 7.07444886736684e-06, 'epoch': 1.83}\n",
      "{'loss': 1.2872, 'grad_norm': 8.26357364654541, 'learning_rate': 7.041453797873363e-06, 'epoch': 1.83}\n",
      "{'loss': 1.3687, 'grad_norm': 7.56323766708374, 'learning_rate': 7.008493989401039e-06, 'epoch': 1.83}\n",
      "{'loss': 1.3177, 'grad_norm': 12.057421684265137, 'learning_rate': 6.975569834776757e-06, 'epoch': 1.84}\n",
      "{'loss': 1.299, 'grad_norm': 8.772522926330566, 'learning_rate': 6.942681726402474e-06, 'epoch': 1.84}\n",
      "{'loss': 1.1792, 'grad_norm': 7.906858444213867, 'learning_rate': 6.909830056250527e-06, 'epoch': 1.84}\n",
      "{'loss': 1.399, 'grad_norm': 7.554241180419922, 'learning_rate': 6.8770152158589806e-06, 'epoch': 1.84}\n",
      "{'loss': 1.3235, 'grad_norm': 10.228592872619629, 'learning_rate': 6.844237596326941e-06, 'epoch': 1.85}\n",
      "{'loss': 1.3706, 'grad_norm': 10.4474515914917, 'learning_rate': 6.811497588309901e-06, 'epoch': 1.85}\n",
      "{'loss': 1.4246, 'grad_norm': 8.3517484664917, 'learning_rate': 6.778795582015096e-06, 'epoch': 1.85}\n",
      "{'loss': 1.4508, 'grad_norm': 8.311480522155762, 'learning_rate': 6.746131967196834e-06, 'epoch': 1.86}\n",
      "{'loss': 1.3497, 'grad_norm': 8.277278900146484, 'learning_rate': 6.7135071331518575e-06, 'epoch': 1.86}\n",
      "{'loss': 1.2716, 'grad_norm': 7.848539352416992, 'learning_rate': 6.680921468714718e-06, 'epoch': 1.86}\n",
      "{'loss': 1.2848, 'grad_norm': 12.787500381469727, 'learning_rate': 6.648375362253119e-06, 'epoch': 1.87}\n",
      "{'loss': 1.2476, 'grad_norm': 7.814301490783691, 'learning_rate': 6.615869201663296e-06, 'epoch': 1.87}\n",
      "{'loss': 1.2492, 'grad_norm': 7.73518705368042, 'learning_rate': 6.583403374365406e-06, 'epoch': 1.87}\n",
      "{'loss': 1.2225, 'grad_norm': 7.878384113311768, 'learning_rate': 6.550978267298893e-06, 'epoch': 1.88}\n",
      "{'loss': 1.4966, 'grad_norm': 9.964056968688965, 'learning_rate': 6.518594266917883e-06, 'epoch': 1.88}\n",
      "{'loss': 1.3193, 'grad_norm': 7.431002616882324, 'learning_rate': 6.486251759186573e-06, 'epoch': 1.88}\n",
      "{'loss': 1.2123, 'grad_norm': 7.275434494018555, 'learning_rate': 6.453951129574644e-06, 'epoch': 1.89}\n",
      "{'loss': 1.1253, 'grad_norm': 10.359984397888184, 'learning_rate': 6.421692763052654e-06, 'epoch': 1.89}\n",
      "{'loss': 1.2249, 'grad_norm': 7.593205451965332, 'learning_rate': 6.3894770440874525e-06, 'epoch': 1.89}\n",
      "{'loss': 1.561, 'grad_norm': 11.667189598083496, 'learning_rate': 6.357304356637606e-06, 'epoch': 1.9}\n",
      "{'loss': 1.255, 'grad_norm': 12.292037963867188, 'learning_rate': 6.325175084148809e-06, 'epoch': 1.9}\n",
      "{'loss': 0.9966, 'grad_norm': 7.687627792358398, 'learning_rate': 6.293089609549325e-06, 'epoch': 1.9}\n",
      "{'loss': 1.2194, 'grad_norm': 7.700528144836426, 'learning_rate': 6.261048315245419e-06, 'epoch': 1.91}\n",
      "{'loss': 1.398, 'grad_norm': 11.076128005981445, 'learning_rate': 6.229051583116796e-06, 'epoch': 1.91}\n",
      "{'loss': 1.3553, 'grad_norm': 11.88124942779541, 'learning_rate': 6.197099794512056e-06, 'epoch': 1.91}\n",
      "{'loss': 1.237, 'grad_norm': 7.710247993469238, 'learning_rate': 6.165193330244144e-06, 'epoch': 1.92}\n",
      "{'loss': 1.1901, 'grad_norm': 12.207176208496094, 'learning_rate': 6.133332570585813e-06, 'epoch': 1.92}\n",
      "{'loss': 1.2343, 'grad_norm': 10.076744079589844, 'learning_rate': 6.101517895265094e-06, 'epoch': 1.92}\n",
      "{'loss': 1.2819, 'grad_norm': 10.389777183532715, 'learning_rate': 6.069749683460765e-06, 'epoch': 1.92}\n",
      "{'loss': 1.2305, 'grad_norm': 9.115442276000977, 'learning_rate': 6.03802831379784e-06, 'epoch': 1.93}\n",
      "{'loss': 1.2833, 'grad_norm': 11.678800582885742, 'learning_rate': 6.006354164343047e-06, 'epoch': 1.93}\n",
      "{'loss': 1.2453, 'grad_norm': 10.674928665161133, 'learning_rate': 5.9747276126003265e-06, 'epoch': 1.93}\n",
      "{'loss': 1.2914, 'grad_norm': 12.290911674499512, 'learning_rate': 5.943149035506337e-06, 'epoch': 1.94}\n",
      "{'loss': 0.9309, 'grad_norm': 7.473154544830322, 'learning_rate': 5.911618809425952e-06, 'epoch': 1.94}\n",
      "{'loss': 0.8642, 'grad_norm': 9.921716690063477, 'learning_rate': 5.880137310147782e-06, 'epoch': 1.94}\n",
      "{'loss': 1.0899, 'grad_norm': 11.355504989624023, 'learning_rate': 5.848704912879699e-06, 'epoch': 1.95}\n",
      "{'loss': 0.7426, 'grad_norm': 10.54151725769043, 'learning_rate': 5.8173219922443516e-06, 'epoch': 1.95}\n",
      "{'loss': 0.9626, 'grad_norm': 9.547293663024902, 'learning_rate': 5.785988922274711e-06, 'epoch': 1.95}\n",
      "{'loss': 1.0452, 'grad_norm': 11.873251914978027, 'learning_rate': 5.754706076409613e-06, 'epoch': 1.96}\n",
      "{'loss': 0.8819, 'grad_norm': 14.011845588684082, 'learning_rate': 5.723473827489301e-06, 'epoch': 1.96}\n",
      "{'loss': 1.0925, 'grad_norm': 9.763715744018555, 'learning_rate': 5.692292547750989e-06, 'epoch': 1.96}\n",
      "{'loss': 1.1582, 'grad_norm': 10.631465911865234, 'learning_rate': 5.66116260882442e-06, 'epoch': 1.97}\n",
      "{'loss': 1.4751, 'grad_norm': 10.212469100952148, 'learning_rate': 5.630084381727434e-06, 'epoch': 1.97}\n",
      "{'loss': 1.5387, 'grad_norm': 10.771257400512695, 'learning_rate': 5.599058236861559e-06, 'epoch': 1.97}\n",
      "{'loss': 1.3264, 'grad_norm': 12.329891204833984, 'learning_rate': 5.5680845440075885e-06, 'epoch': 1.98}\n",
      "{'loss': 1.0582, 'grad_norm': 9.105908393859863, 'learning_rate': 5.537163672321161e-06, 'epoch': 1.98}\n",
      "{'loss': 1.1617, 'grad_norm': 8.528635025024414, 'learning_rate': 5.5062959903283855e-06, 'epoch': 1.98}\n",
      "{'loss': 1.2691, 'grad_norm': 10.00726318359375, 'learning_rate': 5.475481865921441e-06, 'epoch': 1.99}\n",
      "{'loss': 1.2878, 'grad_norm': 12.437085151672363, 'learning_rate': 5.444721666354169e-06, 'epoch': 1.99}\n",
      "{'loss': 1.2689, 'grad_norm': 9.02550220489502, 'learning_rate': 5.414015758237734e-06, 'epoch': 1.99}\n",
      "{'loss': 1.0845, 'grad_norm': 9.598016738891602, 'learning_rate': 5.3833645075362295e-06, 'epoch': 2.0}\n",
      "{'loss': 0.9399, 'grad_norm': 12.952614784240723, 'learning_rate': 5.352768279562315e-06, 'epoch': 2.0}\n",
      "{'loss': 1.1046, 'grad_norm': 19.40254783630371, 'learning_rate': 5.32222743897288e-06, 'epoch': 2.0}\n",
      "{'loss': 0.9925, 'grad_norm': 9.31238842010498, 'learning_rate': 5.2917423497646834e-06, 'epoch': 2.0}\n",
      "{'loss': 1.0531, 'grad_norm': 10.0696439743042, 'learning_rate': 5.2613133752700145e-06, 'epoch': 2.01}\n",
      "{'loss': 1.1095, 'grad_norm': 10.005908966064453, 'learning_rate': 5.230940878152371e-06, 'epoch': 2.01}\n",
      "{'loss': 0.9987, 'grad_norm': 8.558470726013184, 'learning_rate': 5.200625220402139e-06, 'epoch': 2.01}\n",
      "{'loss': 1.0295, 'grad_norm': 8.735945701599121, 'learning_rate': 5.1703667633322575e-06, 'epoch': 2.02}\n",
      "{'loss': 1.0722, 'grad_norm': 8.929398536682129, 'learning_rate': 5.14016586757394e-06, 'epoch': 2.02}\n",
      "{'loss': 1.12, 'grad_norm': 9.480853080749512, 'learning_rate': 5.110022893072361e-06, 'epoch': 2.02}\n",
      "{'loss': 1.1938, 'grad_norm': 9.312661170959473, 'learning_rate': 5.079938199082363e-06, 'epoch': 2.03}\n",
      "{'loss': 1.2419, 'grad_norm': 10.786375045776367, 'learning_rate': 5.049912144164186e-06, 'epoch': 2.03}\n",
      "{'loss': 1.2917, 'grad_norm': 12.095785140991211, 'learning_rate': 5.019945086179192e-06, 'epoch': 2.03}\n",
      "{'loss': 1.2104, 'grad_norm': 11.38571548461914, 'learning_rate': 4.9900373822855805e-06, 'epoch': 2.04}\n",
      "{'loss': 1.1907, 'grad_norm': 13.698524475097656, 'learning_rate': 4.960189388934163e-06, 'epoch': 2.04}\n",
      "{'loss': 1.2675, 'grad_norm': 14.752087593078613, 'learning_rate': 4.930401461864099e-06, 'epoch': 2.04}\n",
      "{'loss': 1.2839, 'grad_norm': 11.668098449707031, 'learning_rate': 4.900673956098644e-06, 'epoch': 2.04}\n",
      "{'loss': 1.1684, 'grad_norm': 8.749882698059082, 'learning_rate': 4.87100722594094e-06, 'epoch': 2.05}\n",
      "{'loss': 1.1841, 'grad_norm': 10.85978889465332, 'learning_rate': 4.841401624969782e-06, 'epoch': 2.05}\n",
      "{'loss': 1.1028, 'grad_norm': 11.039562225341797, 'learning_rate': 4.811857506035407e-06, 'epoch': 2.05}\n",
      "{'loss': 1.0962, 'grad_norm': 9.479867935180664, 'learning_rate': 4.7823752212552855e-06, 'epoch': 2.06}\n",
      "{'loss': 1.3014, 'grad_norm': 11.98455810546875, 'learning_rate': 4.75295512200992e-06, 'epoch': 2.06}\n",
      "{'loss': 1.0584, 'grad_norm': 12.377083778381348, 'learning_rate': 4.7235975589386715e-06, 'epoch': 2.06}\n",
      "{'loss': 1.2081, 'grad_norm': 10.22639274597168, 'learning_rate': 4.694302881935574e-06, 'epoch': 2.07}\n",
      "{'loss': 1.2907, 'grad_norm': 10.16584587097168, 'learning_rate': 4.66507144014515e-06, 'epoch': 2.07}\n",
      "{'loss': 1.1335, 'grad_norm': 8.838982582092285, 'learning_rate': 4.635903581958276e-06, 'epoch': 2.07}\n",
      "{'loss': 1.2493, 'grad_norm': 10.061490058898926, 'learning_rate': 4.606799655008009e-06, 'epoch': 2.08}\n",
      "{'loss': 1.0995, 'grad_norm': 10.535850524902344, 'learning_rate': 4.5777600061654505e-06, 'epoch': 2.08}\n",
      "{'loss': 1.1451, 'grad_norm': 9.053805351257324, 'learning_rate': 4.5487849815356145e-06, 'epoch': 2.08}\n",
      "{'loss': 0.9808, 'grad_norm': 11.06705093383789, 'learning_rate': 4.519874926453303e-06, 'epoch': 2.09}\n",
      "{'loss': 1.0592, 'grad_norm': 8.520135879516602, 'learning_rate': 4.491030185478976e-06, 'epoch': 2.09}\n",
      "{'loss': 1.2225, 'grad_norm': 9.007512092590332, 'learning_rate': 4.462251102394669e-06, 'epoch': 2.09}\n",
      "{'loss': 0.9117, 'grad_norm': 9.583879470825195, 'learning_rate': 4.433538020199882e-06, 'epoch': 2.1}\n",
      "{'loss': 0.962, 'grad_norm': 9.465499877929688, 'learning_rate': 4.404891281107482e-06, 'epoch': 2.1}\n",
      "{'loss': 1.0307, 'grad_norm': 12.826892852783203, 'learning_rate': 4.3763112265396445e-06, 'epoch': 2.1}\n",
      "{'loss': 1.0033, 'grad_norm': 10.05333423614502, 'learning_rate': 4.347798197123777e-06, 'epoch': 2.11}\n",
      "{'loss': 0.9876, 'grad_norm': 7.932883262634277, 'learning_rate': 4.319352532688444e-06, 'epoch': 2.11}\n",
      "{'loss': 1.0711, 'grad_norm': 12.050382614135742, 'learning_rate': 4.290974572259342e-06, 'epoch': 2.11}\n",
      "{'loss': 0.9627, 'grad_norm': 8.358365058898926, 'learning_rate': 4.262664654055247e-06, 'epoch': 2.12}\n",
      "{'loss': 0.9681, 'grad_norm': 8.814803123474121, 'learning_rate': 4.234423115483971e-06, 'epoch': 2.12}\n",
      "{'loss': 1.2023, 'grad_norm': 9.960518836975098, 'learning_rate': 4.206250293138366e-06, 'epoch': 2.12}\n",
      "{'loss': 1.0009, 'grad_norm': 8.831777572631836, 'learning_rate': 4.178146522792296e-06, 'epoch': 2.12}\n",
      "{'loss': 1.0608, 'grad_norm': 11.36906623840332, 'learning_rate': 4.15011213939663e-06, 'epoch': 2.13}\n",
      "{'loss': 1.0516, 'grad_norm': 12.24788761138916, 'learning_rate': 4.12214747707527e-06, 'epoch': 2.13}\n",
      "{'loss': 0.9625, 'grad_norm': 10.550830841064453, 'learning_rate': 4.094252869121153e-06, 'epoch': 2.13}\n",
      "{'loss': 1.1176, 'grad_norm': 10.30018424987793, 'learning_rate': 4.066428647992275e-06, 'epoch': 2.14}\n",
      "{'loss': 1.0342, 'grad_norm': 8.993060111999512, 'learning_rate': 4.038675145307747e-06, 'epoch': 2.14}\n",
      "{'loss': 0.6903, 'grad_norm': 7.975245952606201, 'learning_rate': 4.010992691843829e-06, 'epoch': 2.14}\n",
      "{'loss': 0.9276, 'grad_norm': 10.44146728515625, 'learning_rate': 3.98338161752999e-06, 'epoch': 2.15}\n",
      "{'loss': 0.8189, 'grad_norm': 8.911561965942383, 'learning_rate': 3.955842251444978e-06, 'epoch': 2.15}\n",
      "{'loss': 0.5679, 'grad_norm': 8.66077995300293, 'learning_rate': 3.9283749218128885e-06, 'epoch': 2.15}\n",
      "{'loss': 0.7993, 'grad_norm': 10.426013946533203, 'learning_rate': 3.900979955999271e-06, 'epoch': 2.16}\n",
      "{'loss': 0.7405, 'grad_norm': 11.230121612548828, 'learning_rate': 3.8736576805072165e-06, 'epoch': 2.16}\n",
      "{'loss': 1.1602, 'grad_norm': 12.184996604919434, 'learning_rate': 3.846408420973456e-06, 'epoch': 2.16}\n",
      "{'loss': 0.943, 'grad_norm': 10.260454177856445, 'learning_rate': 3.819232502164499e-06, 'epoch': 2.17}\n",
      "{'loss': 1.0077, 'grad_norm': 10.320455551147461, 'learning_rate': 3.792130247972756e-06, 'epoch': 2.17}\n",
      "{'loss': 1.0058, 'grad_norm': 7.274523735046387, 'learning_rate': 3.7651019814126656e-06, 'epoch': 2.17}\n",
      "{'loss': 1.0207, 'grad_norm': 9.073034286499023, 'learning_rate': 3.738148024616863e-06, 'epoch': 2.18}\n",
      "{'loss': 1.1362, 'grad_norm': 8.30877685546875, 'learning_rate': 3.7112686988323353e-06, 'epoch': 2.18}\n",
      "{'loss': 1.0618, 'grad_norm': 8.419196128845215, 'learning_rate': 3.684464324416578e-06, 'epoch': 2.18}\n",
      "{'loss': 1.1543, 'grad_norm': 8.60595417022705, 'learning_rate': 3.6577352208338015e-06, 'epoch': 2.19}\n",
      "{'loss': 1.1296, 'grad_norm': 7.4001970291137695, 'learning_rate': 3.6310817066511106e-06, 'epoch': 2.19}\n",
      "{'loss': 1.1824, 'grad_norm': 9.545214653015137, 'learning_rate': 3.604504099534696e-06, 'epoch': 2.19}\n",
      "{'loss': 1.5176, 'grad_norm': 9.834617614746094, 'learning_rate': 3.578002716246074e-06, 'epoch': 2.2}\n",
      "{'loss': 1.1879, 'grad_norm': 10.92955207824707, 'learning_rate': 3.5515778726382967e-06, 'epoch': 2.2}\n",
      "{'loss': 1.1276, 'grad_norm': 11.625956535339355, 'learning_rate': 3.525229883652177e-06, 'epoch': 2.2}\n",
      "{'loss': 1.2392, 'grad_norm': 9.181438446044922, 'learning_rate': 3.4989590633125583e-06, 'epoch': 2.2}\n",
      "{'loss': 1.4399, 'grad_norm': 10.990561485290527, 'learning_rate': 3.4727657247245607e-06, 'epoch': 2.21}\n",
      "{'loss': 1.1009, 'grad_norm': 8.660582542419434, 'learning_rate': 3.446650180069837e-06, 'epoch': 2.21}\n",
      "{'loss': 1.0184, 'grad_norm': 7.365581512451172, 'learning_rate': 3.4206127406028744e-06, 'epoch': 2.21}\n",
      "{'loss': 1.0696, 'grad_norm': 7.511904716491699, 'learning_rate': 3.394653716647277e-06, 'epoch': 2.22}\n",
      "{'loss': 1.0935, 'grad_norm': 8.917716979980469, 'learning_rate': 3.3687734175920505e-06, 'epoch': 2.22}\n",
      "{'loss': 1.2478, 'grad_norm': 9.114049911499023, 'learning_rate': 3.342972151887941e-06, 'epoch': 2.22}\n",
      "{'loss': 1.0586, 'grad_norm': 10.553746223449707, 'learning_rate': 3.317250227043746e-06, 'epoch': 2.23}\n",
      "{'loss': 1.2063, 'grad_norm': 10.683267593383789, 'learning_rate': 3.2916079496226407e-06, 'epoch': 2.23}\n",
      "{'loss': 1.0682, 'grad_norm': 7.82099723815918, 'learning_rate': 3.266045625238539e-06, 'epoch': 2.23}\n",
      "{'loss': 1.133, 'grad_norm': 8.731369972229004, 'learning_rate': 3.2405635585524566e-06, 'epoch': 2.24}\n",
      "{'loss': 1.2762, 'grad_norm': 15.09862232208252, 'learning_rate': 3.21516205326885e-06, 'epoch': 2.24}\n",
      "{'loss': 1.0854, 'grad_norm': 7.646235942840576, 'learning_rate': 3.1898414121320277e-06, 'epoch': 2.24}\n",
      "{'loss': 1.0882, 'grad_norm': 7.90578556060791, 'learning_rate': 3.1646019369225277e-06, 'epoch': 2.25}\n",
      "{'loss': 1.1578, 'grad_norm': 7.401391983032227, 'learning_rate': 3.1394439284535206e-06, 'epoch': 2.25}\n",
      "{'loss': 1.1347, 'grad_norm': 7.966800689697266, 'learning_rate': 3.114367686567228e-06, 'epoch': 2.25}\n",
      "{'loss': 1.0686, 'grad_norm': 10.457164764404297, 'learning_rate': 3.089373510131354e-06, 'epoch': 2.26}\n",
      "{'loss': 1.0477, 'grad_norm': 7.873352527618408, 'learning_rate': 3.064461697035506e-06, 'epoch': 2.26}\n",
      "{'loss': 0.9581, 'grad_norm': 7.998051643371582, 'learning_rate': 3.0396325441876627e-06, 'epoch': 2.26}\n",
      "{'loss': 0.9134, 'grad_norm': 7.798986911773682, 'learning_rate': 3.0148863475106315e-06, 'epoch': 2.27}\n",
      "{'loss': 1.2566, 'grad_norm': 8.074917793273926, 'learning_rate': 2.9902234019385056e-06, 'epoch': 2.27}\n",
      "{'loss': 0.9226, 'grad_norm': 10.722527503967285, 'learning_rate': 2.9656440014131737e-06, 'epoch': 2.27}\n",
      "{'loss': 0.9427, 'grad_norm': 12.083124160766602, 'learning_rate': 2.941148438880803e-06, 'epoch': 2.28}\n",
      "{'loss': 0.8939, 'grad_norm': 8.042149543762207, 'learning_rate': 2.9167370062883403e-06, 'epoch': 2.28}\n",
      "{'loss': 1.0812, 'grad_norm': 8.69479751586914, 'learning_rate': 2.8924099945800533e-06, 'epoch': 2.28}\n",
      "{'loss': 0.9456, 'grad_norm': 9.80831241607666, 'learning_rate': 2.8681676936940397e-06, 'epoch': 2.28}\n",
      "{'loss': 1.0379, 'grad_norm': 8.051023483276367, 'learning_rate': 2.8440103925587904e-06, 'epoch': 2.29}\n",
      "{'loss': 0.9719, 'grad_norm': 9.413684844970703, 'learning_rate': 2.8199383790897405e-06, 'epoch': 2.29}\n",
      "{'loss': 1.0981, 'grad_norm': 10.060179710388184, 'learning_rate': 2.795951940185827e-06, 'epoch': 2.29}\n",
      "{'loss': 0.9763, 'grad_norm': 10.1113920211792, 'learning_rate': 2.7720513617260857e-06, 'epoch': 2.3}\n",
      "{'loss': 0.9655, 'grad_norm': 10.4473876953125, 'learning_rate': 2.748236928566238e-06, 'epoch': 2.3}\n",
      "{'loss': 0.9496, 'grad_norm': 12.850913047790527, 'learning_rate': 2.7245089245352864e-06, 'epoch': 2.3}\n",
      "{'loss': 0.7574, 'grad_norm': 10.063583374023438, 'learning_rate': 2.700867632432145e-06, 'epoch': 2.31}\n",
      "{'loss': 0.7814, 'grad_norm': 10.494169235229492, 'learning_rate': 2.6773133340222677e-06, 'epoch': 2.31}\n",
      "{'loss': 0.6831, 'grad_norm': 13.830262184143066, 'learning_rate': 2.6538463100342773e-06, 'epoch': 2.31}\n",
      "{'loss': 0.5775, 'grad_norm': 11.440420150756836, 'learning_rate': 2.6304668401566334e-06, 'epoch': 2.32}\n",
      "{'loss': 0.6661, 'grad_norm': 12.880931854248047, 'learning_rate': 2.607175203034299e-06, 'epoch': 2.32}\n",
      "{'loss': 1.1216, 'grad_norm': 9.455260276794434, 'learning_rate': 2.5839716762654e-06, 'epoch': 2.32}\n",
      "{'loss': 0.9856, 'grad_norm': 7.911375999450684, 'learning_rate': 2.56085653639795e-06, 'epoch': 2.33}\n",
      "{'loss': 0.9509, 'grad_norm': 9.114357948303223, 'learning_rate': 2.5378300589265258e-06, 'epoch': 2.33}\n",
      "{'loss': 0.9654, 'grad_norm': 11.126029014587402, 'learning_rate': 2.514892518288988e-06, 'epoch': 2.33}\n",
      "{'loss': 1.0442, 'grad_norm': 10.003908157348633, 'learning_rate': 2.4920441878632273e-06, 'epoch': 2.34}\n",
      "{'loss': 0.9923, 'grad_norm': 8.652706146240234, 'learning_rate': 2.469285339963892e-06, 'epoch': 2.34}\n",
      "{'loss': 1.085, 'grad_norm': 11.994494438171387, 'learning_rate': 2.4466162458391364e-06, 'epoch': 2.34}\n",
      "{'loss': 1.1904, 'grad_norm': 13.14014720916748, 'learning_rate': 2.4240371756674063e-06, 'epoch': 2.35}\n",
      "{'loss': 1.3826, 'grad_norm': 10.44692611694336, 'learning_rate': 2.401548398554213e-06, 'epoch': 2.35}\n",
      "{'loss': 1.2776, 'grad_norm': 11.278865814208984, 'learning_rate': 2.379150182528909e-06, 'epoch': 2.35}\n",
      "{'loss': 1.1554, 'grad_norm': 10.533470153808594, 'learning_rate': 2.3568427945415163e-06, 'epoch': 2.36}\n",
      "{'loss': 1.2947, 'grad_norm': 9.836421012878418, 'learning_rate': 2.334626500459539e-06, 'epoch': 2.36}\n",
      "{'loss': 1.3781, 'grad_norm': 11.232010841369629, 'learning_rate': 2.3125015650647798e-06, 'epoch': 2.36}\n",
      "{'loss': 1.0177, 'grad_norm': 10.154953956604004, 'learning_rate': 2.290468252050204e-06, 'epoch': 2.36}\n",
      "{'loss': 1.3796, 'grad_norm': 16.770029067993164, 'learning_rate': 2.26852682401679e-06, 'epoch': 2.37}\n",
      "{'loss': 1.4475, 'grad_norm': 12.046056747436523, 'learning_rate': 2.246677542470388e-06, 'epoch': 2.37}\n",
      "{'loss': 1.1143, 'grad_norm': 16.914533615112305, 'learning_rate': 2.224920667818622e-06, 'epoch': 2.37}\n",
      "{'loss': 1.2432, 'grad_norm': 12.661273002624512, 'learning_rate': 2.2032564593677773e-06, 'epoch': 2.38}\n",
      "{'loss': 1.3541, 'grad_norm': 11.138178825378418, 'learning_rate': 2.1816851753197023e-06, 'epoch': 2.38}\n",
      "{'loss': 1.1582, 'grad_norm': 9.724348068237305, 'learning_rate': 2.1602070727687463e-06, 'epoch': 2.38}\n",
      "{'loss': 1.2002, 'grad_norm': 9.721034049987793, 'learning_rate': 2.1388224076986872e-06, 'epoch': 2.39}\n",
      "{'loss': 1.0612, 'grad_norm': 13.810600280761719, 'learning_rate': 2.117531434979675e-06, 'epoch': 2.39}\n",
      "{'loss': 1.1013, 'grad_norm': 10.274882316589355, 'learning_rate': 2.096334408365207e-06, 'epoch': 2.39}\n",
      "{'loss': 1.076, 'grad_norm': 9.950191497802734, 'learning_rate': 2.075231580489098e-06, 'epoch': 2.4}\n",
      "{'loss': 1.0347, 'grad_norm': 9.439678192138672, 'learning_rate': 2.0542232028624585e-06, 'epoch': 2.4}\n",
      "{'loss': 1.1127, 'grad_norm': 10.45068645477295, 'learning_rate': 2.033309525870717e-06, 'epoch': 2.4}\n",
      "{'loss': 0.958, 'grad_norm': 14.401384353637695, 'learning_rate': 2.0124907987706243e-06, 'epoch': 2.41}\n",
      "{'loss': 1.1104, 'grad_norm': 9.415379524230957, 'learning_rate': 1.991767269687278e-06, 'epoch': 2.41}\n",
      "{'loss': 1.0655, 'grad_norm': 13.533660888671875, 'learning_rate': 1.971139185611176e-06, 'epoch': 2.41}\n",
      "{'loss': 0.9132, 'grad_norm': 15.619601249694824, 'learning_rate': 1.9506067923952676e-06, 'epoch': 2.42}\n",
      "{'loss': 1.0211, 'grad_norm': 10.648078918457031, 'learning_rate': 1.930170334752025e-06, 'epoch': 2.42}\n",
      "{'loss': 1.1715, 'grad_norm': 14.409460067749023, 'learning_rate': 1.9098300562505266e-06, 'epoch': 2.42}\n",
      "{'loss': 1.1417, 'grad_norm': 14.02224063873291, 'learning_rate': 1.8895861993135444e-06, 'epoch': 2.43}\n",
      "{'loss': 1.1025, 'grad_norm': 14.46092700958252, 'learning_rate': 1.8694390052146737e-06, 'epoch': 2.43}\n",
      "{'loss': 1.0424, 'grad_norm': 13.028404235839844, 'learning_rate': 1.8493887140754462e-06, 'epoch': 2.43}\n",
      "{'loss': 1.0847, 'grad_norm': 14.046993255615234, 'learning_rate': 1.8294355648624607e-06, 'epoch': 2.44}\n",
      "{'loss': 0.9709, 'grad_norm': 13.750466346740723, 'learning_rate': 1.8095797953845507e-06, 'epoch': 2.44}\n",
      "{'loss': 1.1404, 'grad_norm': 9.754804611206055, 'learning_rate': 1.789821642289945e-06, 'epoch': 2.44}\n",
      "{'loss': 1.1808, 'grad_norm': 10.252853393554688, 'learning_rate': 1.7701613410634367e-06, 'epoch': 2.44}\n",
      "{'loss': 0.9595, 'grad_norm': 45.715396881103516, 'learning_rate': 1.750599126023591e-06, 'epoch': 2.45}\n",
      "{'loss': 1.1885, 'grad_norm': 15.594269752502441, 'learning_rate': 1.731135230319948e-06, 'epoch': 2.45}\n",
      "{'loss': 0.9495, 'grad_norm': 10.916524887084961, 'learning_rate': 1.7117698859302357e-06, 'epoch': 2.45}\n",
      "{'loss': 0.9858, 'grad_norm': 10.418642044067383, 'learning_rate': 1.692503323657617e-06, 'epoch': 2.46}\n",
      "{'loss': 1.1611, 'grad_norm': 9.676848411560059, 'learning_rate': 1.6733357731279375e-06, 'epoch': 2.46}\n",
      "{'loss': 0.829, 'grad_norm': 11.410253524780273, 'learning_rate': 1.6542674627869738e-06, 'epoch': 2.46}\n",
      "{'loss': 0.858, 'grad_norm': 10.25106143951416, 'learning_rate': 1.6352986198977327e-06, 'epoch': 2.47}\n",
      "{'loss': 0.9086, 'grad_norm': 12.180051803588867, 'learning_rate': 1.6164294705377292e-06, 'epoch': 2.47}\n",
      "{'loss': 0.7447, 'grad_norm': 10.351420402526855, 'learning_rate': 1.5976602395962892e-06, 'epoch': 2.47}\n",
      "{'loss': 0.8855, 'grad_norm': 20.452058792114258, 'learning_rate': 1.5789911507718824e-06, 'epoch': 2.48}\n",
      "{'loss': 0.8259, 'grad_norm': 12.324240684509277, 'learning_rate': 1.560422426569449e-06, 'epoch': 2.48}\n",
      "{'loss': 0.9402, 'grad_norm': 13.175338745117188, 'learning_rate': 1.5419542882977367e-06, 'epoch': 2.48}\n",
      "{'loss': 1.0759, 'grad_norm': 8.076598167419434, 'learning_rate': 1.523586956066686e-06, 'epoch': 2.49}\n",
      "{'loss': 0.9529, 'grad_norm': 9.902657508850098, 'learning_rate': 1.5053206487847916e-06, 'epoch': 2.49}\n",
      "{'loss': 0.8685, 'grad_norm': 9.04281234741211, 'learning_rate': 1.4871555841564889e-06, 'epoch': 2.49}\n",
      "{'loss': 1.0274, 'grad_norm': 11.32139778137207, 'learning_rate': 1.4690919786795766e-06, 'epoch': 2.5}\n",
      "{'loss': 1.1334, 'grad_norm': 18.41847801208496, 'learning_rate': 1.4511300476426227e-06, 'epoch': 2.5}\n",
      "{'loss': 1.2599, 'grad_norm': 9.678349494934082, 'learning_rate': 1.433270005122399e-06, 'epoch': 2.5}\n",
      "{'loss': 1.1444, 'grad_norm': 9.974730491638184, 'learning_rate': 1.4155120639813392e-06, 'epoch': 2.51}\n",
      "{'loss': 1.2375, 'grad_norm': 13.125566482543945, 'learning_rate': 1.3978564358649926e-06, 'epoch': 2.51}\n",
      "{'loss': 1.208, 'grad_norm': 9.207242965698242, 'learning_rate': 1.3803033311995072e-06, 'epoch': 2.51}\n",
      "{'loss': 1.2477, 'grad_norm': 9.962656021118164, 'learning_rate': 1.3628529591891181e-06, 'epoch': 2.52}\n",
      "{'loss': 1.114, 'grad_norm': 10.041435241699219, 'learning_rate': 1.345505527813652e-06, 'epoch': 2.52}\n",
      "{'loss': 1.2789, 'grad_norm': 9.576586723327637, 'learning_rate': 1.3282612438260578e-06, 'epoch': 2.52}\n",
      "{'loss': 1.2862, 'grad_norm': 11.846923828125, 'learning_rate': 1.311120312749935e-06, 'epoch': 2.52}\n",
      "{'loss': 1.3148, 'grad_norm': 12.769562721252441, 'learning_rate': 1.2940829388770837e-06, 'epoch': 2.53}\n",
      "{'loss': 1.2267, 'grad_norm': 10.29100227355957, 'learning_rate': 1.2771493252650723e-06, 'epoch': 2.53}\n",
      "{'loss': 1.2786, 'grad_norm': 12.404952049255371, 'learning_rate': 1.2603196737348211e-06, 'epoch': 2.53}\n",
      "{'loss': 1.2384, 'grad_norm': 11.241174697875977, 'learning_rate': 1.2435941848681864e-06, 'epoch': 2.54}\n",
      "{'loss': 1.1897, 'grad_norm': 12.166672706604004, 'learning_rate': 1.2269730580055806e-06, 'epoch': 2.54}\n",
      "{'loss': 1.3005, 'grad_norm': 9.659017562866211, 'learning_rate': 1.2104564912435924e-06, 'epoch': 2.54}\n",
      "{'loss': 1.0067, 'grad_norm': 8.292366981506348, 'learning_rate': 1.19404468143262e-06, 'epoch': 2.55}\n",
      "{'loss': 1.0542, 'grad_norm': 13.296773910522461, 'learning_rate': 1.1777378241745385e-06, 'epoch': 2.55}\n",
      "{'loss': 1.2331, 'grad_norm': 12.398039817810059, 'learning_rate': 1.1615361138203574e-06, 'epoch': 2.55}\n",
      "{'loss': 1.1187, 'grad_norm': 11.00707721710205, 'learning_rate': 1.1454397434679022e-06, 'epoch': 2.56}\n",
      "{'loss': 1.175, 'grad_norm': 12.509212493896484, 'learning_rate': 1.1294489049595247e-06, 'epoch': 2.56}\n",
      "{'loss': 1.0333, 'grad_norm': 9.451119422912598, 'learning_rate': 1.1135637888798101e-06, 'epoch': 2.56}\n",
      "{'loss': 1.1168, 'grad_norm': 12.599231719970703, 'learning_rate': 1.0977845845533009e-06, 'epoch': 2.57}\n",
      "{'loss': 1.1264, 'grad_norm': 8.780135154724121, 'learning_rate': 1.0821114800422482e-06, 'epoch': 2.57}\n",
      "{'loss': 1.2221, 'grad_norm': 11.955119132995605, 'learning_rate': 1.066544662144371e-06, 'epoch': 2.57}\n",
      "{'loss': 1.0591, 'grad_norm': 12.834741592407227, 'learning_rate': 1.0510843163906148e-06, 'epoch': 2.58}\n",
      "{'loss': 1.2057, 'grad_norm': 11.893834114074707, 'learning_rate': 1.0357306270429623e-06, 'epoch': 2.58}\n",
      "{'loss': 1.0825, 'grad_norm': 10.100226402282715, 'learning_rate': 1.020483777092226e-06, 'epoch': 2.58}\n",
      "{'loss': 1.0534, 'grad_norm': 10.302994728088379, 'learning_rate': 1.0053439482558602e-06, 'epoch': 2.59}\n",
      "{'loss': 1.0168, 'grad_norm': 10.157187461853027, 'learning_rate': 9.903113209758098e-07, 'epoch': 2.59}\n",
      "{'loss': 0.9396, 'grad_norm': 10.557306289672852, 'learning_rate': 9.753860744163524e-07, 'epoch': 2.59}\n",
      "{'loss': 1.1653, 'grad_norm': 10.962141036987305, 'learning_rate': 9.605683864619574e-07, 'epoch': 2.6}\n",
      "{'loss': 0.8722, 'grad_norm': 11.127864837646484, 'learning_rate': 9.458584337151811e-07, 'epoch': 2.6}\n",
      "{'loss': 1.0471, 'grad_norm': 11.223523139953613, 'learning_rate': 9.312563914945461e-07, 'epoch': 2.6}\n",
      "{'loss': 1.0452, 'grad_norm': 9.747879981994629, 'learning_rate': 9.167624338324599e-07, 'epoch': 2.6}\n",
      "{'loss': 1.1547, 'grad_norm': 11.648822784423828, 'learning_rate': 9.023767334731426e-07, 'epoch': 2.61}\n",
      "{'loss': 1.1111, 'grad_norm': 10.780269622802734, 'learning_rate': 8.880994618705574e-07, 'epoch': 2.61}\n",
      "{'loss': 0.9914, 'grad_norm': 11.514026641845703, 'learning_rate': 8.739307891863813e-07, 'epoch': 2.61}\n",
      "{'loss': 0.8802, 'grad_norm': 20.184314727783203, 'learning_rate': 8.598708842879688e-07, 'epoch': 2.62}\n",
      "{'loss': 0.9102, 'grad_norm': 9.364696502685547, 'learning_rate': 8.459199147463371e-07, 'epoch': 2.62}\n",
      "{'loss': 1.0278, 'grad_norm': 10.7337064743042, 'learning_rate': 8.320780468341761e-07, 'epoch': 2.62}\n",
      "{'loss': 0.7582, 'grad_norm': 8.56815242767334, 'learning_rate': 8.183454455238638e-07, 'epoch': 2.63}\n",
      "{'loss': 0.7889, 'grad_norm': 11.106283187866211, 'learning_rate': 8.047222744854943e-07, 'epoch': 2.63}\n",
      "{'loss': 0.7079, 'grad_norm': 11.608138084411621, 'learning_rate': 7.912086960849374e-07, 'epoch': 2.63}\n",
      "{'loss': 0.7483, 'grad_norm': 11.032539367675781, 'learning_rate': 7.778048713818975e-07, 'epoch': 2.64}\n",
      "{'loss': 0.6536, 'grad_norm': 12.358854293823242, 'learning_rate': 7.645109601279921e-07, 'epoch': 2.64}\n",
      "{'loss': 0.9606, 'grad_norm': 10.761581420898438, 'learning_rate': 7.513271207648531e-07, 'epoch': 2.64}\n",
      "{'loss': 1.0945, 'grad_norm': 7.770103454589844, 'learning_rate': 7.382535104222366e-07, 'epoch': 2.65}\n",
      "{'loss': 1.0204, 'grad_norm': 7.577558994293213, 'learning_rate': 7.252902849161436e-07, 'epoch': 2.65}\n",
      "{'loss': 1.0494, 'grad_norm': 10.386930465698242, 'learning_rate': 7.124375987469767e-07, 'epoch': 2.65}\n",
      "{'loss': 0.9455, 'grad_norm': 6.6203083992004395, 'learning_rate': 6.996956050976878e-07, 'epoch': 2.66}\n",
      "{'loss': 1.0529, 'grad_norm': 7.73621129989624, 'learning_rate': 6.870644558319528e-07, 'epoch': 2.66}\n",
      "{'loss': 1.1308, 'grad_norm': 9.051628112792969, 'learning_rate': 6.745443014923658e-07, 'epoch': 2.66}\n",
      "{'loss': 1.2215, 'grad_norm': 10.67851734161377, 'learning_rate': 6.621352912986468e-07, 'epoch': 2.67}\n",
      "{'loss': 1.228, 'grad_norm': 11.661253929138184, 'learning_rate': 6.498375731458529e-07, 'epoch': 2.67}\n",
      "{'loss': 1.2262, 'grad_norm': 9.546639442443848, 'learning_rate': 6.37651293602628e-07, 'epoch': 2.67}\n",
      "{'loss': 1.351, 'grad_norm': 9.353370666503906, 'learning_rate': 6.255765979094519e-07, 'epoch': 2.68}\n",
      "{'loss': 1.302, 'grad_norm': 8.038541793823242, 'learning_rate': 6.136136299768991e-07, 'epoch': 2.68}\n",
      "{'loss': 1.1675, 'grad_norm': 12.249580383300781, 'learning_rate': 6.017625323839415e-07, 'epoch': 2.68}\n",
      "{'loss': 1.1191, 'grad_norm': 8.013357162475586, 'learning_rate': 5.900234463762367e-07, 'epoch': 2.68}\n",
      "{'loss': 1.3464, 'grad_norm': 9.420924186706543, 'learning_rate': 5.783965118644441e-07, 'epoch': 2.69}\n",
      "{'loss': 1.0771, 'grad_norm': 10.872457504272461, 'learning_rate': 5.668818674225684e-07, 'epoch': 2.69}\n",
      "{'loss': 1.0256, 'grad_norm': 8.324236869812012, 'learning_rate': 5.554796502862958e-07, 'epoch': 2.69}\n",
      "{'loss': 1.4373, 'grad_norm': 10.39631175994873, 'learning_rate': 5.441899963513631e-07, 'epoch': 2.7}\n",
      "{'loss': 1.0148, 'grad_norm': 12.24020767211914, 'learning_rate': 5.330130401719413e-07, 'epoch': 2.7}\n",
      "{'loss': 1.077, 'grad_norm': 12.39035415649414, 'learning_rate': 5.219489149590251e-07, 'epoch': 2.7}\n",
      "{'loss': 1.0719, 'grad_norm': 8.458290100097656, 'learning_rate': 5.109977525788512e-07, 'epoch': 2.71}\n",
      "{'loss': 0.9455, 'grad_norm': 8.561354637145996, 'learning_rate': 5.001596835513256e-07, 'epoch': 2.71}\n",
      "{'loss': 1.1273, 'grad_norm': 9.075613021850586, 'learning_rate': 4.894348370484648e-07, 'epoch': 2.71}\n",
      "{'loss': 0.9953, 'grad_norm': 8.973812103271484, 'learning_rate': 4.788233408928588e-07, 'epoch': 2.72}\n",
      "{'loss': 1.2275, 'grad_norm': 12.91464614868164, 'learning_rate': 4.6832532155614895e-07, 'epoch': 2.72}\n",
      "{'loss': 1.0794, 'grad_norm': 10.935402870178223, 'learning_rate': 4.5794090415751666e-07, 'epoch': 2.72}\n",
      "{'loss': 1.08, 'grad_norm': 10.29338264465332, 'learning_rate': 4.4767021246219566e-07, 'epoch': 2.73}\n",
      "{'loss': 1.1695, 'grad_norm': 12.2828369140625, 'learning_rate': 4.3751336887999597e-07, 'epoch': 2.73}\n",
      "{'loss': 1.1784, 'grad_norm': 10.734950065612793, 'learning_rate': 4.27470494463843e-07, 'epoch': 2.73}\n",
      "{'loss': 1.2367, 'grad_norm': 12.626730918884277, 'learning_rate': 4.1754170890833777e-07, 'epoch': 2.74}\n",
      "{'loss': 1.1415, 'grad_norm': 12.288880348205566, 'learning_rate': 4.077271305483321e-07, 'epoch': 2.74}\n",
      "{'loss': 1.1483, 'grad_norm': 11.096635818481445, 'learning_rate': 3.980268763575079e-07, 'epoch': 2.74}\n",
      "{'loss': 1.0768, 'grad_norm': 9.659050941467285, 'learning_rate': 3.8844106194699696e-07, 'epoch': 2.75}\n",
      "{'loss': 1.1231, 'grad_norm': 15.0892972946167, 'learning_rate': 3.7896980156399533e-07, 'epoch': 2.75}\n",
      "{'loss': 0.9019, 'grad_norm': 9.045454025268555, 'learning_rate': 3.6961320809039914e-07, 'epoch': 2.75}\n",
      "{'loss': 1.0963, 'grad_norm': 11.604092597961426, 'learning_rate': 3.603713930414676e-07, 'epoch': 2.76}\n",
      "{'loss': 1.0471, 'grad_norm': 19.177867889404297, 'learning_rate': 3.5124446656448654e-07, 'epoch': 2.76}\n",
      "{'loss': 1.0053, 'grad_norm': 11.57597541809082, 'learning_rate': 3.42232537437458e-07, 'epoch': 2.76}\n",
      "{'loss': 0.9265, 'grad_norm': 10.609359741210938, 'learning_rate': 3.33335713067805e-07, 'epoch': 2.76}\n",
      "{'loss': 1.0429, 'grad_norm': 10.590194702148438, 'learning_rate': 3.245540994910934e-07, 'epoch': 2.77}\n",
      "{'loss': 1.0155, 'grad_norm': 11.960984230041504, 'learning_rate': 3.158878013697586e-07, 'epoch': 2.77}\n",
      "{'loss': 0.8595, 'grad_norm': 14.082931518554688, 'learning_rate': 3.073369219918698e-07, 'epoch': 2.77}\n",
      "{'loss': 1.0401, 'grad_norm': 13.758130073547363, 'learning_rate': 2.989015632698944e-07, 'epoch': 2.78}\n",
      "{'loss': 1.0967, 'grad_norm': 11.494680404663086, 'learning_rate': 2.905818257394799e-07, 'epoch': 2.78}\n",
      "{'loss': 0.7772, 'grad_norm': 9.844059944152832, 'learning_rate': 2.8237780855825957e-07, 'epoch': 2.78}\n",
      "{'loss': 0.9628, 'grad_norm': 12.687658309936523, 'learning_rate': 2.742896095046732e-07, 'epoch': 2.79}\n",
      "{'loss': 0.9008, 'grad_norm': 11.48720645904541, 'learning_rate': 2.6631732497679363e-07, 'epoch': 2.79}\n",
      "{'loss': 0.8691, 'grad_norm': 11.887350082397461, 'learning_rate': 2.584610499911833e-07, 'epoch': 2.79}\n",
      "{'loss': 0.7918, 'grad_norm': 11.939542770385742, 'learning_rate': 2.507208781817638e-07, 'epoch': 2.8}\n",
      "{'loss': 0.7262, 'grad_norm': 15.061749458312988, 'learning_rate': 2.4309690179869503e-07, 'epoch': 2.8}\n",
      "{'loss': 1.1023, 'grad_norm': 8.665984153747559, 'learning_rate': 2.355892117072789e-07, 'epoch': 2.8}\n",
      "{'loss': 0.9416, 'grad_norm': 9.066670417785645, 'learning_rate': 2.2819789738687482e-07, 'epoch': 2.81}\n",
      "{'loss': 0.9363, 'grad_norm': 8.193490982055664, 'learning_rate': 2.2092304692983402e-07, 'epoch': 2.81}\n",
      "{'loss': 0.9154, 'grad_norm': 7.852944374084473, 'learning_rate': 2.1376474704044693e-07, 'epoch': 2.81}\n",
      "{'loss': 0.9397, 'grad_norm': 14.032806396484375, 'learning_rate': 2.067230830339184e-07, 'epoch': 2.82}\n",
      "{'loss': 0.9817, 'grad_norm': 10.423746109008789, 'learning_rate': 1.9979813883533762e-07, 'epoch': 2.82}\n",
      "{'loss': 1.0743, 'grad_norm': 8.122654914855957, 'learning_rate': 1.929899969786897e-07, 'epoch': 2.82}\n",
      "{'loss': 1.1533, 'grad_norm': 8.166731834411621, 'learning_rate': 1.8629873860586567e-07, 'epoch': 2.83}\n",
      "{'loss': 1.0709, 'grad_norm': 18.270212173461914, 'learning_rate': 1.7972444346569752e-07, 'epoch': 2.83}\n",
      "{'loss': 1.3004, 'grad_norm': 10.959127426147461, 'learning_rate': 1.7326718991300563e-07, 'epoch': 2.83}\n",
      "{'loss': 1.2222, 'grad_norm': 8.938993453979492, 'learning_rate': 1.6692705490766958e-07, 'epoch': 2.84}\n",
      "{'loss': 1.3746, 'grad_norm': 13.3725004196167, 'learning_rate': 1.6070411401370335e-07, 'epoch': 2.84}\n",
      "{'loss': 1.2276, 'grad_norm': 20.989852905273438, 'learning_rate': 1.5459844139836476e-07, 'epoch': 2.84}\n",
      "{'loss': 1.04, 'grad_norm': 8.77480697631836, 'learning_rate': 1.4861010983126202e-07, 'epoch': 2.84}\n",
      "{'loss': 1.1453, 'grad_norm': 11.504671096801758, 'learning_rate': 1.4273919068349184e-07, 'epoch': 2.85}\n",
      "{'loss': 1.3138, 'grad_norm': 9.645145416259766, 'learning_rate': 1.3698575392678492e-07, 'epoch': 2.85}\n",
      "{'loss': 1.2894, 'grad_norm': 10.392913818359375, 'learning_rate': 1.3134986813267968e-07, 'epoch': 2.85}\n",
      "{'loss': 1.0888, 'grad_norm': 10.017230033874512, 'learning_rate': 1.258316004716953e-07, 'epoch': 2.86}\n",
      "{'loss': 1.1159, 'grad_norm': 32.234344482421875, 'learning_rate': 1.2043101671253553e-07, 'epoch': 2.86}\n",
      "{'loss': 1.0556, 'grad_norm': 9.380772590637207, 'learning_rate': 1.1514818122130844e-07, 'epoch': 2.86}\n",
      "{'loss': 1.1951, 'grad_norm': 11.791672706604004, 'learning_rate': 1.0998315696075123e-07, 'epoch': 2.87}\n",
      "{'loss': 1.0711, 'grad_norm': 9.766292572021484, 'learning_rate': 1.0493600548948879e-07, 'epoch': 2.87}\n",
      "{'loss': 1.0793, 'grad_norm': 11.93438720703125, 'learning_rate': 1.0000678696129307e-07, 'epoch': 2.87}\n",
      "{'loss': 1.0566, 'grad_norm': 9.533082962036133, 'learning_rate': 9.519556012436815e-08, 'epoch': 2.88}\n",
      "{'loss': 1.028, 'grad_norm': 9.23598861694336, 'learning_rate': 9.0502382320653e-08, 'epoch': 2.88}\n",
      "{'loss': 1.2186, 'grad_norm': 12.016555786132812, 'learning_rate': 8.592730948513205e-08, 'epoch': 2.88}\n",
      "{'loss': 1.0498, 'grad_norm': 10.064327239990234, 'learning_rate': 8.147039614517571e-08, 'epoch': 2.89}\n",
      "{'loss': 1.0095, 'grad_norm': 8.350394248962402, 'learning_rate': 7.71316954198853e-08, 'epoch': 2.89}\n",
      "{'loss': 1.0874, 'grad_norm': 11.089082717895508, 'learning_rate': 7.291125901946027e-08, 'epoch': 2.89}\n",
      "{'loss': 1.0979, 'grad_norm': 10.763749122619629, 'learning_rate': 6.880913724458538e-08, 'epoch': 2.9}\n",
      "{'loss': 0.9606, 'grad_norm': 16.711402893066406, 'learning_rate': 6.482537898582886e-08, 'epoch': 2.9}\n",
      "{'loss': 1.0176, 'grad_norm': 10.152336120605469, 'learning_rate': 6.096003172305742e-08, 'epoch': 2.9}\n",
      "{'loss': 0.983, 'grad_norm': 8.367863655090332, 'learning_rate': 5.721314152487556e-08, 'epoch': 2.91}\n",
      "{'loss': 1.2199, 'grad_norm': 14.729659080505371, 'learning_rate': 5.3584753048073756e-08, 'epoch': 2.91}\n",
      "{'loss': 1.1473, 'grad_norm': 10.517155647277832, 'learning_rate': 5.007490953709227e-08, 'epoch': 2.91}\n",
      "{'loss': 1.0595, 'grad_norm': 10.87724494934082, 'learning_rate': 4.6683652823513725e-08, 'epoch': 2.92}\n",
      "{'loss': 1.0917, 'grad_norm': 10.585076332092285, 'learning_rate': 4.3411023325560245e-08, 'epoch': 2.92}\n",
      "{'loss': 0.829, 'grad_norm': 13.269614219665527, 'learning_rate': 4.025706004760932e-08, 'epoch': 2.92}\n",
      "{'loss': 1.1356, 'grad_norm': 10.62142276763916, 'learning_rate': 3.7221800579735346e-08, 'epoch': 2.92}\n",
      "{'loss': 1.029, 'grad_norm': 11.233745574951172, 'learning_rate': 3.430528109725439e-08, 'epoch': 2.93}\n",
      "{'loss': 1.0298, 'grad_norm': 9.89525032043457, 'learning_rate': 3.150753636029902e-08, 'epoch': 2.93}\n",
      "{'loss': 1.0888, 'grad_norm': 9.956789016723633, 'learning_rate': 2.8828599713398575e-08, 'epoch': 2.93}\n",
      "{'loss': 0.9384, 'grad_norm': 10.863969802856445, 'learning_rate': 2.6268503085089547e-08, 'epoch': 2.94}\n",
      "{'loss': 1.0193, 'grad_norm': 12.156610488891602, 'learning_rate': 2.3827276987524738e-08, 'epoch': 2.94}\n",
      "{'loss': 0.7558, 'grad_norm': 12.280303001403809, 'learning_rate': 2.1504950516118007e-08, 'epoch': 2.94}\n",
      "{'loss': 0.8112, 'grad_norm': 11.65664005279541, 'learning_rate': 1.9301551349195648e-08, 'epoch': 2.95}\n",
      "{'loss': 0.6685, 'grad_norm': 10.738299369812012, 'learning_rate': 1.721710574766333e-08, 'epoch': 2.95}\n",
      "{'loss': 0.6771, 'grad_norm': 8.518482208251953, 'learning_rate': 1.5251638554694137e-08, 'epoch': 2.95}\n",
      "{'loss': 0.826, 'grad_norm': 14.674985885620117, 'learning_rate': 1.340517319543877e-08, 'epoch': 2.96}\n",
      "{'loss': 0.6748, 'grad_norm': 12.905729293823242, 'learning_rate': 1.1677731676733584e-08, 'epoch': 2.96}\n",
      "{'loss': 0.9752, 'grad_norm': 14.204628944396973, 'learning_rate': 1.0069334586854106e-08, 'epoch': 2.96}\n",
      "{'loss': 1.0607, 'grad_norm': 7.756155490875244, 'learning_rate': 8.580001095253032e-09, 'epoch': 2.97}\n",
      "{'loss': 1.3703, 'grad_norm': 12.293281555175781, 'learning_rate': 7.209748952347051e-09, 'epoch': 2.97}\n",
      "{'loss': 1.3159, 'grad_norm': 18.132360458374023, 'learning_rate': 5.958594489295921e-09, 'epoch': 2.97}\n",
      "{'loss': 1.0414, 'grad_norm': 10.71348762512207, 'learning_rate': 4.826552617807067e-09, 'epoch': 2.98}\n",
      "{'loss': 1.1022, 'grad_norm': 9.997384071350098, 'learning_rate': 3.8136368299668266e-09, 'epoch': 2.98}\n",
      "{'loss': 0.8508, 'grad_norm': 10.523101806640625, 'learning_rate': 2.9198591980705847e-09, 'epoch': 2.98}\n",
      "{'loss': 1.0908, 'grad_norm': 9.798343658447266, 'learning_rate': 2.145230374481777e-09, 'epoch': 2.99}\n",
      "{'loss': 1.081, 'grad_norm': 11.102654457092285, 'learning_rate': 1.4897595915053242e-09, 'epoch': 2.99}\n",
      "{'loss': 0.9298, 'grad_norm': 12.072626113891602, 'learning_rate': 9.534546612810502e-10, 'epoch': 2.99}\n",
      "{'loss': 1.1921, 'grad_norm': 13.684348106384277, 'learning_rate': 5.363219756837624e-10, 'epoch': 3.0}\n",
      "{'loss': 0.5534, 'grad_norm': 9.137158393859863, 'learning_rate': 2.3836650624997627e-10, 'epoch': 3.0}\n",
      "{'loss': 0.7049, 'grad_norm': 22.553224563598633, 'learning_rate': 5.959180412129506e-11, 'epoch': 3.0}\n",
      "{'train_runtime': 1136.4203, 'train_samples_per_second': 26.399, 'train_steps_per_second': 0.826, 'train_loss': 1.274349150375817, 'epoch': 3.0}\n",
      "100% 939/939 [18:54<00:00,  1.21s/it]\n",
      "*** Trainer State & Trained Model Saved To --> res/pythia-410m_cirriculum_full/output/ ***\n",
      "*** Trainer State & Trained Model Save-Pretrained To --> res/pythia-410m_cirriculum_full/output//pretrained ***\n",
      "*** Training Done!\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mdesert-grass-11\u001b[0m at: \u001b[34m\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251129_015943-gqyllnen/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train.py --config_file configs/full/pythia-410m_cirriculum_full.yml --wandb_key \"0944191bcf43ea6231189f995e76d66cc523c13d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad62c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'data/CoT/aqua_rat.json': 3460, 'data/CoT/math50k_camel.json': 1899, 'data/CoT/gsm_rft.json': 1060, 'data/PoT/mathqa.json': 934, 'data/PoT/numglue.json': 513, 'data/PoT/gsm_gpt4.json': 511, 'data/CoT/MATH_train.json': 420, 'data/PoT/MATH_train.json': 404, 'data/PoT/aqua_rat_filtered.json': 377, 'data/CoT/gsm_train.json': 289, 'data/CoT/college_math.json': 72, 'data/PoT/TheoremQA.json': 32, 'data/CoT/TheoremQA.json': 18, 'data/CoT/number_comparison.json': 11})\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# from collections import Counter\n",
    "# dataset = load_dataset(\"TIGER-Lab/MathInstruct\",split=\"train[:10000]\")\n",
    "# sources = [example[\"source\"] for example in dataset]\n",
    "# print(Counter(sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f56caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Pythia 410M Trained On Full Dataset On GSM8K\n",
      "Evaluator: res/pythia-410m_cirriculum_full/output initialized!\n",
      "Evaluator: Tokenizer initialized!\n",
      "Evaluator: tokenizer and embedding resize done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Evaluation\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'tqdm.tqdm(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2383141894.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#full\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing Pythia 410M Trained On Full Dataset On GSM8K\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m evaluate_model_accuracy(model_path=\"res/pythia-410m_cirriculum_full/output\",\n\u001b[0m\u001b[1;32m      6\u001b[0m                         \u001b[0mdataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"openai/gsm8k\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0mstart_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/S2L_Cirriculum/evaluate.py\u001b[0m in \u001b[0;36mevaluate_model_accuracy\u001b[0;34m(model_path, dataset_path, start_idx, end_idx)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Begin Evaluation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"EleutherAI/hendrycks_math\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0minstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"problem\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable. Did you mean: 'tqdm.tqdm(...)'?"
     ]
    }
   ],
   "source": [
    "from evaluate import evaluate_model_accuracy\n",
    "\n",
    "#full\n",
    "print(\"Testing Pythia 410M Trained On Full Dataset On GSM8K\")\n",
    "evaluate_model_accuracy(model_path=\"res/pythia-410m_cirriculum_full/output\",\n",
    "                        dataset_path=\"openai/gsm8k\",\n",
    "                        start_idx=0,\n",
    "                        end_idx=100)\n",
    "\n",
    "#s2l\n",
    "print(\"Testing Pythia 410M Trained On S2L Dataset On GSM8K \")\n",
    "evaluate_model_accuracy(model_path=\"res/pythia-410m_cirriculum_s2l/output\",\n",
    "                        dataset_path=\"openai/gsm8k\",\n",
    "                        start_idx=0,\n",
    "                        end_idx=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf27c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_model_accuracy\n",
    "\n",
    "#full\n",
    "print(\"Testing Pythia 410M Trained On Full Dataset On MATH\")\n",
    "evaluate_model_accuracy(model_path=\"res/pythia-410m_cirriculum_full/output\",\n",
    "                        dataset_path=\"EleutherAI/hendrycks_math\",\n",
    "                        start_idx=0,\n",
    "                        end_idx=5000)\n",
    "\n",
    "#s2l\n",
    "print(\"Testing Pythia 410M Trained On S2L Dataset On MATH \")\n",
    "evaluate_model_accuracy(model_path=\"res/pythia-410m_cirriculum_s2l/output\",\n",
    "                        dataset_path=\"EleutherAI/hendrycks_math\",\n",
    "                        start_idx=0,\n",
    "                        end_idx=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48587715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "\u001b[33mcommit 4f8a914e03a504b548f92b842e5b1b646492ccf1\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmain\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/main\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 19:55:49 2025 -0800\n",
      "\n",
      "    feat: clean eval\n",
      "\n",
      "\u001b[33mcommit 9c0028ca0c4691897728051614afdb894cc6bea5\u001b[m\n",
      "Author: giyushino <allanzhang440@gmail.com>\n",
      "Date:   Fri Nov 28 19:53:57 2025 -0800\n",
      "\n",
      "    save losses to another folder as well\n",
      "\n",
      "\u001b[33mcommit 5aa0b78acf352adb109bf328ab17c36f1bf25762\u001b[m\n",
      "Author: giyushino <allanzhang440@gmail.com>\n",
      "Date:   Fri Nov 28 19:52:47 2025 -0800\n",
      "\n",
      "    save losses to another folder as well\n",
      "\n",
      "\u001b[33mcommit ae6926fea2c7fda4f6319fcff33d941b775fd388\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 19:52:50 2025 -0800\n",
      "\n",
      "    feat: tqdm\n",
      "\n",
      "\u001b[33mcommit 37309e348b142f3e61d34436b10dee80c5d473d0\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 19:51:15 2025 -0800\n",
      "\n",
      "    diagnostic: some basic prints\n",
      "\n",
      "\u001b[33mcommit 7a564ed3f7c8579ed0526ec0771c0f2b82f07053\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 19:00:15 2025 -0800\n",
      "\n",
      "    refactor: fixed typo\n",
      "\n",
      "\u001b[33mcommit f6b65aa6032240113efa31f5d75d656a36c8a011\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 18:58:05 2025 -0800\n",
      "\n",
      "    refactor: clean up dead code\n",
      "\n",
      "\u001b[33mcommit 9c0ca8fce8e7dde946d6ffefdf94d5980aa60362\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 18:16:17 2025 -0800\n",
      "\n",
      "    feat: better evaluations\n",
      "\n",
      "\u001b[33mcommit 94dad2c29d5ad1525ab47c6fd1299cf36d269812\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 05:06:28 2025 -0800\n",
      "\n",
      "    feat: remove unneeded logs\n",
      "\n",
      "\u001b[33mcommit d3fb75116e56c59696b1eb6971283ab831c1b8f7\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 05:02:15 2025 -0800\n",
      "\n",
      "    fix: fixed small bug\n",
      "\n",
      "\u001b[33mcommit 82b65da8c5f05b1d98d5038d4a229d0004eba6b5\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 04:43:02 2025 -0800\n",
      "\n",
      "    feat: s2l changes\n",
      "\n",
      "\u001b[33mcommit 6d206867c054ddcb9481f13c0c0cb22424c03d20\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 04:17:52 2025 -0800\n",
      "\n",
      "    feat: fixed subset size\n",
      "\n",
      "\u001b[33mcommit c6ba29c7a2854f4b597f1b71552ab24342427486\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 03:41:59 2025 -0800\n",
      "\n",
      "    feat: evaluation script\n",
      "\n",
      "\u001b[33mcommit fb4408616eb73f9562d29be0f55ff53fba06d305\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 03:33:56 2025 -0800\n",
      "\n",
      "    fix: s2l yml\n",
      "\n",
      "\u001b[33mcommit b62eef8033fc3e517c2ccfe9d2ead4d50f4c78a5\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 03:31:29 2025 -0800\n",
      "\n",
      "    feat: evaluation script\n",
      "\n",
      "\u001b[33mcommit e02f17a515ca3fb54d2fb8462a934658be100402\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 02:51:19 2025 -0800\n",
      "\n",
      "    feat: better config file names\n",
      "\n",
      "\u001b[33mcommit 56ea7ab21694586a17795e5824cae9bb08a32d6d\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Fri Nov 28 02:01:21 2025 -0800\n",
      "\n",
      "    feat: new step size\n",
      "\n",
      "\u001b[33mcommit 6123460b188daf53e4073d796235402de86b4a3c\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Thu Nov 27 23:50:26 2025 -0800\n",
      "\n",
      "    feat: force dataset to be smaller\n",
      "\n",
      "\u001b[33mcommit 90b0bbaf7f4b98bc42439bb2bfc2c48a0ea50981\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Thu Nov 27 23:38:58 2025 -0800\n",
      "\n",
      "    feat: move model to gpu just in case\n",
      "\n",
      "\u001b[33mcommit b327524d0299f9dbe4e09f98b605a6ab0a92924c\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Thu Nov 27 23:35:29 2025 -0800\n",
      "\n",
      "    diagnostic: some logs\n",
      "\n",
      "\u001b[33mcommit b8280f5107b325327d14fcf8f963b8cde2798000\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Thu Nov 27 23:20:23 2025 -0800\n",
      "\n",
      "    feat: new wandb project name\n",
      "\n",
      "\u001b[33mcommit d873533f1ab290337ba5ebc9bd89b8089c548045\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Thu Nov 27 22:42:25 2025 -0800\n",
      "\n",
      "    feat: wandb sanity check\n",
      "\n",
      "\u001b[33mcommit 31900ad7825e6b6923b71e2cbb92f124a5730dd4\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Thu Nov 27 22:40:04 2025 -0800\n",
      "\n",
      "    feat: new configs and wandb\n",
      "\n",
      "\u001b[33mcommit 52c42c72fda50c7a502923d4b12f962c2699925d\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Thu Nov 27 22:11:02 2025 -0800\n",
      "\n",
      "    fix: new cache dir\n",
      "\n",
      "\u001b[33mcommit 9735c29cab4f28abbbb873dd4cf85ea0306ea85d\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Thu Nov 27 22:08:13 2025 -0800\n",
      "\n",
      "    feat: new 70 config\n",
      "\n",
      "\u001b[33mcommit a75112c63936037eb4b9c5f70ea0efac6c490e0b\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Thu Nov 27 21:54:31 2025 -0800\n",
      "\n",
      "    fix: forgot to add small compatibility code for prints\n",
      "\n",
      "\u001b[33mcommit c1922401f10cd4fabec93eb02419a87489aae175\u001b[m\n",
      "Author: LiuHenry9800 <henryliu9800@gmail.com>\n",
      "Date:   Thu Nov 27 21:51:24 2025 -0800\n",
      "\n",
      "    fix: make it compatible with running on colab\n",
      "\n",
      "\u001b[33mcommit 2bc84dda1a054d0ff0e704ae9658ee700d5c25fa\u001b[m\n",
      "Author: YuYang0901 <48270938+YuYang0901@users.noreply.github.com>\n",
      "Date:   Sat Dec 7 19:40:51 2024 -0800\n",
      "\n",
      "    add overview\n",
      "\n",
      "\u001b[33mcommit 99eaf52f9d29808d98a71516ccb1726d87ee4a0e\u001b[m\n",
      "Author: YuYang0901 <48270938+YuYang0901@users.noreply.github.com>\n",
      "Date:   Sat Dec 7 11:25:37 2024 -0800\n",
      "\n",
      "    first commit\n",
      "\n",
      "\u001b[33mcommit 5607fdace74ae1eb1e5fc8827225a57770533e9c\u001b[m\n",
      "Author: Yu Yang <48270938+YuYang0901@users.noreply.github.com>\n",
      "Date:   Thu Oct 31 00:57:54 2024 -0700\n",
      "\n",
      "    Create README.md\n"
     ]
    }
   ],
   "source": [
    "!git pull\n",
    "!git log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb481d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-400\tcheckpoint-700\tconfig.json\t\t tokenizer_config.json\n",
      "checkpoint-450\tcheckpoint-750\tgeneration_config.json\t tokenizer.json\n",
      "checkpoint-500\tcheckpoint-800\tmodel.safetensors\t trainer_state.json\n",
      "checkpoint-550\tcheckpoint-850\tpretrained\t\t training_args.bin\n",
      "checkpoint-600\tcheckpoint-900\truns\n",
      "checkpoint-650\tcheckpoint-939\tspecial_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# !ls\n",
    "!ls res/pythia-70m_cirriculum_full/output\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
