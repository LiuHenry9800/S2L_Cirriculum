{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b459e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'S2L_Cirriculum'...\n",
      "remote: Enumerating objects: 120, done.\u001b[K\n",
      "remote: Counting objects: 100% (120/120), done.\u001b[K\n",
      "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
      "remote: Total 120 (delta 61), reused 89 (delta 33), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (120/120), 155.65 KiB | 14.15 MiB/s, done.\n",
      "Resolving deltas: 100% (61/61), done.\n",
      "S2L_Cirriculum\tsample_data\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/LiuHenry9800/S2L_Cirriculum.git\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e51ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/S2L_Cirriculum\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.46.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.13.0\n"
     ]
    }
   ],
   "source": [
    "%cd S2L_Cirriculum\n",
    "!pip install accelerate wandb faiss-cpu transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7316a4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9698e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"S2L_Cirriculum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a30a7425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-29 01:48:29.688203: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 01:48:29.705163: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764380909.725862    8009 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764380909.732151    8009 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764380909.748113    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764380909.748139    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764380909.748142    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764380909.748146    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 01:48:29.752759: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhenryliu999\u001b[0m (\u001b[33mhenryliu999-other\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-70m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-70m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "ref_model_path: null\n",
      "n_components: -1\n",
      "num_loss_ckpts: -1\n",
      "distance: euclidean\n",
      "seed: 42\n",
      "\n",
      "config.json: 100% 567/567 [00:00<00:00, 4.46MB/s]\n",
      "model.safetensors: 100% 166M/166M [00:02<00:00, 80.4MB/s]\n",
      "*** Model initialized!\n",
      "tokenizer_config.json: 100% 396/396 [00:00<00:00, 2.82MB/s]\n",
      "tokenizer.json: 2.11MB [00:00, 149MB/s]\n",
      "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 950kB/s]\n",
      "*** Tokenizer initialized!\n",
      "*** Smart tokenizer and embedding resize done!\n",
      "README.md: 2.72kB [00:00, 17.0MB/s]\n",
      "MathInstruct.json: 100% 212M/212M [00:04<00:00, 42.8MB/s] \n",
      "Generating train split: 100% 262039/262039 [00:02<00:00, 127352.55 examples/s]\n",
      "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n",
      "*** Schedule built!\n",
      "*** labeled_idx: tensor([   0,    1,    2,  ..., 9997, 9998, 9999])\n",
      "*** jdump(labeled_data_json_format, labeled_data_path) SUCESSFUL to --> res/pythia-70m_cirriculum_full/data/labeled.json\n",
      "*** jdump(unlabeled_data_json_format, unlabeled_data_path) SUCESSFUL to --> res/pythia-70m_cirriculum_full/data/unlabeled.json\n",
      "*** Training-Data-Size = 10000\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "*** SANITY-CHECK: Training-Sample#1. - TEXT.:\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "The distance between two stars is 6.52 √ó 10^5 light years. What is the distance between the two stars in parsecs? (1 parsec = 3.26 light years)\n",
      "Answer Choices: (A) 2 √ó 10^5 (B) 4 √ó 10^6 (C) 5 √ó 10^7 (D) 7 √ó 10^7 (E) 9 √ó 10^8\n",
      "\n",
      "### Response:Let's think about the multi-choice question.\n",
      "6.52 √ó 10^5 ly / (3.26 ly/parsec) = 2 x 10^5 persec\n",
      "The answer is A.</s>\n",
      "\n",
      "\n",
      "/content/S2L_Cirriculum/schedule_base.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=self.model,\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 50278, 'bos_token_id': 50279, 'pad_token_id': 50277}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m setting up run 1gbsfdew (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run 1gbsfdew (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m setting up run 1gbsfdew (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/S2L_Cirriculum/wandb/run-20251129_014918-1gbsfdew\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcelestial-sunset-10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/henryliu999-other/S2L_Cirriculum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/henryliu999-other/S2L_Cirriculum/runs/1gbsfdew\u001b[0m\n",
      "{'loss': 3.7658, 'grad_norm': 422.35272216796875, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'loss': 3.7338, 'grad_norm': 451.20465087890625, 'learning_rate': 6.896551724137931e-07, 'epoch': 0.01}\n",
      "{'loss': 3.5719, 'grad_norm': 419.6539001464844, 'learning_rate': 1.3793103448275862e-06, 'epoch': 0.01}\n",
      "{'loss': 3.4757, 'grad_norm': 413.008544921875, 'learning_rate': 2.0689655172413796e-06, 'epoch': 0.01}\n",
      "{'loss': 3.8353, 'grad_norm': 507.3951721191406, 'learning_rate': 2.7586206896551725e-06, 'epoch': 0.02}\n",
      "{'loss': 4.0288, 'grad_norm': 641.5845336914062, 'learning_rate': 3.448275862068966e-06, 'epoch': 0.02}\n",
      "{'loss': 4.0561, 'grad_norm': 589.9639282226562, 'learning_rate': 4.137931034482759e-06, 'epoch': 0.02}\n",
      "{'loss': 4.1921, 'grad_norm': 689.5428466796875, 'learning_rate': 4.8275862068965525e-06, 'epoch': 0.03}\n",
      "{'loss': 4.1345, 'grad_norm': 783.1348266601562, 'learning_rate': 5.517241379310345e-06, 'epoch': 0.03}\n",
      "{'loss': 4.2642, 'grad_norm': 746.8333740234375, 'learning_rate': 6.206896551724138e-06, 'epoch': 0.03}\n",
      "{'loss': 4.5432, 'grad_norm': 724.808837890625, 'learning_rate': 6.896551724137932e-06, 'epoch': 0.04}\n",
      "{'loss': 4.4541, 'grad_norm': 776.2188110351562, 'learning_rate': 7.586206896551724e-06, 'epoch': 0.04}\n",
      "{'loss': 4.5921, 'grad_norm': 839.0402221679688, 'learning_rate': 8.275862068965518e-06, 'epoch': 0.04}\n",
      "{'loss': 4.4551, 'grad_norm': 876.8865966796875, 'learning_rate': 8.965517241379312e-06, 'epoch': 0.04}\n",
      "{'loss': 4.7138, 'grad_norm': 915.0836181640625, 'learning_rate': 9.655172413793105e-06, 'epoch': 0.05}\n",
      "{'loss': 4.3166, 'grad_norm': 867.3264770507812, 'learning_rate': 1.0344827586206898e-05, 'epoch': 0.05}\n",
      "{'loss': 4.3668, 'grad_norm': 994.2124633789062, 'learning_rate': 1.103448275862069e-05, 'epoch': 0.05}\n",
      "{'loss': 4.3416, 'grad_norm': 1057.8489990234375, 'learning_rate': 1.1724137931034483e-05, 'epoch': 0.06}\n",
      "{'loss': 4.8721, 'grad_norm': 967.5595703125, 'learning_rate': 1.2413793103448277e-05, 'epoch': 0.06}\n",
      "{'loss': 4.4301, 'grad_norm': 1057.0362548828125, 'learning_rate': 1.310344827586207e-05, 'epoch': 0.06}\n",
      "{'loss': 4.735, 'grad_norm': 998.0591430664062, 'learning_rate': 1.3793103448275863e-05, 'epoch': 0.07}\n",
      "{'loss': 4.35, 'grad_norm': 957.4970092773438, 'learning_rate': 1.4482758620689657e-05, 'epoch': 0.07}\n",
      "{'loss': 4.275, 'grad_norm': 1146.5919189453125, 'learning_rate': 1.5172413793103448e-05, 'epoch': 0.07}\n",
      "{'loss': 4.7599, 'grad_norm': 1015.6348266601562, 'learning_rate': 1.586206896551724e-05, 'epoch': 0.08}\n",
      "{'loss': 4.4953, 'grad_norm': 1293.0433349609375, 'learning_rate': 1.6551724137931037e-05, 'epoch': 0.08}\n",
      "{'loss': 4.4784, 'grad_norm': 959.790283203125, 'learning_rate': 1.7241379310344828e-05, 'epoch': 0.08}\n",
      "{'loss': 4.4796, 'grad_norm': 1364.8568115234375, 'learning_rate': 1.7931034482758623e-05, 'epoch': 0.09}\n",
      "{'loss': 4.4238, 'grad_norm': 1302.243408203125, 'learning_rate': 1.8620689655172415e-05, 'epoch': 0.09}\n",
      "{'loss': 4.539, 'grad_norm': 1117.89306640625, 'learning_rate': 1.931034482758621e-05, 'epoch': 0.09}\n",
      "{'loss': 4.4704, 'grad_norm': 1154.024169921875, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 4.5203, 'grad_norm': 1331.9298095703125, 'learning_rate': 1.9999940408195878e-05, 'epoch': 0.1}\n",
      "{'loss': 4.5085, 'grad_norm': 1112.92919921875, 'learning_rate': 1.9999761633493754e-05, 'epoch': 0.1}\n",
      "{'loss': 4.436, 'grad_norm': 1098.4649658203125, 'learning_rate': 1.9999463678024317e-05, 'epoch': 0.11}\n",
      "{'loss': 4.4976, 'grad_norm': 1290.3721923828125, 'learning_rate': 1.999904654533872e-05, 'epoch': 0.11}\n",
      "{'loss': 4.5959, 'grad_norm': 1061.7178955078125, 'learning_rate': 1.9998510240408495e-05, 'epoch': 0.11}\n",
      "{'loss': 4.3478, 'grad_norm': 1028.8590087890625, 'learning_rate': 1.999785476962552e-05, 'epoch': 0.12}\n",
      "{'loss': 4.3721, 'grad_norm': 1032.20947265625, 'learning_rate': 1.9997080140801932e-05, 'epoch': 0.12}\n",
      "{'loss': 4.3482, 'grad_norm': 1106.6773681640625, 'learning_rate': 1.9996186363170037e-05, 'epoch': 0.12}\n",
      "{'loss': 4.1389, 'grad_norm': 898.416015625, 'learning_rate': 1.9995173447382193e-05, 'epoch': 0.12}\n",
      "{'loss': 4.4642, 'grad_norm': 1161.6463623046875, 'learning_rate': 1.9994041405510705e-05, 'epoch': 0.13}\n",
      "{'loss': 4.3778, 'grad_norm': 968.447021484375, 'learning_rate': 1.9992790251047655e-05, 'epoch': 0.13}\n",
      "{'loss': 4.3078, 'grad_norm': 949.67919921875, 'learning_rate': 1.999141999890475e-05, 'epoch': 0.13}\n",
      "{'loss': 4.6665, 'grad_norm': 867.0065307617188, 'learning_rate': 1.9989930665413148e-05, 'epoch': 0.14}\n",
      "{'loss': 4.245, 'grad_norm': 995.5661010742188, 'learning_rate': 1.998832226832327e-05, 'epoch': 0.14}\n",
      "{'loss': 3.825, 'grad_norm': 863.858154296875, 'learning_rate': 1.9986594826804563e-05, 'epoch': 0.14}\n",
      "{'loss': 4.0193, 'grad_norm': 982.7911376953125, 'learning_rate': 1.9984748361445306e-05, 'epoch': 0.15}\n",
      "{'loss': 4.1683, 'grad_norm': 1233.708984375, 'learning_rate': 1.998278289425234e-05, 'epoch': 0.15}\n",
      "{'loss': 4.0453, 'grad_norm': 1162.9830322265625, 'learning_rate': 1.9980698448650805e-05, 'epoch': 0.15}\n",
      "{'loss': 4.1416, 'grad_norm': 1015.9295043945312, 'learning_rate': 1.9978495049483883e-05, 'epoch': 0.16}\n",
      "{'loss': 4.5732, 'grad_norm': 1149.9755859375, 'learning_rate': 1.997617272301248e-05, 'epoch': 0.16}\n",
      "{'loss': 3.5761, 'grad_norm': 485.80743408203125, 'learning_rate': 1.9973731496914914e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4697, 'grad_norm': 426.6372985839844, 'learning_rate': 1.9971171400286602e-05, 'epoch': 0.17}\n",
      "{'loss': 3.584, 'grad_norm': 519.0010375976562, 'learning_rate': 1.9968492463639704e-05, 'epoch': 0.17}\n",
      "{'loss': 3.6141, 'grad_norm': 439.4888916015625, 'learning_rate': 1.9965694718902745e-05, 'epoch': 0.17}\n",
      "{'loss': 3.612, 'grad_norm': 638.9364624023438, 'learning_rate': 1.9962778199420265e-05, 'epoch': 0.18}\n",
      "{'loss': 3.6236, 'grad_norm': 708.377197265625, 'learning_rate': 1.9959742939952393e-05, 'epoch': 0.18}\n",
      "{'loss': 3.5421, 'grad_norm': 555.514892578125, 'learning_rate': 1.9956588976674442e-05, 'epoch': 0.18}\n",
      "{'loss': 3.8792, 'grad_norm': 712.4412841796875, 'learning_rate': 1.995331634717649e-05, 'epoch': 0.19}\n",
      "{'loss': 4.0043, 'grad_norm': 686.3456420898438, 'learning_rate': 1.994992509046291e-05, 'epoch': 0.19}\n",
      "{'loss': 3.8543, 'grad_norm': 1017.60595703125, 'learning_rate': 1.9946415246951928e-05, 'epoch': 0.19}\n",
      "{'loss': 4.3037, 'grad_norm': 982.1740112304688, 'learning_rate': 1.9942786858475126e-05, 'epoch': 0.2}\n",
      "{'loss': 4.1594, 'grad_norm': 815.5997924804688, 'learning_rate': 1.9939039968276942e-05, 'epoch': 0.2}\n",
      "{'loss': 4.1602, 'grad_norm': 769.6072387695312, 'learning_rate': 1.9935174621014173e-05, 'epoch': 0.2}\n",
      "{'loss': 3.9514, 'grad_norm': 888.363525390625, 'learning_rate': 1.9931190862755416e-05, 'epoch': 0.2}\n",
      "{'loss': 3.9517, 'grad_norm': 787.8326416015625, 'learning_rate': 1.992708874098054e-05, 'epoch': 0.21}\n",
      "{'loss': 4.0151, 'grad_norm': 700.5433349609375, 'learning_rate': 1.992286830458012e-05, 'epoch': 0.21}\n",
      "{'loss': 4.1257, 'grad_norm': 735.136962890625, 'learning_rate': 1.9918529603854825e-05, 'epoch': 0.21}\n",
      "{'loss': 3.9699, 'grad_norm': 715.1495971679688, 'learning_rate': 1.991407269051487e-05, 'epoch': 0.22}\n",
      "{'loss': 4.2512, 'grad_norm': 893.5902709960938, 'learning_rate': 1.990949761767935e-05, 'epoch': 0.22}\n",
      "{'loss': 4.1972, 'grad_norm': 738.2159423828125, 'learning_rate': 1.9904804439875635e-05, 'epoch': 0.22}\n",
      "{'loss': 4.2964, 'grad_norm': 770.48046875, 'learning_rate': 1.989999321303871e-05, 'epoch': 0.23}\n",
      "{'loss': 4.1421, 'grad_norm': 791.3756103515625, 'learning_rate': 1.9895063994510512e-05, 'epoch': 0.23}\n",
      "{'loss': 4.1972, 'grad_norm': 821.22119140625, 'learning_rate': 1.989001684303925e-05, 'epoch': 0.23}\n",
      "{'loss': 3.7938, 'grad_norm': 653.3248901367188, 'learning_rate': 1.9884851818778695e-05, 'epoch': 0.24}\n",
      "{'loss': 4.0045, 'grad_norm': 713.2860107421875, 'learning_rate': 1.9879568983287468e-05, 'epoch': 0.24}\n",
      "{'loss': 4.3865, 'grad_norm': 630.612548828125, 'learning_rate': 1.9874168399528307e-05, 'epoch': 0.24}\n",
      "{'loss': 3.6999, 'grad_norm': 711.52392578125, 'learning_rate': 1.986865013186732e-05, 'epoch': 0.25}\n",
      "{'loss': 4.0894, 'grad_norm': 669.0634765625, 'learning_rate': 1.9863014246073216e-05, 'epoch': 0.25}\n",
      "{'loss': 4.1404, 'grad_norm': 664.8681030273438, 'learning_rate': 1.985726080931651e-05, 'epoch': 0.25}\n",
      "{'loss': 4.0165, 'grad_norm': 818.0161743164062, 'learning_rate': 1.9851389890168738e-05, 'epoch': 0.26}\n",
      "{'loss': 4.2078, 'grad_norm': 745.3424682617188, 'learning_rate': 1.9845401558601634e-05, 'epoch': 0.26}\n",
      "{'loss': 4.0232, 'grad_norm': 729.968017578125, 'learning_rate': 1.98392958859863e-05, 'epoch': 0.26}\n",
      "{'loss': 4.2939, 'grad_norm': 771.5748901367188, 'learning_rate': 1.9833072945092334e-05, 'epoch': 0.27}\n",
      "{'loss': 4.1605, 'grad_norm': 897.3772583007812, 'learning_rate': 1.9826732810087e-05, 'epoch': 0.27}\n",
      "{'loss': 3.9693, 'grad_norm': 695.7412719726562, 'learning_rate': 1.9820275556534306e-05, 'epoch': 0.27}\n",
      "{'loss': 3.8213, 'grad_norm': 793.1810302734375, 'learning_rate': 1.9813701261394136e-05, 'epoch': 0.28}\n",
      "{'loss': 4.0736, 'grad_norm': 727.0285034179688, 'learning_rate': 1.980701000302131e-05, 'epoch': 0.28}\n",
      "{'loss': 3.6419, 'grad_norm': 731.1748046875, 'learning_rate': 1.9800201861164665e-05, 'epoch': 0.28}\n",
      "{'loss': 4.174, 'grad_norm': 878.7958984375, 'learning_rate': 1.979327691696608e-05, 'epoch': 0.28}\n",
      "{'loss': 4.2802, 'grad_norm': 679.501953125, 'learning_rate': 1.9786235252959555e-05, 'epoch': 0.29}\n",
      "{'loss': 3.97, 'grad_norm': 820.3197021484375, 'learning_rate': 1.977907695307017e-05, 'epoch': 0.29}\n",
      "{'loss': 3.6461, 'grad_norm': 755.010009765625, 'learning_rate': 1.9771802102613127e-05, 'epoch': 0.29}\n",
      "{'loss': 3.6474, 'grad_norm': 922.7470703125, 'learning_rate': 1.9764410788292724e-05, 'epoch': 0.3}\n",
      "{'loss': 4.1151, 'grad_norm': 1175.52099609375, 'learning_rate': 1.975690309820131e-05, 'epoch': 0.3}\n",
      "{'loss': 3.7153, 'grad_norm': 824.7984619140625, 'learning_rate': 1.9749279121818235e-05, 'epoch': 0.3}\n",
      "{'loss': 3.8259, 'grad_norm': 797.1742553710938, 'learning_rate': 1.9741538950008817e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3699, 'grad_norm': 848.6318969726562, 'learning_rate': 1.9733682675023207e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3918, 'grad_norm': 908.2991943359375, 'learning_rate': 1.972571039049533e-05, 'epoch': 0.31}\n",
      "{'loss': 3.56, 'grad_norm': 690.3239135742188, 'learning_rate': 1.971762219144174e-05, 'epoch': 0.32}\n",
      "{'loss': 3.581, 'grad_norm': 1318.628662109375, 'learning_rate': 1.9709418174260523e-05, 'epoch': 0.32}\n",
      "{'loss': 3.5691, 'grad_norm': 360.3301696777344, 'learning_rate': 1.9701098436730108e-05, 'epoch': 0.32}\n",
      "{'loss': 3.5794, 'grad_norm': 384.8272705078125, 'learning_rate': 1.969266307800813e-05, 'epoch': 0.33}\n",
      "{'loss': 3.7001, 'grad_norm': 414.98516845703125, 'learning_rate': 1.9684112198630246e-05, 'epoch': 0.33}\n",
      "{'loss': 3.4672, 'grad_norm': 480.7335510253906, 'learning_rate': 1.967544590050891e-05, 'epoch': 0.33}\n",
      "{'loss': 3.4846, 'grad_norm': 425.4966125488281, 'learning_rate': 1.9666664286932198e-05, 'epoch': 0.34}\n",
      "{'loss': 3.6409, 'grad_norm': 561.1715087890625, 'learning_rate': 1.9657767462562544e-05, 'epoch': 0.34}\n",
      "{'loss': 3.7004, 'grad_norm': 611.4946899414062, 'learning_rate': 1.9648755533435517e-05, 'epoch': 0.34}\n",
      "{'loss': 3.7913, 'grad_norm': 611.8681030273438, 'learning_rate': 1.9639628606958535e-05, 'epoch': 0.35}\n",
      "{'loss': 3.864, 'grad_norm': 613.6282348632812, 'learning_rate': 1.96303867919096e-05, 'epoch': 0.35}\n",
      "{'loss': 3.8333, 'grad_norm': 716.2864379882812, 'learning_rate': 1.9621030198436007e-05, 'epoch': 0.35}\n",
      "{'loss': 3.8313, 'grad_norm': 635.6500244140625, 'learning_rate': 1.9611558938053003e-05, 'epoch': 0.36}\n",
      "{'loss': 3.9256, 'grad_norm': 734.4190063476562, 'learning_rate': 1.9601973123642493e-05, 'epoch': 0.36}\n",
      "{'loss': 3.7517, 'grad_norm': 607.7977905273438, 'learning_rate': 1.9592272869451672e-05, 'epoch': 0.36}\n",
      "{'loss': 3.7954, 'grad_norm': 570.5303344726562, 'learning_rate': 1.9582458291091664e-05, 'epoch': 0.36}\n",
      "{'loss': 3.9452, 'grad_norm': 653.4329833984375, 'learning_rate': 1.957252950553616e-05, 'epoch': 0.37}\n",
      "{'loss': 4.2676, 'grad_norm': 635.9562377929688, 'learning_rate': 1.9562486631120007e-05, 'epoch': 0.37}\n",
      "{'loss': 4.0707, 'grad_norm': 560.937255859375, 'learning_rate': 1.9552329787537805e-05, 'epoch': 0.37}\n",
      "{'loss': 4.2767, 'grad_norm': 579.6201782226562, 'learning_rate': 1.9542059095842484e-05, 'epoch': 0.38}\n",
      "{'loss': 3.6956, 'grad_norm': 565.275146484375, 'learning_rate': 1.9531674678443853e-05, 'epoch': 0.38}\n",
      "{'loss': 3.785, 'grad_norm': 558.6279907226562, 'learning_rate': 1.952117665910714e-05, 'epoch': 0.38}\n",
      "{'loss': 4.2072, 'grad_norm': 485.4454650878906, 'learning_rate': 1.9510565162951538e-05, 'epoch': 0.39}\n",
      "{'loss': 3.9771, 'grad_norm': 515.3506469726562, 'learning_rate': 1.9499840316448675e-05, 'epoch': 0.39}\n",
      "{'loss': 3.9049, 'grad_norm': 630.528076171875, 'learning_rate': 1.948900224742115e-05, 'epoch': 0.39}\n",
      "{'loss': 3.8167, 'grad_norm': 471.240966796875, 'learning_rate': 1.9478051085040978e-05, 'epoch': 0.4}\n",
      "{'loss': 3.7696, 'grad_norm': 445.46051025390625, 'learning_rate': 1.9466986959828063e-05, 'epoch': 0.4}\n",
      "{'loss': 3.9851, 'grad_norm': 530.3735961914062, 'learning_rate': 1.945581000364864e-05, 'epoch': 0.4}\n",
      "{'loss': 3.4821, 'grad_norm': 513.404541015625, 'learning_rate': 1.9444520349713705e-05, 'epoch': 0.41}\n",
      "{'loss': 3.6048, 'grad_norm': 485.7118225097656, 'learning_rate': 1.9433118132577432e-05, 'epoch': 0.41}\n",
      "{'loss': 3.7038, 'grad_norm': 533.864013671875, 'learning_rate': 1.942160348813556e-05, 'epoch': 0.41}\n",
      "{'loss': 3.9442, 'grad_norm': 591.3815307617188, 'learning_rate': 1.9409976553623767e-05, 'epoch': 0.42}\n",
      "{'loss': 3.627, 'grad_norm': 425.6654052734375, 'learning_rate': 1.9398237467616063e-05, 'epoch': 0.42}\n",
      "{'loss': 3.5278, 'grad_norm': 391.8355407714844, 'learning_rate': 1.9386386370023104e-05, 'epoch': 0.42}\n",
      "{'loss': 3.8114, 'grad_norm': 508.605712890625, 'learning_rate': 1.9374423402090553e-05, 'epoch': 0.43}\n",
      "{'loss': 4.265, 'grad_norm': 393.5132751464844, 'learning_rate': 1.9362348706397374e-05, 'epoch': 0.43}\n",
      "{'loss': 3.8713, 'grad_norm': 348.7862243652344, 'learning_rate': 1.9350162426854152e-05, 'epoch': 0.43}\n",
      "{'loss': 3.8632, 'grad_norm': 385.10595703125, 'learning_rate': 1.933786470870136e-05, 'epoch': 0.44}\n",
      "{'loss': 4.1432, 'grad_norm': 382.6690979003906, 'learning_rate': 1.9325455698507638e-05, 'epoch': 0.44}\n",
      "{'loss': 4.3206, 'grad_norm': 424.7291259765625, 'learning_rate': 1.931293554416805e-05, 'epoch': 0.44}\n",
      "{'loss': 3.628, 'grad_norm': 342.1102600097656, 'learning_rate': 1.9300304394902315e-05, 'epoch': 0.44}\n",
      "{'loss': 3.8524, 'grad_norm': 369.2185363769531, 'learning_rate': 1.9287562401253023e-05, 'epoch': 0.45}\n",
      "{'loss': 3.7849, 'grad_norm': 389.68646240234375, 'learning_rate': 1.927470971508386e-05, 'epoch': 0.45}\n",
      "{'loss': 4.0378, 'grad_norm': 391.1567687988281, 'learning_rate': 1.9261746489577767e-05, 'epoch': 0.45}\n",
      "{'loss': 3.8945, 'grad_norm': 402.6817626953125, 'learning_rate': 1.924867287923515e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2948, 'grad_norm': 403.0817565917969, 'learning_rate': 1.923548903987201e-05, 'epoch': 0.46}\n",
      "{'loss': 3.5839, 'grad_norm': 413.55364990234375, 'learning_rate': 1.9222195128618108e-05, 'epoch': 0.46}\n",
      "{'loss': 3.7777, 'grad_norm': 429.12530517578125, 'learning_rate': 1.9208791303915063e-05, 'epoch': 0.47}\n",
      "{'loss': 3.5295, 'grad_norm': 403.9469299316406, 'learning_rate': 1.919527772551451e-05, 'epoch': 0.47}\n",
      "{'loss': 3.5079, 'grad_norm': 389.1956787109375, 'learning_rate': 1.918165455447614e-05, 'epoch': 0.47}\n",
      "{'loss': 3.6291, 'grad_norm': 515.6707763671875, 'learning_rate': 1.9167921953165827e-05, 'epoch': 0.48}\n",
      "{'loss': 3.8446, 'grad_norm': 1205.201904296875, 'learning_rate': 1.9154080085253665e-05, 'epoch': 0.48}\n",
      "{'loss': 3.7407, 'grad_norm': 254.86614990234375, 'learning_rate': 1.9140129115712035e-05, 'epoch': 0.48}\n",
      "{'loss': 3.5472, 'grad_norm': 292.95013427734375, 'learning_rate': 1.912606921081362e-05, 'epoch': 0.49}\n",
      "{'loss': 3.6844, 'grad_norm': 236.79605102539062, 'learning_rate': 1.9111900538129443e-05, 'epoch': 0.49}\n",
      "{'loss': 3.6248, 'grad_norm': 237.50804138183594, 'learning_rate': 1.909762326652686e-05, 'epoch': 0.49}\n",
      "{'loss': 3.7138, 'grad_norm': 268.8944091796875, 'learning_rate': 1.908323756616754e-05, 'epoch': 0.5}\n",
      "{'loss': 3.704, 'grad_norm': 336.31329345703125, 'learning_rate': 1.9068743608505454e-05, 'epoch': 0.5}\n",
      "{'loss': 3.7503, 'grad_norm': 309.8310852050781, 'learning_rate': 1.9054141566284822e-05, 'epoch': 0.5}\n",
      "{'loss': 3.9026, 'grad_norm': 333.2486267089844, 'learning_rate': 1.9039431613538047e-05, 'epoch': 0.51}\n",
      "{'loss': 4.176, 'grad_norm': 381.112548828125, 'learning_rate': 1.9024613925583652e-05, 'epoch': 0.51}\n",
      "{'loss': 3.8957, 'grad_norm': 327.3789978027344, 'learning_rate': 1.900968867902419e-05, 'epoch': 0.51}\n",
      "{'loss': 3.9879, 'grad_norm': 311.39581298828125, 'learning_rate': 1.899465605174414e-05, 'epoch': 0.52}\n",
      "{'loss': 4.002, 'grad_norm': 419.7293701171875, 'learning_rate': 1.8979516222907776e-05, 'epoch': 0.52}\n",
      "{'loss': 3.7966, 'grad_norm': 328.15252685546875, 'learning_rate': 1.896426937295704e-05, 'epoch': 0.52}\n",
      "{'loss': 4.1663, 'grad_norm': 335.1750793457031, 'learning_rate': 1.8948915683609387e-05, 'epoch': 0.52}\n",
      "{'loss': 3.6649, 'grad_norm': 286.1286315917969, 'learning_rate': 1.8933455337855633e-05, 'epoch': 0.53}\n",
      "{'loss': 4.158, 'grad_norm': 349.30712890625, 'learning_rate': 1.8917888519957756e-05, 'epoch': 0.53}\n",
      "{'loss': 3.6339, 'grad_norm': 283.63909912109375, 'learning_rate': 1.89022154154467e-05, 'epoch': 0.53}\n",
      "{'loss': 3.7626, 'grad_norm': 350.4730529785156, 'learning_rate': 1.8886436211120195e-05, 'epoch': 0.54}\n",
      "{'loss': 4.077, 'grad_norm': 387.3306579589844, 'learning_rate': 1.8870551095040476e-05, 'epoch': 0.54}\n",
      "{'loss': 3.7142, 'grad_norm': 395.45623779296875, 'learning_rate': 1.8854560256532098e-05, 'epoch': 0.54}\n",
      "{'loss': 3.7203, 'grad_norm': 345.4356384277344, 'learning_rate': 1.8838463886179647e-05, 'epoch': 0.55}\n",
      "{'loss': 3.9156, 'grad_norm': 346.55615234375, 'learning_rate': 1.8822262175825463e-05, 'epoch': 0.55}\n",
      "{'loss': 3.9523, 'grad_norm': 357.5606994628906, 'learning_rate': 1.880595531856738e-05, 'epoch': 0.55}\n",
      "{'loss': 4.0147, 'grad_norm': 428.5819396972656, 'learning_rate': 1.878954350875641e-05, 'epoch': 0.56}\n",
      "{'loss': 4.0394, 'grad_norm': 345.8794250488281, 'learning_rate': 1.877302694199442e-05, 'epoch': 0.56}\n",
      "{'loss': 3.8355, 'grad_norm': 407.66241455078125, 'learning_rate': 1.8756405815131815e-05, 'epoch': 0.56}\n",
      "{'loss': 3.7202, 'grad_norm': 395.7967224121094, 'learning_rate': 1.873968032626518e-05, 'epoch': 0.57}\n",
      "{'loss': 3.9053, 'grad_norm': 341.1965637207031, 'learning_rate': 1.872285067473493e-05, 'epoch': 0.57}\n",
      "{'loss': 3.8407, 'grad_norm': 386.27630615234375, 'learning_rate': 1.8705917061122917e-05, 'epoch': 0.57}\n",
      "{'loss': 3.823, 'grad_norm': 411.7342224121094, 'learning_rate': 1.8688879687250067e-05, 'epoch': 0.58}\n",
      "{'loss': 3.88, 'grad_norm': 377.2381896972656, 'learning_rate': 1.8671738756173946e-05, 'epoch': 0.58}\n",
      "{'loss': 3.8312, 'grad_norm': 454.8456726074219, 'learning_rate': 1.8654494472186352e-05, 'epoch': 0.58}\n",
      "{'loss': 3.9315, 'grad_norm': 371.6017761230469, 'learning_rate': 1.8637147040810884e-05, 'epoch': 0.59}\n",
      "{'loss': 3.6284, 'grad_norm': 483.7190246582031, 'learning_rate': 1.8619696668800494e-05, 'epoch': 0.59}\n",
      "{'loss': 3.8684, 'grad_norm': 427.1687927246094, 'learning_rate': 1.860214356413501e-05, 'epoch': 0.59}\n",
      "{'loss': 3.4849, 'grad_norm': 435.3529968261719, 'learning_rate': 1.8584487936018663e-05, 'epoch': 0.6}\n",
      "{'loss': 3.628, 'grad_norm': 353.3283996582031, 'learning_rate': 1.8566729994877604e-05, 'epoch': 0.6}\n",
      "{'loss': 3.6086, 'grad_norm': 478.9867858886719, 'learning_rate': 1.854886995235738e-05, 'epoch': 0.6}\n",
      "{'loss': 3.7628, 'grad_norm': 403.65423583984375, 'learning_rate': 1.8530908021320427e-05, 'epoch': 0.6}\n",
      "{'loss': 3.4015, 'grad_norm': 376.2209777832031, 'learning_rate': 1.8512844415843514e-05, 'epoch': 0.61}\n",
      "{'loss': 3.6969, 'grad_norm': 423.84747314453125, 'learning_rate': 1.8494679351215212e-05, 'epoch': 0.61}\n",
      "{'loss': 3.6371, 'grad_norm': 328.0155944824219, 'learning_rate': 1.8476413043933316e-05, 'epoch': 0.61}\n",
      "{'loss': 3.9675, 'grad_norm': 433.15521240234375, 'learning_rate': 1.8458045711702264e-05, 'epoch': 0.62}\n",
      "{'loss': 3.5686, 'grad_norm': 393.9710998535156, 'learning_rate': 1.8439577573430557e-05, 'epoch': 0.62}\n",
      "{'loss': 3.1505, 'grad_norm': 378.61688232421875, 'learning_rate': 1.842100884922812e-05, 'epoch': 0.62}\n",
      "{'loss': 3.6061, 'grad_norm': 368.7760925292969, 'learning_rate': 1.8402339760403715e-05, 'epoch': 0.63}\n",
      "{'loss': 3.1004, 'grad_norm': 335.7938232421875, 'learning_rate': 1.8383570529462273e-05, 'epoch': 0.63}\n",
      "{'loss': 3.4779, 'grad_norm': 608.5419921875, 'learning_rate': 1.8364701380102267e-05, 'epoch': 0.63}\n",
      "{'loss': 3.6583, 'grad_norm': 453.32080078125, 'learning_rate': 1.834573253721303e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3538, 'grad_norm': 473.1178894042969, 'learning_rate': 1.8326664226872063e-05, 'epoch': 0.64}\n",
      "{'loss': 3.5992, 'grad_norm': 227.5435028076172, 'learning_rate': 1.8307496676342384e-05, 'epoch': 0.64}\n",
      "{'loss': 3.5169, 'grad_norm': 239.9166717529297, 'learning_rate': 1.828823011406977e-05, 'epoch': 0.65}\n",
      "{'loss': 3.6663, 'grad_norm': 254.79383850097656, 'learning_rate': 1.8268864769680054e-05, 'epoch': 0.65}\n",
      "{'loss': 3.4983, 'grad_norm': 203.3697967529297, 'learning_rate': 1.824940087397641e-05, 'epoch': 0.65}\n",
      "{'loss': 3.9071, 'grad_norm': 255.58706665039062, 'learning_rate': 1.8229838658936566e-05, 'epoch': 0.66}\n",
      "{'loss': 3.8385, 'grad_norm': 357.4445495605469, 'learning_rate': 1.8210178357710057e-05, 'epoch': 0.66}\n",
      "{'loss': 3.747, 'grad_norm': 261.80535888671875, 'learning_rate': 1.819042020461545e-05, 'epoch': 0.66}\n",
      "{'loss': 4.0473, 'grad_norm': 281.1827697753906, 'learning_rate': 1.8170564435137542e-05, 'epoch': 0.67}\n",
      "{'loss': 3.963, 'grad_norm': 276.4062805175781, 'learning_rate': 1.8150611285924556e-05, 'epoch': 0.67}\n",
      "{'loss': 4.1137, 'grad_norm': 257.4859619140625, 'learning_rate': 1.8130560994785325e-05, 'epoch': 0.67}\n",
      "{'loss': 4.0075, 'grad_norm': 227.27186584472656, 'learning_rate': 1.8110413800686456e-05, 'epoch': 0.68}\n",
      "{'loss': 4.0944, 'grad_norm': 209.96896362304688, 'learning_rate': 1.8090169943749477e-05, 'epoch': 0.68}\n",
      "{'loss': 4.0706, 'grad_norm': 236.29586791992188, 'learning_rate': 1.8069829665247975e-05, 'epoch': 0.68}\n",
      "{'loss': 3.7988, 'grad_norm': 253.44863891601562, 'learning_rate': 1.8049393207604734e-05, 'epoch': 0.68}\n",
      "{'loss': 4.1506, 'grad_norm': 172.69796752929688, 'learning_rate': 1.8028860814388826e-05, 'epoch': 0.69}\n",
      "{'loss': 3.8004, 'grad_norm': 201.54898071289062, 'learning_rate': 1.8008232730312724e-05, 'epoch': 0.69}\n",
      "{'loss': 4.0954, 'grad_norm': 239.25157165527344, 'learning_rate': 1.7987509201229378e-05, 'epoch': 0.69}\n",
      "{'loss': 3.9949, 'grad_norm': 256.1719665527344, 'learning_rate': 1.7966690474129285e-05, 'epoch': 0.7}\n",
      "{'loss': 4.1024, 'grad_norm': 195.30233764648438, 'learning_rate': 1.7945776797137544e-05, 'epoch': 0.7}\n",
      "{'loss': 3.6817, 'grad_norm': 225.9134521484375, 'learning_rate': 1.7924768419510906e-05, 'epoch': 0.7}\n",
      "{'loss': 4.1511, 'grad_norm': 193.69857788085938, 'learning_rate': 1.7903665591634794e-05, 'epoch': 0.71}\n",
      "{'loss': 3.6439, 'grad_norm': 233.27627563476562, 'learning_rate': 1.7882468565020327e-05, 'epoch': 0.71}\n",
      "{'loss': 3.8499, 'grad_norm': 233.2991485595703, 'learning_rate': 1.786117759230132e-05, 'epoch': 0.71}\n",
      "{'loss': 3.6448, 'grad_norm': 287.9501037597656, 'learning_rate': 1.7839792927231253e-05, 'epoch': 0.72}\n",
      "{'loss': 4.0365, 'grad_norm': 234.7271728515625, 'learning_rate': 1.78183148246803e-05, 'epoch': 0.72}\n",
      "{'loss': 3.8103, 'grad_norm': 228.12147521972656, 'learning_rate': 1.7796743540632226e-05, 'epoch': 0.72}\n",
      "{'loss': 4.0825, 'grad_norm': 258.7074890136719, 'learning_rate': 1.777507933218138e-05, 'epoch': 0.73}\n",
      "{'loss': 4.0034, 'grad_norm': 243.20013427734375, 'learning_rate': 1.7753322457529615e-05, 'epoch': 0.73}\n",
      "{'loss': 3.8797, 'grad_norm': 267.5581359863281, 'learning_rate': 1.7731473175983215e-05, 'epoch': 0.73}\n",
      "{'loss': 3.8175, 'grad_norm': 257.2535095214844, 'learning_rate': 1.7709531747949796e-05, 'epoch': 0.74}\n",
      "{'loss': 3.8978, 'grad_norm': 236.46435546875, 'learning_rate': 1.7687498434935224e-05, 'epoch': 0.74}\n",
      "{'loss': 3.7081, 'grad_norm': 282.7410583496094, 'learning_rate': 1.7665373499540464e-05, 'epoch': 0.74}\n",
      "{'loss': 3.8133, 'grad_norm': 365.6062316894531, 'learning_rate': 1.7643157205458483e-05, 'epoch': 0.75}\n",
      "{'loss': 3.8235, 'grad_norm': 227.24229431152344, 'learning_rate': 1.7620849817471094e-05, 'epoch': 0.75}\n",
      "{'loss': 3.373, 'grad_norm': 296.7728271484375, 'learning_rate': 1.759845160144579e-05, 'epoch': 0.75}\n",
      "{'loss': 3.5041, 'grad_norm': 276.9547424316406, 'learning_rate': 1.7575962824332595e-05, 'epoch': 0.76}\n",
      "{'loss': 3.8357, 'grad_norm': 265.7020263671875, 'learning_rate': 1.7553383754160864e-05, 'epoch': 0.76}\n",
      "{'loss': 3.6945, 'grad_norm': 242.42933654785156, 'learning_rate': 1.7530714660036112e-05, 'epoch': 0.76}\n",
      "{'loss': 4.0331, 'grad_norm': 308.3870849609375, 'learning_rate': 1.7507955812136775e-05, 'epoch': 0.76}\n",
      "{'loss': 3.8315, 'grad_norm': 273.7275390625, 'learning_rate': 1.7485107481711014e-05, 'epoch': 0.77}\n",
      "{'loss': 3.5928, 'grad_norm': 269.6811828613281, 'learning_rate': 1.7462169941073478e-05, 'epoch': 0.77}\n",
      "{'loss': 3.7827, 'grad_norm': 288.7001037597656, 'learning_rate': 1.7439143463602052e-05, 'epoch': 0.77}\n",
      "{'loss': 4.0602, 'grad_norm': 290.8311462402344, 'learning_rate': 1.74160283237346e-05, 'epoch': 0.78}\n",
      "{'loss': 3.6739, 'grad_norm': 366.4314270019531, 'learning_rate': 1.7392824796965703e-05, 'epoch': 0.78}\n",
      "{'loss': 3.5044, 'grad_norm': 343.53680419921875, 'learning_rate': 1.7369533159843368e-05, 'epoch': 0.78}\n",
      "{'loss': 2.9619, 'grad_norm': 320.541748046875, 'learning_rate': 1.734615368996573e-05, 'epoch': 0.79}\n",
      "{'loss': 3.0017, 'grad_norm': 358.0092468261719, 'learning_rate': 1.7322686665977738e-05, 'epoch': 0.79}\n",
      "{'loss': 3.1052, 'grad_norm': 345.7085266113281, 'learning_rate': 1.7299132367567856e-05, 'epoch': 0.79}\n",
      "{'loss': 3.9739, 'grad_norm': 378.9649658203125, 'learning_rate': 1.7275491075464716e-05, 'epoch': 0.8}\n",
      "{'loss': 3.4665, 'grad_norm': 522.474365234375, 'learning_rate': 1.7251763071433767e-05, 'epoch': 0.8}\n",
      "{'loss': 3.4397, 'grad_norm': 189.32200622558594, 'learning_rate': 1.7227948638273918e-05, 'epoch': 0.8}\n",
      "{'loss': 3.5687, 'grad_norm': 180.89002990722656, 'learning_rate': 1.7204048059814175e-05, 'epoch': 0.81}\n",
      "{'loss': 3.6708, 'grad_norm': 213.6588134765625, 'learning_rate': 1.7180061620910263e-05, 'epoch': 0.81}\n",
      "{'loss': 3.6975, 'grad_norm': 179.828125, 'learning_rate': 1.715598960744121e-05, 'epoch': 0.81}\n",
      "{'loss': 3.5792, 'grad_norm': 245.00982666015625, 'learning_rate': 1.7131832306305964e-05, 'epoch': 0.82}\n",
      "{'loss': 3.7479, 'grad_norm': 207.39306640625, 'learning_rate': 1.710759000541995e-05, 'epoch': 0.82}\n",
      "{'loss': 3.8106, 'grad_norm': 250.56822204589844, 'learning_rate': 1.7083262993711663e-05, 'epoch': 0.82}\n",
      "{'loss': 3.8781, 'grad_norm': 253.75485229492188, 'learning_rate': 1.7058851561119198e-05, 'epoch': 0.83}\n",
      "{'loss': 3.8536, 'grad_norm': 201.4251708984375, 'learning_rate': 1.7034355998586828e-05, 'epoch': 0.83}\n",
      "{'loss': 4.2335, 'grad_norm': 262.30377197265625, 'learning_rate': 1.7009776598061496e-05, 'epoch': 0.83}\n",
      "{'loss': 3.9617, 'grad_norm': 202.80520629882812, 'learning_rate': 1.6985113652489374e-05, 'epoch': 0.84}\n",
      "{'loss': 3.8283, 'grad_norm': 233.06814575195312, 'learning_rate': 1.6960367455812336e-05, 'epoch': 0.84}\n",
      "{'loss': 3.96, 'grad_norm': 259.1177673339844, 'learning_rate': 1.6935538302964496e-05, 'epoch': 0.84}\n",
      "{'loss': 4.2096, 'grad_norm': 248.18994140625, 'learning_rate': 1.691062648986865e-05, 'epoch': 0.84}\n",
      "{'loss': 3.9578, 'grad_norm': 219.859619140625, 'learning_rate': 1.6885632313432772e-05, 'epoch': 0.85}\n",
      "{'loss': 3.7402, 'grad_norm': 226.76007080078125, 'learning_rate': 1.686055607154648e-05, 'epoch': 0.85}\n",
      "{'loss': 3.6938, 'grad_norm': 265.12255859375, 'learning_rate': 1.6835398063077476e-05, 'epoch': 0.85}\n",
      "{'loss': 3.6796, 'grad_norm': 246.4375762939453, 'learning_rate': 1.6810158587867973e-05, 'epoch': 0.86}\n",
      "{'loss': 3.9773, 'grad_norm': 244.4228515625, 'learning_rate': 1.6784837946731148e-05, 'epoch': 0.86}\n",
      "{'loss': 3.79, 'grad_norm': 320.3280029296875, 'learning_rate': 1.6759436441447544e-05, 'epoch': 0.86}\n",
      "{'loss': 3.6145, 'grad_norm': 274.9872131347656, 'learning_rate': 1.673395437476146e-05, 'epoch': 0.87}\n",
      "{'loss': 3.7488, 'grad_norm': 301.2032775878906, 'learning_rate': 1.6708392050377365e-05, 'epoch': 0.87}\n",
      "{'loss': 3.8057, 'grad_norm': 269.6919860839844, 'learning_rate': 1.668274977295626e-05, 'epoch': 0.87}\n",
      "{'loss': 3.7773, 'grad_norm': 229.89903259277344, 'learning_rate': 1.6657027848112064e-05, 'epoch': 0.88}\n",
      "{'loss': 4.1109, 'grad_norm': 279.5765686035156, 'learning_rate': 1.6631226582407954e-05, 'epoch': 0.88}\n",
      "{'loss': 3.6196, 'grad_norm': 293.50775146484375, 'learning_rate': 1.660534628335273e-05, 'epoch': 0.88}\n",
      "{'loss': 3.4464, 'grad_norm': 227.59181213378906, 'learning_rate': 1.657938725939713e-05, 'epoch': 0.89}\n",
      "{'loss': 3.6343, 'grad_norm': 225.41326904296875, 'learning_rate': 1.6553349819930167e-05, 'epoch': 0.89}\n",
      "{'loss': 3.791, 'grad_norm': 203.53965759277344, 'learning_rate': 1.6527234275275445e-05, 'epoch': 0.89}\n",
      "{'loss': 3.9601, 'grad_norm': 260.8529968261719, 'learning_rate': 1.6501040936687444e-05, 'epoch': 0.9}\n",
      "{'loss': 3.431, 'grad_norm': 232.5889892578125, 'learning_rate': 1.6474770116347824e-05, 'epoch': 0.9}\n",
      "{'loss': 3.7655, 'grad_norm': 257.8642883300781, 'learning_rate': 1.6448422127361707e-05, 'epoch': 0.9}\n",
      "{'loss': 3.6802, 'grad_norm': 313.7982482910156, 'learning_rate': 1.6421997283753928e-05, 'epoch': 0.91}\n",
      "{'loss': 3.5705, 'grad_norm': 269.6110534667969, 'learning_rate': 1.6395495900465306e-05, 'epoch': 0.91}\n",
      "{'loss': 3.6542, 'grad_norm': 300.8401794433594, 'learning_rate': 1.6368918293348893e-05, 'epoch': 0.91}\n",
      "{'loss': 3.8497, 'grad_norm': 280.759033203125, 'learning_rate': 1.63422647791662e-05, 'epoch': 0.92}\n",
      "{'loss': 3.7817, 'grad_norm': 300.58587646484375, 'learning_rate': 1.6315535675583425e-05, 'epoch': 0.92}\n",
      "{'loss': 3.8347, 'grad_norm': 310.1039123535156, 'learning_rate': 1.6288731301167667e-05, 'epoch': 0.92}\n",
      "{'loss': 3.8355, 'grad_norm': 311.36077880859375, 'learning_rate': 1.626185197538314e-05, 'epoch': 0.92}\n",
      "{'loss': 3.6746, 'grad_norm': 381.5538024902344, 'learning_rate': 1.6234898018587336e-05, 'epoch': 0.93}\n",
      "{'loss': 3.714, 'grad_norm': 308.0241394042969, 'learning_rate': 1.6207869752027248e-05, 'epoch': 0.93}\n",
      "{'loss': 3.4817, 'grad_norm': 318.5294189453125, 'learning_rate': 1.6180767497835503e-05, 'epoch': 0.93}\n",
      "{'loss': 3.3901, 'grad_norm': 435.1400146484375, 'learning_rate': 1.6153591579026545e-05, 'epoch': 0.94}\n",
      "{'loss': 3.0869, 'grad_norm': 273.54168701171875, 'learning_rate': 1.6126342319492784e-05, 'epoch': 0.94}\n",
      "{'loss': 3.3272, 'grad_norm': 346.5904846191406, 'learning_rate': 1.609902004400073e-05, 'epoch': 0.94}\n",
      "{'loss': 3.3269, 'grad_norm': 400.756103515625, 'learning_rate': 1.6071625078187113e-05, 'epoch': 0.95}\n",
      "{'loss': 3.3844, 'grad_norm': 410.6910705566406, 'learning_rate': 1.6044157748555024e-05, 'epoch': 0.95}\n",
      "{'loss': 3.2422, 'grad_norm': 408.6650390625, 'learning_rate': 1.6016618382470014e-05, 'epoch': 0.95}\n",
      "{'loss': 3.6854, 'grad_norm': 451.6663818359375, 'learning_rate': 1.598900730815617e-05, 'epoch': 0.96}\n",
      "{'loss': 3.4134, 'grad_norm': 533.9920654296875, 'learning_rate': 1.5961324854692254e-05, 'epoch': 0.96}\n",
      "{'loss': 3.6918, 'grad_norm': 189.71005249023438, 'learning_rate': 1.593357135200773e-05, 'epoch': 0.96}\n",
      "{'loss': 3.679, 'grad_norm': 210.22906494140625, 'learning_rate': 1.5905747130878853e-05, 'epoch': 0.97}\n",
      "{'loss': 4.0425, 'grad_norm': 263.99237060546875, 'learning_rate': 1.5877852522924733e-05, 'epoch': 0.97}\n",
      "{'loss': 4.0937, 'grad_norm': 346.00360107421875, 'learning_rate': 1.5849887860603374e-05, 'epoch': 0.97}\n",
      "{'loss': 3.8139, 'grad_norm': 289.4018859863281, 'learning_rate': 1.582185347720771e-05, 'epoch': 0.98}\n",
      "{'loss': 3.677, 'grad_norm': 307.5521545410156, 'learning_rate': 1.5793749706861637e-05, 'epoch': 0.98}\n",
      "{'loss': 4.0964, 'grad_norm': 280.57330322265625, 'learning_rate': 1.576557688451603e-05, 'epoch': 0.98}\n",
      "{'loss': 4.07, 'grad_norm': 377.9961242675781, 'learning_rate': 1.5737335345944758e-05, 'epoch': 0.99}\n",
      "{'loss': 3.8657, 'grad_norm': 258.3074035644531, 'learning_rate': 1.570902542774066e-05, 'epoch': 0.99}\n",
      "{'loss': 3.4727, 'grad_norm': 310.66571044921875, 'learning_rate': 1.568064746731156e-05, 'epoch': 0.99}\n",
      "{'loss': 3.605, 'grad_norm': 313.0718688964844, 'learning_rate': 1.5652201802876227e-05, 'epoch': 1.0}\n",
      "{'loss': 3.0715, 'grad_norm': 373.41131591796875, 'learning_rate': 1.5623688773460358e-05, 'epoch': 1.0}\n",
      "{'loss': 3.0948, 'grad_norm': 537.1317138671875, 'learning_rate': 1.559510871889252e-05, 'epoch': 1.0}\n",
      "{'loss': 3.7736, 'grad_norm': 183.36773681640625, 'learning_rate': 1.556646197980012e-05, 'epoch': 1.0}\n",
      "{'loss': 3.5218, 'grad_norm': 192.42735290527344, 'learning_rate': 1.553774889760533e-05, 'epoch': 1.01}\n",
      "{'loss': 3.5428, 'grad_norm': 172.6455078125, 'learning_rate': 1.5508969814521026e-05, 'epoch': 1.01}\n",
      "{'loss': 3.4367, 'grad_norm': 188.0196533203125, 'learning_rate': 1.5480125073546705e-05, 'epoch': 1.01}\n",
      "{'loss': 3.6404, 'grad_norm': 190.76451110839844, 'learning_rate': 1.5451215018464386e-05, 'epoch': 1.02}\n",
      "{'loss': 3.8438, 'grad_norm': 222.21128845214844, 'learning_rate': 1.542223999383455e-05, 'epoch': 1.02}\n",
      "{'loss': 3.8146, 'grad_norm': 201.40220642089844, 'learning_rate': 1.5393200344991993e-05, 'epoch': 1.02}\n",
      "{'loss': 3.9612, 'grad_norm': 203.3741912841797, 'learning_rate': 1.5364096418041723e-05, 'epoch': 1.03}\n",
      "{'loss': 3.8593, 'grad_norm': 253.8621368408203, 'learning_rate': 1.533492855985485e-05, 'epoch': 1.03}\n",
      "{'loss': 3.8283, 'grad_norm': 281.75384521484375, 'learning_rate': 1.530569711806443e-05, 'epoch': 1.03}\n",
      "{'loss': 4.1273, 'grad_norm': 275.68243408203125, 'learning_rate': 1.527640244106133e-05, 'epoch': 1.04}\n",
      "{'loss': 3.971, 'grad_norm': 240.1037139892578, 'learning_rate': 1.524704487799008e-05, 'epoch': 1.04}\n",
      "{'loss': 4.0375, 'grad_norm': 302.9636535644531, 'learning_rate': 1.5217624778744718e-05, 'epoch': 1.04}\n",
      "{'loss': 4.0873, 'grad_norm': 280.4884338378906, 'learning_rate': 1.5188142493964595e-05, 'epoch': 1.04}\n",
      "{'loss': 3.9474, 'grad_norm': 271.6686096191406, 'learning_rate': 1.5158598375030218e-05, 'epoch': 1.05}\n",
      "{'loss': 3.9183, 'grad_norm': 285.09222412109375, 'learning_rate': 1.5128992774059063e-05, 'epoch': 1.05}\n",
      "{'loss': 3.7232, 'grad_norm': 299.57183837890625, 'learning_rate': 1.5099326043901361e-05, 'epoch': 1.05}\n",
      "{'loss': 3.752, 'grad_norm': 296.18157958984375, 'learning_rate': 1.5069598538135905e-05, 'epoch': 1.06}\n",
      "{'loss': 3.8363, 'grad_norm': 302.93145751953125, 'learning_rate': 1.503981061106584e-05, 'epoch': 1.06}\n",
      "{'loss': 4.0865, 'grad_norm': 286.0591125488281, 'learning_rate': 1.5009962617714425e-05, 'epoch': 1.06}\n",
      "{'loss': 3.9612, 'grad_norm': 318.4242248535156, 'learning_rate': 1.4980054913820814e-05, 'epoch': 1.07}\n",
      "{'loss': 3.4397, 'grad_norm': 313.8877258300781, 'learning_rate': 1.4950087855835816e-05, 'epoch': 1.07}\n",
      "{'loss': 3.9223, 'grad_norm': 357.3001403808594, 'learning_rate': 1.4920061800917637e-05, 'epoch': 1.07}\n",
      "{'loss': 3.7615, 'grad_norm': 323.5860900878906, 'learning_rate': 1.4889977106927642e-05, 'epoch': 1.08}\n",
      "{'loss': 3.6263, 'grad_norm': 398.5797424316406, 'learning_rate': 1.485983413242606e-05, 'epoch': 1.08}\n",
      "{'loss': 3.7276, 'grad_norm': 423.0249328613281, 'learning_rate': 1.4829633236667746e-05, 'epoch': 1.08}\n",
      "{'loss': 3.5561, 'grad_norm': 355.04248046875, 'learning_rate': 1.4799374779597866e-05, 'epoch': 1.09}\n",
      "{'loss': 3.8209, 'grad_norm': 375.3748474121094, 'learning_rate': 1.476905912184763e-05, 'epoch': 1.09}\n",
      "{'loss': 3.572, 'grad_norm': 398.02618408203125, 'learning_rate': 1.4738686624729987e-05, 'epoch': 1.09}\n",
      "{'loss': 3.638, 'grad_norm': 351.74676513671875, 'learning_rate': 1.470825765023532e-05, 'epoch': 1.1}\n",
      "{'loss': 3.6787, 'grad_norm': 401.6466979980469, 'learning_rate': 1.4677772561027121e-05, 'epoch': 1.1}\n",
      "{'loss': 3.5346, 'grad_norm': 359.5524597167969, 'learning_rate': 1.4647231720437687e-05, 'epoch': 1.1}\n",
      "{'loss': 3.7521, 'grad_norm': 415.85772705078125, 'learning_rate': 1.4616635492463775e-05, 'epoch': 1.11}\n",
      "{'loss': 3.6696, 'grad_norm': 300.148681640625, 'learning_rate': 1.4585984241762268e-05, 'epoch': 1.11}\n",
      "{'loss': 3.5011, 'grad_norm': 359.0833435058594, 'learning_rate': 1.4555278333645833e-05, 'epoch': 1.11}\n",
      "{'loss': 3.6281, 'grad_norm': 371.59014892578125, 'learning_rate': 1.4524518134078565e-05, 'epoch': 1.12}\n",
      "{'loss': 3.7777, 'grad_norm': 329.5606994628906, 'learning_rate': 1.4493704009671614e-05, 'epoch': 1.12}\n",
      "{'loss': 3.3799, 'grad_norm': 408.97021484375, 'learning_rate': 1.446283632767884e-05, 'epoch': 1.12}\n",
      "{'loss': 3.5102, 'grad_norm': 318.2907409667969, 'learning_rate': 1.4431915455992416e-05, 'epoch': 1.12}\n",
      "{'loss': 3.5127, 'grad_norm': 336.3554382324219, 'learning_rate': 1.440094176313844e-05, 'epoch': 1.13}\n",
      "{'loss': 3.6893, 'grad_norm': 328.5307312011719, 'learning_rate': 1.4369915618272568e-05, 'epoch': 1.13}\n",
      "{'loss': 3.3297, 'grad_norm': 364.7291259765625, 'learning_rate': 1.4338837391175582e-05, 'epoch': 1.13}\n",
      "{'loss': 3.113, 'grad_norm': 326.18853759765625, 'learning_rate': 1.4307707452249013e-05, 'epoch': 1.14}\n",
      "{'loss': 3.8404, 'grad_norm': 339.25146484375, 'learning_rate': 1.42765261725107e-05, 'epoch': 1.14}\n",
      "{'loss': 3.3531, 'grad_norm': 381.1528625488281, 'learning_rate': 1.424529392359039e-05, 'epoch': 1.14}\n",
      "{'loss': 3.365, 'grad_norm': 387.93890380859375, 'learning_rate': 1.4214011077725293e-05, 'epoch': 1.15}\n",
      "{'loss': 2.7689, 'grad_norm': 402.8507385253906, 'learning_rate': 1.4182678007755653e-05, 'epoch': 1.15}\n",
      "{'loss': 2.7849, 'grad_norm': 300.97906494140625, 'learning_rate': 1.4151295087120307e-05, 'epoch': 1.15}\n",
      "{'loss': 3.1254, 'grad_norm': 563.7349853515625, 'learning_rate': 1.4119862689852224e-05, 'epoch': 1.16}\n",
      "{'loss': 3.1857, 'grad_norm': 563.33203125, 'learning_rate': 1.4088381190574051e-05, 'epoch': 1.16}\n",
      "{'loss': 3.3636, 'grad_norm': 257.908203125, 'learning_rate': 1.4056850964493668e-05, 'epoch': 1.16}\n",
      "{'loss': 3.6198, 'grad_norm': 203.20767211914062, 'learning_rate': 1.4025272387399676e-05, 'epoch': 1.17}\n",
      "{'loss': 3.4456, 'grad_norm': 221.9078826904297, 'learning_rate': 1.3993645835656955e-05, 'epoch': 1.17}\n",
      "{'loss': 3.3665, 'grad_norm': 222.35653686523438, 'learning_rate': 1.3961971686202163e-05, 'epoch': 1.17}\n",
      "{'loss': 3.535, 'grad_norm': 229.93624877929688, 'learning_rate': 1.3930250316539237e-05, 'epoch': 1.18}\n",
      "{'loss': 3.5742, 'grad_norm': 268.2081298828125, 'learning_rate': 1.3898482104734909e-05, 'epoch': 1.18}\n",
      "{'loss': 3.7538, 'grad_norm': 232.27975463867188, 'learning_rate': 1.3866667429414188e-05, 'epoch': 1.18}\n",
      "{'loss': 4.1067, 'grad_norm': 272.919189453125, 'learning_rate': 1.383480666975586e-05, 'epoch': 1.19}\n",
      "{'loss': 4.0732, 'grad_norm': 317.59637451171875, 'learning_rate': 1.3802900205487948e-05, 'epoch': 1.19}\n",
      "{'loss': 4.0229, 'grad_norm': 265.5113220214844, 'learning_rate': 1.3770948416883205e-05, 'epoch': 1.19}\n",
      "{'loss': 3.909, 'grad_norm': 242.55148315429688, 'learning_rate': 1.3738951684754585e-05, 'epoch': 1.2}\n",
      "{'loss': 3.6926, 'grad_norm': 274.654541015625, 'learning_rate': 1.3706910390450679e-05, 'epoch': 1.2}\n",
      "{'loss': 3.8036, 'grad_norm': 321.1575622558594, 'learning_rate': 1.3674824915851193e-05, 'epoch': 1.2}\n",
      "{'loss': 3.8652, 'grad_norm': 432.94940185546875, 'learning_rate': 1.3642695643362398e-05, 'epoch': 1.2}\n",
      "{'loss': 3.8427, 'grad_norm': 331.1347961425781, 'learning_rate': 1.3610522955912551e-05, 'epoch': 1.21}\n",
      "{'loss': 3.6117, 'grad_norm': 259.0882873535156, 'learning_rate': 1.3578307236947348e-05, 'epoch': 1.21}\n",
      "{'loss': 3.9156, 'grad_norm': 263.1017761230469, 'learning_rate': 1.3546048870425356e-05, 'epoch': 1.21}\n",
      "{'loss': 3.6381, 'grad_norm': 338.0381164550781, 'learning_rate': 1.3513748240813429e-05, 'epoch': 1.22}\n",
      "{'loss': 3.6697, 'grad_norm': 335.01824951171875, 'learning_rate': 1.3481405733082118e-05, 'epoch': 1.22}\n",
      "{'loss': 3.7028, 'grad_norm': 345.1737060546875, 'learning_rate': 1.3449021732701106e-05, 'epoch': 1.22}\n",
      "{'loss': 3.7279, 'grad_norm': 293.1334228515625, 'learning_rate': 1.3416596625634595e-05, 'epoch': 1.23}\n",
      "{'loss': 3.9064, 'grad_norm': 314.87420654296875, 'learning_rate': 1.3384130798336705e-05, 'epoch': 1.23}\n",
      "{'loss': 4.0129, 'grad_norm': 334.58892822265625, 'learning_rate': 1.3351624637746885e-05, 'epoch': 1.23}\n",
      "{'loss': 3.6124, 'grad_norm': 358.75762939453125, 'learning_rate': 1.3319078531285286e-05, 'epoch': 1.24}\n",
      "{'loss': 3.5568, 'grad_norm': 290.2950134277344, 'learning_rate': 1.3286492866848143e-05, 'epoch': 1.24}\n",
      "{'loss': 3.7259, 'grad_norm': 348.2655029296875, 'learning_rate': 1.3253868032803171e-05, 'epoch': 1.24}\n",
      "{'loss': 3.4806, 'grad_norm': 311.4715270996094, 'learning_rate': 1.3221204417984907e-05, 'epoch': 1.25}\n",
      "{'loss': 3.6536, 'grad_norm': 284.98504638671875, 'learning_rate': 1.3188502411690101e-05, 'epoch': 1.25}\n",
      "{'loss': 3.6713, 'grad_norm': 274.98016357421875, 'learning_rate': 1.3155762403673065e-05, 'epoch': 1.25}\n",
      "{'loss': 3.7235, 'grad_norm': 284.0035400390625, 'learning_rate': 1.3122984784141021e-05, 'epoch': 1.26}\n",
      "{'loss': 3.2992, 'grad_norm': 283.0602722167969, 'learning_rate': 1.3090169943749475e-05, 'epoch': 1.26}\n",
      "{'loss': 3.6191, 'grad_norm': 284.5191345214844, 'learning_rate': 1.3057318273597531e-05, 'epoch': 1.26}\n",
      "{'loss': 3.2969, 'grad_norm': 302.2982177734375, 'learning_rate': 1.3024430165223245e-05, 'epoch': 1.27}\n",
      "{'loss': 3.4499, 'grad_norm': 241.2523651123047, 'learning_rate': 1.2991506010598965e-05, 'epoch': 1.27}\n",
      "{'loss': 3.3512, 'grad_norm': 246.1725311279297, 'learning_rate': 1.2958546202126638e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1214, 'grad_norm': 280.9154052734375, 'learning_rate': 1.2925551132633164e-05, 'epoch': 1.28}\n",
      "{'loss': 3.6704, 'grad_norm': 261.22088623046875, 'learning_rate': 1.2892521195365679e-05, 'epoch': 1.28}\n",
      "{'loss': 3.8046, 'grad_norm': 288.2083740234375, 'learning_rate': 1.2859456783986892e-05, 'epoch': 1.28}\n",
      "{'loss': 3.4525, 'grad_norm': 312.58502197265625, 'learning_rate': 1.2826358292570398e-05, 'epoch': 1.28}\n",
      "{'loss': 3.3964, 'grad_norm': 362.14874267578125, 'learning_rate': 1.2793226115595951e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1308, 'grad_norm': 238.31082153320312, 'learning_rate': 1.2760060647944794e-05, 'epoch': 1.29}\n",
      "{'loss': 3.5573, 'grad_norm': 302.9606018066406, 'learning_rate': 1.2726862284894939e-05, 'epoch': 1.29}\n",
      "{'loss': 3.8729, 'grad_norm': 251.40792846679688, 'learning_rate': 1.2693631422116455e-05, 'epoch': 1.3}\n",
      "{'loss': 2.8582, 'grad_norm': 328.90521240234375, 'learning_rate': 1.2660368455666752e-05, 'epoch': 1.3}\n",
      "{'loss': 3.3253, 'grad_norm': 279.33770751953125, 'learning_rate': 1.262707378198587e-05, 'epoch': 1.3}\n",
      "{'loss': 3.1682, 'grad_norm': 359.2819519042969, 'learning_rate': 1.2593747797891743e-05, 'epoch': 1.31}\n",
      "{'loss': 2.8662, 'grad_norm': 265.2848205566406, 'learning_rate': 1.2560390900575472e-05, 'epoch': 1.31}\n",
      "{'loss': 3.5508, 'grad_norm': 459.4678955078125, 'learning_rate': 1.2527003487596598e-05, 'epoch': 1.31}\n",
      "{'loss': 2.9489, 'grad_norm': 322.06622314453125, 'learning_rate': 1.2493585956878354e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1273, 'grad_norm': 290.7131042480469, 'learning_rate': 1.2460138706702929e-05, 'epoch': 1.32}\n",
      "{'loss': 3.4924, 'grad_norm': 175.58670043945312, 'learning_rate': 1.242666213570672e-05, 'epoch': 1.32}\n",
      "{'loss': 3.4871, 'grad_norm': 190.61854553222656, 'learning_rate': 1.2393156642875579e-05, 'epoch': 1.33}\n",
      "{'loss': 3.517, 'grad_norm': 171.37786865234375, 'learning_rate': 1.2359622627540059e-05, 'epoch': 1.33}\n",
      "{'loss': 3.3959, 'grad_norm': 168.6323699951172, 'learning_rate': 1.2326060489370655e-05, 'epoch': 1.33}\n",
      "{'loss': 3.4086, 'grad_norm': 231.1112060546875, 'learning_rate': 1.229247062837304e-05, 'epoch': 1.34}\n",
      "{'loss': 3.437, 'grad_norm': 216.8575897216797, 'learning_rate': 1.2258853444883297e-05, 'epoch': 1.34}\n",
      "{'loss': 3.5516, 'grad_norm': 228.28941345214844, 'learning_rate': 1.2225209339563144e-05, 'epoch': 1.34}\n",
      "{'loss': 3.6696, 'grad_norm': 238.57398986816406, 'learning_rate': 1.219153871339518e-05, 'epoch': 1.35}\n",
      "{'loss': 3.9555, 'grad_norm': 308.90185546875, 'learning_rate': 1.2157841967678064e-05, 'epoch': 1.35}\n",
      "{'loss': 3.6502, 'grad_norm': 299.10748291015625, 'learning_rate': 1.2124119504021776e-05, 'epoch': 1.35}\n",
      "{'loss': 3.7835, 'grad_norm': 265.5299377441406, 'learning_rate': 1.2090371724342804e-05, 'epoch': 1.36}\n",
      "{'loss': 4.0837, 'grad_norm': 339.8041076660156, 'learning_rate': 1.2056599030859367e-05, 'epoch': 1.36}\n",
      "{'loss': 3.6538, 'grad_norm': 319.4033508300781, 'learning_rate': 1.2022801826086609e-05, 'epoch': 1.36}\n",
      "{'loss': 3.6627, 'grad_norm': 340.22296142578125, 'learning_rate': 1.1988980512831809e-05, 'epoch': 1.36}\n",
      "{'loss': 3.8491, 'grad_norm': 264.0929260253906, 'learning_rate': 1.195513549418959e-05, 'epoch': 1.37}\n",
      "{'loss': 3.4962, 'grad_norm': 265.4762268066406, 'learning_rate': 1.1921267173537085e-05, 'epoch': 1.37}\n",
      "{'loss': 3.3344, 'grad_norm': 288.8684387207031, 'learning_rate': 1.1887375954529167e-05, 'epoch': 1.37}\n",
      "{'loss': 3.8882, 'grad_norm': 247.17770385742188, 'learning_rate': 1.1853462241093614e-05, 'epoch': 1.38}\n",
      "{'loss': 3.5546, 'grad_norm': 305.73248291015625, 'learning_rate': 1.1819526437426298e-05, 'epoch': 1.38}\n",
      "{'loss': 3.5288, 'grad_norm': 290.132080078125, 'learning_rate': 1.1785568947986368e-05, 'epoch': 1.38}\n",
      "{'loss': 3.6445, 'grad_norm': 245.9775390625, 'learning_rate': 1.1751590177491441e-05, 'epoch': 1.39}\n",
      "{'loss': 3.6305, 'grad_norm': 284.5626220703125, 'learning_rate': 1.1717590530912764e-05, 'epoch': 1.39}\n",
      "{'loss': 3.3522, 'grad_norm': 234.50094604492188, 'learning_rate': 1.1683570413470384e-05, 'epoch': 1.39}\n",
      "{'loss': 3.2212, 'grad_norm': 260.24212646484375, 'learning_rate': 1.164953023062835e-05, 'epoch': 1.4}\n",
      "{'loss': 3.4719, 'grad_norm': 326.0618591308594, 'learning_rate': 1.1615470388089836e-05, 'epoch': 1.4}\n",
      "{'loss': 3.7557, 'grad_norm': 277.2721862792969, 'learning_rate': 1.1581391291792336e-05, 'epoch': 1.4}\n",
      "{'loss': 3.4395, 'grad_norm': 298.3232421875, 'learning_rate': 1.1547293347902813e-05, 'epoch': 1.41}\n",
      "{'loss': 3.6284, 'grad_norm': 241.5583953857422, 'learning_rate': 1.151317696281287e-05, 'epoch': 1.41}\n",
      "{'loss': 3.4159, 'grad_norm': 222.34295654296875, 'learning_rate': 1.1479042543133895e-05, 'epoch': 1.41}\n",
      "{'loss': 3.0541, 'grad_norm': 201.26718139648438, 'learning_rate': 1.1444890495692214e-05, 'epoch': 1.42}\n",
      "{'loss': 3.9752, 'grad_norm': 268.0467834472656, 'learning_rate': 1.1410721227524256e-05, 'epoch': 1.42}\n",
      "{'loss': 3.7982, 'grad_norm': 216.8923797607422, 'learning_rate': 1.1376535145871685e-05, 'epoch': 1.42}\n",
      "{'loss': 3.203, 'grad_norm': 249.0296173095703, 'learning_rate': 1.1342332658176556e-05, 'epoch': 1.43}\n",
      "{'loss': 3.6622, 'grad_norm': 219.9296875, 'learning_rate': 1.1308114172076464e-05, 'epoch': 1.43}\n",
      "{'loss': 3.235, 'grad_norm': 241.98097229003906, 'learning_rate': 1.1273880095399667e-05, 'epoch': 1.43}\n",
      "{'loss': 3.3909, 'grad_norm': 246.54782104492188, 'learning_rate': 1.1239630836160246e-05, 'epoch': 1.44}\n",
      "{'loss': 3.1758, 'grad_norm': 253.1094970703125, 'learning_rate': 1.1205366802553231e-05, 'epoch': 1.44}\n",
      "{'loss': 3.2892, 'grad_norm': 322.36737060546875, 'learning_rate': 1.1171088402949739e-05, 'epoch': 1.44}\n",
      "{'loss': 3.5062, 'grad_norm': 305.8359069824219, 'learning_rate': 1.1136796045892102e-05, 'epoch': 1.44}\n",
      "{'loss': 3.6161, 'grad_norm': 256.3968811035156, 'learning_rate': 1.1102490140089009e-05, 'epoch': 1.45}\n",
      "{'loss': 3.4902, 'grad_norm': 293.2936706542969, 'learning_rate': 1.1068171094410618e-05, 'epoch': 1.45}\n",
      "{'loss': 3.2456, 'grad_norm': 227.0699005126953, 'learning_rate': 1.10338393178837e-05, 'epoch': 1.45}\n",
      "{'loss': 3.4629, 'grad_norm': 289.85955810546875, 'learning_rate': 1.0999495219686762e-05, 'epoch': 1.46}\n",
      "{'loss': 3.4357, 'grad_norm': 301.1109619140625, 'learning_rate': 1.0965139209145153e-05, 'epoch': 1.46}\n",
      "{'loss': 3.344, 'grad_norm': 345.1794128417969, 'learning_rate': 1.0930771695726201e-05, 'epoch': 1.46}\n",
      "{'loss': 2.9472, 'grad_norm': 265.6766357421875, 'learning_rate': 1.0896393089034336e-05, 'epoch': 1.47}\n",
      "{'loss': 2.8742, 'grad_norm': 440.6820068359375, 'learning_rate': 1.0862003798806195e-05, 'epoch': 1.47}\n",
      "{'loss': 2.5608, 'grad_norm': 330.95013427734375, 'learning_rate': 1.0827604234905749e-05, 'epoch': 1.47}\n",
      "{'loss': 2.9496, 'grad_norm': 314.1139831542969, 'learning_rate': 1.079319480731941e-05, 'epoch': 1.48}\n",
      "{'loss': 3.3622, 'grad_norm': 391.30499267578125, 'learning_rate': 1.0758775926151155e-05, 'epoch': 1.48}\n",
      "{'loss': 3.4482, 'grad_norm': 190.32562255859375, 'learning_rate': 1.0724348001617626e-05, 'epoch': 1.48}\n",
      "{'loss': 3.5306, 'grad_norm': 178.54342651367188, 'learning_rate': 1.0689911444043249e-05, 'epoch': 1.49}\n",
      "{'loss': 3.3611, 'grad_norm': 154.67904663085938, 'learning_rate': 1.0655466663855349e-05, 'epoch': 1.49}\n",
      "{'loss': 3.3546, 'grad_norm': 177.05740356445312, 'learning_rate': 1.0621014071579241e-05, 'epoch': 1.49}\n",
      "{'loss': 3.4756, 'grad_norm': 197.32528686523438, 'learning_rate': 1.0586554077833346e-05, 'epoch': 1.5}\n",
      "{'loss': 3.5979, 'grad_norm': 255.38365173339844, 'learning_rate': 1.0552087093324314e-05, 'epoch': 1.5}\n",
      "{'loss': 3.4662, 'grad_norm': 235.41165161132812, 'learning_rate': 1.0517613528842096e-05, 'epoch': 1.5}\n",
      "{'loss': 3.6846, 'grad_norm': 282.8819580078125, 'learning_rate': 1.0483133795255072e-05, 'epoch': 1.51}\n",
      "{'loss': 3.9239, 'grad_norm': 244.98492431640625, 'learning_rate': 1.044864830350515e-05, 'epoch': 1.51}\n",
      "{'loss': 3.5119, 'grad_norm': 222.9047088623047, 'learning_rate': 1.0414157464602866e-05, 'epoch': 1.51}\n",
      "{'loss': 3.9033, 'grad_norm': 276.60308837890625, 'learning_rate': 1.0379661689622477e-05, 'epoch': 1.52}\n",
      "{'loss': 3.9359, 'grad_norm': 269.7746887207031, 'learning_rate': 1.0345161389697083e-05, 'epoch': 1.52}\n",
      "{'loss': 3.9498, 'grad_norm': 312.48809814453125, 'learning_rate': 1.0310656976013704e-05, 'epoch': 1.52}\n",
      "{'loss': 3.864, 'grad_norm': 280.66204833984375, 'learning_rate': 1.027614885980839e-05, 'epoch': 1.52}\n",
      "{'loss': 3.5826, 'grad_norm': 280.4068298339844, 'learning_rate': 1.0241637452361323e-05, 'epoch': 1.53}\n",
      "{'loss': 3.5204, 'grad_norm': 322.6304626464844, 'learning_rate': 1.0207123164991912e-05, 'epoch': 1.53}\n",
      "{'loss': 3.796, 'grad_norm': 217.24777221679688, 'learning_rate': 1.0172606409053887e-05, 'epoch': 1.53}\n",
      "{'loss': 3.9354, 'grad_norm': 243.44241333007812, 'learning_rate': 1.0138087595930394e-05, 'epoch': 1.54}\n",
      "{'loss': 3.713, 'grad_norm': 237.39935302734375, 'learning_rate': 1.0103567137029111e-05, 'epoch': 1.54}\n",
      "{'loss': 3.46, 'grad_norm': 240.3647003173828, 'learning_rate': 1.0069045443777318e-05, 'epoch': 1.54}\n",
      "{'loss': 3.6392, 'grad_norm': 233.58615112304688, 'learning_rate': 1.0034522927617014e-05, 'epoch': 1.55}\n",
      "{'loss': 3.6715, 'grad_norm': 219.167724609375, 'learning_rate': 1e-05, 'epoch': 1.55}\n",
      "{'loss': 3.0664, 'grad_norm': 254.32472229003906, 'learning_rate': 9.965477072382989e-06, 'epoch': 1.55}\n",
      "{'loss': 3.4204, 'grad_norm': 292.25128173828125, 'learning_rate': 9.930954556222683e-06, 'epoch': 1.56}\n",
      "{'loss': 3.6312, 'grad_norm': 226.28054809570312, 'learning_rate': 9.896432862970892e-06, 'epoch': 1.56}\n",
      "{'loss': 3.3208, 'grad_norm': 264.69158935546875, 'learning_rate': 9.861912404069608e-06, 'epoch': 1.56}\n",
      "{'loss': 3.733, 'grad_norm': 298.3951110839844, 'learning_rate': 9.827393590946116e-06, 'epoch': 1.57}\n",
      "{'loss': 3.2317, 'grad_norm': 280.9689636230469, 'learning_rate': 9.79287683500809e-06, 'epoch': 1.57}\n",
      "{'loss': 3.8177, 'grad_norm': 295.49530029296875, 'learning_rate': 9.75836254763868e-06, 'epoch': 1.57}\n",
      "{'loss': 3.6656, 'grad_norm': 311.06549072265625, 'learning_rate': 9.723851140191613e-06, 'epoch': 1.58}\n",
      "{'loss': 3.6574, 'grad_norm': 265.635009765625, 'learning_rate': 9.689343023986303e-06, 'epoch': 1.58}\n",
      "{'loss': 3.4428, 'grad_norm': 332.8334045410156, 'learning_rate': 9.654838610302922e-06, 'epoch': 1.58}\n",
      "{'loss': 3.8277, 'grad_norm': 297.5727844238281, 'learning_rate': 9.620338310377526e-06, 'epoch': 1.59}\n",
      "{'loss': 3.3208, 'grad_norm': 298.1198425292969, 'learning_rate': 9.58584253539714e-06, 'epoch': 1.59}\n",
      "{'loss': 2.7613, 'grad_norm': 270.1164245605469, 'learning_rate': 9.551351696494854e-06, 'epoch': 1.59}\n",
      "{'loss': 3.4841, 'grad_norm': 262.400634765625, 'learning_rate': 9.516866204744932e-06, 'epoch': 1.6}\n",
      "{'loss': 3.7522, 'grad_norm': 363.6531982421875, 'learning_rate': 9.482386471157905e-06, 'epoch': 1.6}\n",
      "{'loss': 3.5577, 'grad_norm': 328.5450744628906, 'learning_rate': 9.447912906675687e-06, 'epoch': 1.6}\n",
      "{'loss': 3.3486, 'grad_norm': 316.1466369628906, 'learning_rate': 9.413445922166654e-06, 'epoch': 1.6}\n",
      "{'loss': 2.749, 'grad_norm': 295.333251953125, 'learning_rate': 9.378985928420764e-06, 'epoch': 1.61}\n",
      "{'loss': 3.3079, 'grad_norm': 288.597412109375, 'learning_rate': 9.344533336144653e-06, 'epoch': 1.61}\n",
      "{'loss': 2.8997, 'grad_norm': 251.40469360351562, 'learning_rate': 9.310088555956751e-06, 'epoch': 1.61}\n",
      "{'loss': 2.7704, 'grad_norm': 357.1193542480469, 'learning_rate': 9.275651998382377e-06, 'epoch': 1.62}\n",
      "{'loss': 2.9499, 'grad_norm': 395.808349609375, 'learning_rate': 9.241224073848848e-06, 'epoch': 1.62}\n",
      "{'loss': 2.9555, 'grad_norm': 272.1222839355469, 'learning_rate': 9.206805192680592e-06, 'epoch': 1.62}\n",
      "{'loss': 3.4129, 'grad_norm': 314.55853271484375, 'learning_rate': 9.172395765094255e-06, 'epoch': 1.63}\n",
      "{'loss': 2.8552, 'grad_norm': 371.7373046875, 'learning_rate': 9.137996201193807e-06, 'epoch': 1.63}\n",
      "{'loss': 3.0403, 'grad_norm': 388.0318603515625, 'learning_rate': 9.103606910965666e-06, 'epoch': 1.63}\n",
      "{'loss': 2.9417, 'grad_norm': 461.9571533203125, 'learning_rate': 9.069228304273802e-06, 'epoch': 1.64}\n",
      "{'loss': 3.3882, 'grad_norm': 548.137451171875, 'learning_rate': 9.034860790854848e-06, 'epoch': 1.64}\n",
      "{'loss': 3.2538, 'grad_norm': 156.44183349609375, 'learning_rate': 9.00050478031324e-06, 'epoch': 1.64}\n",
      "{'loss': 3.2421, 'grad_norm': 181.922607421875, 'learning_rate': 8.966160682116301e-06, 'epoch': 1.65}\n",
      "{'loss': 3.3424, 'grad_norm': 162.79000854492188, 'learning_rate': 8.931828905589385e-06, 'epoch': 1.65}\n",
      "{'loss': 3.2627, 'grad_norm': 192.02467346191406, 'learning_rate': 8.897509859910996e-06, 'epoch': 1.65}\n",
      "{'loss': 3.3496, 'grad_norm': 236.33470153808594, 'learning_rate': 8.863203954107902e-06, 'epoch': 1.66}\n",
      "{'loss': 3.4396, 'grad_norm': 349.2449645996094, 'learning_rate': 8.828911597050263e-06, 'epoch': 1.66}\n",
      "{'loss': 3.6792, 'grad_norm': 303.7754821777344, 'learning_rate': 8.79463319744677e-06, 'epoch': 1.66}\n",
      "{'loss': 3.618, 'grad_norm': 250.11863708496094, 'learning_rate': 8.760369163839759e-06, 'epoch': 1.67}\n",
      "{'loss': 3.8887, 'grad_norm': 281.2517395019531, 'learning_rate': 8.726119904600337e-06, 'epoch': 1.67}\n",
      "{'loss': 4.0339, 'grad_norm': 319.9852294921875, 'learning_rate': 8.691885827923541e-06, 'epoch': 1.67}\n",
      "{'loss': 3.8021, 'grad_norm': 251.4886016845703, 'learning_rate': 8.657667341823449e-06, 'epoch': 1.68}\n",
      "{'loss': 3.5653, 'grad_norm': 247.47442626953125, 'learning_rate': 8.62346485412832e-06, 'epoch': 1.68}\n",
      "{'loss': 3.9279, 'grad_norm': 274.34197998046875, 'learning_rate': 8.58927877247575e-06, 'epoch': 1.68}\n",
      "{'loss': 3.8884, 'grad_norm': 360.19012451171875, 'learning_rate': 8.55510950430779e-06, 'epoch': 1.68}\n",
      "{'loss': 3.7888, 'grad_norm': 285.7602844238281, 'learning_rate': 8.520957456866107e-06, 'epoch': 1.69}\n",
      "{'loss': 3.6571, 'grad_norm': 313.18963623046875, 'learning_rate': 8.48682303718713e-06, 'epoch': 1.69}\n",
      "{'loss': 3.6042, 'grad_norm': 292.20709228515625, 'learning_rate': 8.452706652097187e-06, 'epoch': 1.69}\n",
      "{'loss': 3.179, 'grad_norm': 238.78756713867188, 'learning_rate': 8.418608708207667e-06, 'epoch': 1.7}\n",
      "{'loss': 3.4542, 'grad_norm': 248.16807556152344, 'learning_rate': 8.384529611910164e-06, 'epoch': 1.7}\n",
      "{'loss': 3.2384, 'grad_norm': 210.8018798828125, 'learning_rate': 8.35046976937165e-06, 'epoch': 1.7}\n",
      "{'loss': 3.369, 'grad_norm': 206.96884155273438, 'learning_rate': 8.316429586529616e-06, 'epoch': 1.71}\n",
      "{'loss': 3.1661, 'grad_norm': 308.1412353515625, 'learning_rate': 8.28240946908724e-06, 'epoch': 1.71}\n",
      "{'loss': 3.3321, 'grad_norm': 273.1292419433594, 'learning_rate': 8.24840982250856e-06, 'epoch': 1.71}\n",
      "{'loss': 3.7815, 'grad_norm': 246.84054565429688, 'learning_rate': 8.214431052013636e-06, 'epoch': 1.72}\n",
      "{'loss': 3.8752, 'grad_norm': 227.55712890625, 'learning_rate': 8.180473562573705e-06, 'epoch': 1.72}\n",
      "{'loss': 3.4437, 'grad_norm': 234.55914306640625, 'learning_rate': 8.146537758906388e-06, 'epoch': 1.72}\n",
      "{'loss': 3.16, 'grad_norm': 227.4876251220703, 'learning_rate': 8.112624045470834e-06, 'epoch': 1.73}\n",
      "{'loss': 3.1117, 'grad_norm': 235.3656463623047, 'learning_rate': 8.078732826462917e-06, 'epoch': 1.73}\n",
      "{'loss': 3.4653, 'grad_norm': 336.0472106933594, 'learning_rate': 8.044864505810415e-06, 'epoch': 1.73}\n",
      "{'loss': 3.3033, 'grad_norm': 235.77066040039062, 'learning_rate': 8.011019487168193e-06, 'epoch': 1.74}\n",
      "{'loss': 3.3932, 'grad_norm': 301.1643371582031, 'learning_rate': 7.977198173913394e-06, 'epoch': 1.74}\n",
      "{'loss': 3.3469, 'grad_norm': 231.7421112060547, 'learning_rate': 7.943400969140635e-06, 'epoch': 1.74}\n",
      "{'loss': 3.4169, 'grad_norm': 197.64918518066406, 'learning_rate': 7.909628275657199e-06, 'epoch': 1.75}\n",
      "{'loss': 3.2868, 'grad_norm': 216.56979370117188, 'learning_rate': 7.875880495978227e-06, 'epoch': 1.75}\n",
      "{'loss': 3.3161, 'grad_norm': 196.31817626953125, 'learning_rate': 7.84215803232194e-06, 'epoch': 1.75}\n",
      "{'loss': 3.5244, 'grad_norm': 198.25759887695312, 'learning_rate': 7.808461286604828e-06, 'epoch': 1.76}\n",
      "{'loss': 3.4322, 'grad_norm': 201.33680725097656, 'learning_rate': 7.774790660436857e-06, 'epoch': 1.76}\n",
      "{'loss': 2.9572, 'grad_norm': 226.71401977539062, 'learning_rate': 7.741146555116708e-06, 'epoch': 1.76}\n",
      "{'loss': 3.1124, 'grad_norm': 212.16433715820312, 'learning_rate': 7.707529371626966e-06, 'epoch': 1.76}\n",
      "{'loss': 3.1107, 'grad_norm': 219.46945190429688, 'learning_rate': 7.67393951062935e-06, 'epoch': 1.77}\n",
      "{'loss': 3.4089, 'grad_norm': 222.55691528320312, 'learning_rate': 7.640377372459944e-06, 'epoch': 1.77}\n",
      "{'loss': 3.4343, 'grad_norm': 177.10040283203125, 'learning_rate': 7.606843357124426e-06, 'epoch': 1.77}\n",
      "{'loss': 3.7145, 'grad_norm': 255.79283142089844, 'learning_rate': 7.573337864293283e-06, 'epoch': 1.78}\n",
      "{'loss': 2.9097, 'grad_norm': 203.22320556640625, 'learning_rate': 7.539861293297073e-06, 'epoch': 1.78}\n",
      "{'loss': 2.9875, 'grad_norm': 242.09384155273438, 'learning_rate': 7.506414043121647e-06, 'epoch': 1.78}\n",
      "{'loss': 3.2757, 'grad_norm': 250.0952606201172, 'learning_rate': 7.472996512403403e-06, 'epoch': 1.79}\n",
      "{'loss': 2.758, 'grad_norm': 199.7403106689453, 'learning_rate': 7.4396090994245295e-06, 'epoch': 1.79}\n",
      "{'loss': 2.3695, 'grad_norm': 296.4782409667969, 'learning_rate': 7.406252202108258e-06, 'epoch': 1.79}\n",
      "{'loss': 3.3033, 'grad_norm': 338.37982177734375, 'learning_rate': 7.372926218014131e-06, 'epoch': 1.8}\n",
      "{'loss': 2.9804, 'grad_norm': 273.59088134765625, 'learning_rate': 7.33963154433325e-06, 'epoch': 1.8}\n",
      "{'loss': 3.3845, 'grad_norm': 169.11671447753906, 'learning_rate': 7.306368577883547e-06, 'epoch': 1.8}\n",
      "{'loss': 3.5195, 'grad_norm': 202.8220977783203, 'learning_rate': 7.273137715105063e-06, 'epoch': 1.81}\n",
      "{'loss': 3.3364, 'grad_norm': 185.3257598876953, 'learning_rate': 7.239939352055208e-06, 'epoch': 1.81}\n",
      "{'loss': 3.4495, 'grad_norm': 153.33270263671875, 'learning_rate': 7.2067738844040516e-06, 'epoch': 1.81}\n",
      "{'loss': 3.4283, 'grad_norm': 165.542724609375, 'learning_rate': 7.173641707429606e-06, 'epoch': 1.82}\n",
      "{'loss': 3.476, 'grad_norm': 183.89883422851562, 'learning_rate': 7.140543216013109e-06, 'epoch': 1.82}\n",
      "{'loss': 3.4228, 'grad_norm': 189.70327758789062, 'learning_rate': 7.107478804634324e-06, 'epoch': 1.82}\n",
      "{'loss': 3.6358, 'grad_norm': 218.91038513183594, 'learning_rate': 7.07444886736684e-06, 'epoch': 1.83}\n",
      "{'loss': 3.7053, 'grad_norm': 212.86280822753906, 'learning_rate': 7.041453797873363e-06, 'epoch': 1.83}\n",
      "{'loss': 3.7793, 'grad_norm': 223.43553161621094, 'learning_rate': 7.008493989401039e-06, 'epoch': 1.83}\n",
      "{'loss': 3.7195, 'grad_norm': 222.88180541992188, 'learning_rate': 6.975569834776757e-06, 'epoch': 1.84}\n",
      "{'loss': 3.5533, 'grad_norm': 200.8367462158203, 'learning_rate': 6.942681726402474e-06, 'epoch': 1.84}\n",
      "{'loss': 3.3269, 'grad_norm': 208.7314453125, 'learning_rate': 6.909830056250527e-06, 'epoch': 1.84}\n",
      "{'loss': 3.584, 'grad_norm': 213.197265625, 'learning_rate': 6.8770152158589806e-06, 'epoch': 1.84}\n",
      "{'loss': 3.5681, 'grad_norm': 255.65769958496094, 'learning_rate': 6.844237596326941e-06, 'epoch': 1.85}\n",
      "{'loss': 3.6036, 'grad_norm': 190.76535034179688, 'learning_rate': 6.811497588309901e-06, 'epoch': 1.85}\n",
      "{'loss': 3.7522, 'grad_norm': 221.70169067382812, 'learning_rate': 6.778795582015096e-06, 'epoch': 1.85}\n",
      "{'loss': 3.83, 'grad_norm': 240.83877563476562, 'learning_rate': 6.746131967196834e-06, 'epoch': 1.86}\n",
      "{'loss': 3.7992, 'grad_norm': 234.40875244140625, 'learning_rate': 6.7135071331518575e-06, 'epoch': 1.86}\n",
      "{'loss': 3.4172, 'grad_norm': 220.48802185058594, 'learning_rate': 6.680921468714718e-06, 'epoch': 1.86}\n",
      "{'loss': 3.6201, 'grad_norm': 249.543212890625, 'learning_rate': 6.648375362253119e-06, 'epoch': 1.87}\n",
      "{'loss': 3.4473, 'grad_norm': 225.89869689941406, 'learning_rate': 6.615869201663296e-06, 'epoch': 1.87}\n",
      "{'loss': 3.4818, 'grad_norm': 217.48434448242188, 'learning_rate': 6.583403374365406e-06, 'epoch': 1.87}\n",
      "{'loss': 3.4065, 'grad_norm': 219.27342224121094, 'learning_rate': 6.550978267298893e-06, 'epoch': 1.88}\n",
      "{'loss': 4.0498, 'grad_norm': 250.91702270507812, 'learning_rate': 6.518594266917883e-06, 'epoch': 1.88}\n",
      "{'loss': 3.5251, 'grad_norm': 201.4399871826172, 'learning_rate': 6.486251759186573e-06, 'epoch': 1.88}\n",
      "{'loss': 3.372, 'grad_norm': 209.4516143798828, 'learning_rate': 6.453951129574644e-06, 'epoch': 1.89}\n",
      "{'loss': 3.3432, 'grad_norm': 267.96270751953125, 'learning_rate': 6.421692763052654e-06, 'epoch': 1.89}\n",
      "{'loss': 3.2954, 'grad_norm': 183.48776245117188, 'learning_rate': 6.3894770440874525e-06, 'epoch': 1.89}\n",
      "{'loss': 3.8446, 'grad_norm': 177.5939178466797, 'learning_rate': 6.357304356637606e-06, 'epoch': 1.9}\n",
      "{'loss': 3.7283, 'grad_norm': 200.08349609375, 'learning_rate': 6.325175084148809e-06, 'epoch': 1.9}\n",
      "{'loss': 3.2606, 'grad_norm': 241.51651000976562, 'learning_rate': 6.293089609549325e-06, 'epoch': 1.9}\n",
      "{'loss': 3.4675, 'grad_norm': 221.7435302734375, 'learning_rate': 6.261048315245419e-06, 'epoch': 1.91}\n",
      "{'loss': 3.7641, 'grad_norm': 216.3920440673828, 'learning_rate': 6.229051583116796e-06, 'epoch': 1.91}\n",
      "{'loss': 3.7511, 'grad_norm': 249.19296264648438, 'learning_rate': 6.197099794512056e-06, 'epoch': 1.91}\n",
      "{'loss': 3.4566, 'grad_norm': 206.0795440673828, 'learning_rate': 6.165193330244144e-06, 'epoch': 1.92}\n",
      "{'loss': 3.3947, 'grad_norm': 183.5465087890625, 'learning_rate': 6.133332570585813e-06, 'epoch': 1.92}\n",
      "{'loss': 3.5857, 'grad_norm': 227.02333068847656, 'learning_rate': 6.101517895265094e-06, 'epoch': 1.92}\n",
      "{'loss': 3.4406, 'grad_norm': 203.08238220214844, 'learning_rate': 6.069749683460765e-06, 'epoch': 1.92}\n",
      "{'loss': 3.6075, 'grad_norm': 216.77874755859375, 'learning_rate': 6.03802831379784e-06, 'epoch': 1.93}\n",
      "{'loss': 3.7669, 'grad_norm': 288.790771484375, 'learning_rate': 6.006354164343047e-06, 'epoch': 1.93}\n",
      "{'loss': 3.3871, 'grad_norm': 179.71865844726562, 'learning_rate': 5.9747276126003265e-06, 'epoch': 1.93}\n",
      "{'loss': 3.5861, 'grad_norm': 224.923095703125, 'learning_rate': 5.943149035506337e-06, 'epoch': 1.94}\n",
      "{'loss': 2.9376, 'grad_norm': 199.3828125, 'learning_rate': 5.911618809425952e-06, 'epoch': 1.94}\n",
      "{'loss': 2.9087, 'grad_norm': 173.73973083496094, 'learning_rate': 5.880137310147782e-06, 'epoch': 1.94}\n",
      "{'loss': 3.1385, 'grad_norm': 167.63311767578125, 'learning_rate': 5.848704912879699e-06, 'epoch': 1.95}\n",
      "{'loss': 2.6682, 'grad_norm': 188.02784729003906, 'learning_rate': 5.8173219922443516e-06, 'epoch': 1.95}\n",
      "{'loss': 3.272, 'grad_norm': 204.3270721435547, 'learning_rate': 5.785988922274711e-06, 'epoch': 1.95}\n",
      "{'loss': 3.2964, 'grad_norm': 294.686767578125, 'learning_rate': 5.754706076409613e-06, 'epoch': 1.96}\n",
      "{'loss': 3.121, 'grad_norm': 282.8363952636719, 'learning_rate': 5.723473827489301e-06, 'epoch': 1.96}\n",
      "{'loss': 3.3243, 'grad_norm': 146.3834991455078, 'learning_rate': 5.692292547750989e-06, 'epoch': 1.96}\n",
      "{'loss': 3.4224, 'grad_norm': 165.16290283203125, 'learning_rate': 5.66116260882442e-06, 'epoch': 1.97}\n",
      "{'loss': 3.8589, 'grad_norm': 170.9505157470703, 'learning_rate': 5.630084381727434e-06, 'epoch': 1.97}\n",
      "{'loss': 3.9381, 'grad_norm': 193.3003387451172, 'learning_rate': 5.599058236861559e-06, 'epoch': 1.97}\n",
      "{'loss': 3.7761, 'grad_norm': 212.75025939941406, 'learning_rate': 5.5680845440075885e-06, 'epoch': 1.98}\n",
      "{'loss': 3.2734, 'grad_norm': 202.0560302734375, 'learning_rate': 5.537163672321161e-06, 'epoch': 1.98}\n",
      "{'loss': 3.2997, 'grad_norm': 196.40744018554688, 'learning_rate': 5.5062959903283855e-06, 'epoch': 1.98}\n",
      "{'loss': 3.4696, 'grad_norm': 173.98974609375, 'learning_rate': 5.475481865921441e-06, 'epoch': 1.99}\n",
      "{'loss': 3.5799, 'grad_norm': 223.36419677734375, 'learning_rate': 5.444721666354169e-06, 'epoch': 1.99}\n",
      "{'loss': 3.4144, 'grad_norm': 262.3838806152344, 'learning_rate': 5.414015758237734e-06, 'epoch': 1.99}\n",
      "{'loss': 3.1706, 'grad_norm': 244.71107482910156, 'learning_rate': 5.3833645075362295e-06, 'epoch': 2.0}\n",
      "{'loss': 3.1506, 'grad_norm': 221.81089782714844, 'learning_rate': 5.352768279562315e-06, 'epoch': 2.0}\n",
      "{'loss': 3.6307, 'grad_norm': 396.79071044921875, 'learning_rate': 5.32222743897288e-06, 'epoch': 2.0}\n",
      "{'loss': 3.2006, 'grad_norm': 131.85186767578125, 'learning_rate': 5.2917423497646834e-06, 'epoch': 2.0}\n",
      "{'loss': 3.2692, 'grad_norm': 130.96817016601562, 'learning_rate': 5.2613133752700145e-06, 'epoch': 2.01}\n",
      "{'loss': 3.427, 'grad_norm': 127.5953369140625, 'learning_rate': 5.230940878152371e-06, 'epoch': 2.01}\n",
      "{'loss': 3.2401, 'grad_norm': 120.60400390625, 'learning_rate': 5.200625220402139e-06, 'epoch': 2.01}\n",
      "{'loss': 3.3104, 'grad_norm': 133.56692504882812, 'learning_rate': 5.1703667633322575e-06, 'epoch': 2.02}\n",
      "{'loss': 3.468, 'grad_norm': 154.7547607421875, 'learning_rate': 5.14016586757394e-06, 'epoch': 2.02}\n",
      "{'loss': 3.4802, 'grad_norm': 143.68907165527344, 'learning_rate': 5.110022893072361e-06, 'epoch': 2.02}\n",
      "{'loss': 3.5098, 'grad_norm': 155.451904296875, 'learning_rate': 5.079938199082363e-06, 'epoch': 2.03}\n",
      "{'loss': 3.7608, 'grad_norm': 178.9055633544922, 'learning_rate': 5.049912144164186e-06, 'epoch': 2.03}\n",
      "{'loss': 3.7038, 'grad_norm': 182.4522247314453, 'learning_rate': 5.019945086179192e-06, 'epoch': 2.03}\n",
      "{'loss': 3.5626, 'grad_norm': 171.32090759277344, 'learning_rate': 4.9900373822855805e-06, 'epoch': 2.04}\n",
      "{'loss': 3.6174, 'grad_norm': 187.68310546875, 'learning_rate': 4.960189388934163e-06, 'epoch': 2.04}\n",
      "{'loss': 3.7614, 'grad_norm': 212.96987915039062, 'learning_rate': 4.930401461864099e-06, 'epoch': 2.04}\n",
      "{'loss': 3.8492, 'grad_norm': 206.99264526367188, 'learning_rate': 4.900673956098644e-06, 'epoch': 2.04}\n",
      "{'loss': 3.4679, 'grad_norm': 178.2736358642578, 'learning_rate': 4.87100722594094e-06, 'epoch': 2.05}\n",
      "{'loss': 3.537, 'grad_norm': 193.28594970703125, 'learning_rate': 4.841401624969782e-06, 'epoch': 2.05}\n",
      "{'loss': 3.315, 'grad_norm': 182.92880249023438, 'learning_rate': 4.811857506035407e-06, 'epoch': 2.05}\n",
      "{'loss': 3.4659, 'grad_norm': 190.2095947265625, 'learning_rate': 4.7823752212552855e-06, 'epoch': 2.06}\n",
      "{'loss': 3.6656, 'grad_norm': 154.00662231445312, 'learning_rate': 4.75295512200992e-06, 'epoch': 2.06}\n",
      "{'loss': 3.1674, 'grad_norm': 167.06988525390625, 'learning_rate': 4.7235975589386715e-06, 'epoch': 2.06}\n",
      "{'loss': 3.6486, 'grad_norm': 199.1947479248047, 'learning_rate': 4.694302881935574e-06, 'epoch': 2.07}\n",
      "{'loss': 3.7425, 'grad_norm': 204.98707580566406, 'learning_rate': 4.66507144014515e-06, 'epoch': 2.07}\n",
      "{'loss': 3.3951, 'grad_norm': 211.66046142578125, 'learning_rate': 4.635903581958276e-06, 'epoch': 2.07}\n",
      "{'loss': 3.5653, 'grad_norm': 223.82276916503906, 'learning_rate': 4.606799655008009e-06, 'epoch': 2.08}\n",
      "{'loss': 3.3953, 'grad_norm': 211.33358764648438, 'learning_rate': 4.5777600061654505e-06, 'epoch': 2.08}\n",
      "{'loss': 3.3478, 'grad_norm': 208.68157958984375, 'learning_rate': 4.5487849815356145e-06, 'epoch': 2.08}\n",
      "{'loss': 3.2597, 'grad_norm': 200.57821655273438, 'learning_rate': 4.519874926453303e-06, 'epoch': 2.09}\n",
      "{'loss': 3.3018, 'grad_norm': 179.1595001220703, 'learning_rate': 4.491030185478976e-06, 'epoch': 2.09}\n",
      "{'loss': 3.5413, 'grad_norm': 203.1044921875, 'learning_rate': 4.462251102394669e-06, 'epoch': 2.09}\n",
      "{'loss': 3.2256, 'grad_norm': 230.39097595214844, 'learning_rate': 4.433538020199882e-06, 'epoch': 2.1}\n",
      "{'loss': 3.2182, 'grad_norm': 217.7229766845703, 'learning_rate': 4.404891281107482e-06, 'epoch': 2.1}\n",
      "{'loss': 3.317, 'grad_norm': 192.1566925048828, 'learning_rate': 4.3763112265396445e-06, 'epoch': 2.1}\n",
      "{'loss': 3.3684, 'grad_norm': 197.2235870361328, 'learning_rate': 4.347798197123777e-06, 'epoch': 2.11}\n",
      "{'loss': 3.2182, 'grad_norm': 211.42811584472656, 'learning_rate': 4.319352532688444e-06, 'epoch': 2.11}\n",
      "{'loss': 3.3256, 'grad_norm': 210.90611267089844, 'learning_rate': 4.290974572259342e-06, 'epoch': 2.11}\n",
      "{'loss': 3.2452, 'grad_norm': 224.13253784179688, 'learning_rate': 4.262664654055247e-06, 'epoch': 2.12}\n",
      "{'loss': 2.8581, 'grad_norm': 163.63319396972656, 'learning_rate': 4.234423115483971e-06, 'epoch': 2.12}\n",
      "{'loss': 3.5409, 'grad_norm': 206.8763427734375, 'learning_rate': 4.206250293138366e-06, 'epoch': 2.12}\n",
      "{'loss': 3.2685, 'grad_norm': 247.46778869628906, 'learning_rate': 4.178146522792296e-06, 'epoch': 2.12}\n",
      "{'loss': 3.2282, 'grad_norm': 233.4980926513672, 'learning_rate': 4.15011213939663e-06, 'epoch': 2.13}\n",
      "{'loss': 3.325, 'grad_norm': 285.1667785644531, 'learning_rate': 4.12214747707527e-06, 'epoch': 2.13}\n",
      "{'loss': 2.89, 'grad_norm': 186.780029296875, 'learning_rate': 4.094252869121153e-06, 'epoch': 2.13}\n",
      "{'loss': 3.4228, 'grad_norm': 233.66085815429688, 'learning_rate': 4.066428647992275e-06, 'epoch': 2.14}\n",
      "{'loss': 3.2915, 'grad_norm': 324.1951599121094, 'learning_rate': 4.038675145307747e-06, 'epoch': 2.14}\n",
      "{'loss': 2.7073, 'grad_norm': 262.63299560546875, 'learning_rate': 4.010992691843829e-06, 'epoch': 2.14}\n",
      "{'loss': 3.0848, 'grad_norm': 215.17791748046875, 'learning_rate': 3.98338161752999e-06, 'epoch': 2.15}\n",
      "{'loss': 2.9155, 'grad_norm': 224.71377563476562, 'learning_rate': 3.955842251444978e-06, 'epoch': 2.15}\n",
      "{'loss': 2.3369, 'grad_norm': 159.93724060058594, 'learning_rate': 3.9283749218128885e-06, 'epoch': 2.15}\n",
      "{'loss': 3.039, 'grad_norm': 203.04489135742188, 'learning_rate': 3.900979955999271e-06, 'epoch': 2.16}\n",
      "{'loss': 3.0009, 'grad_norm': 263.4537658691406, 'learning_rate': 3.8736576805072165e-06, 'epoch': 2.16}\n",
      "{'loss': 3.5339, 'grad_norm': 127.3361587524414, 'learning_rate': 3.846408420973456e-06, 'epoch': 2.16}\n",
      "{'loss': 3.0545, 'grad_norm': 136.48434448242188, 'learning_rate': 3.819232502164499e-06, 'epoch': 2.17}\n",
      "{'loss': 3.2704, 'grad_norm': 147.41848754882812, 'learning_rate': 3.792130247972756e-06, 'epoch': 2.17}\n",
      "{'loss': 3.233, 'grad_norm': 122.13704681396484, 'learning_rate': 3.7651019814126656e-06, 'epoch': 2.17}\n",
      "{'loss': 3.2546, 'grad_norm': 122.26329040527344, 'learning_rate': 3.738148024616863e-06, 'epoch': 2.18}\n",
      "{'loss': 3.521, 'grad_norm': 139.8589324951172, 'learning_rate': 3.7112686988323353e-06, 'epoch': 2.18}\n",
      "{'loss': 3.3806, 'grad_norm': 142.46641540527344, 'learning_rate': 3.684464324416578e-06, 'epoch': 2.18}\n",
      "{'loss': 3.5299, 'grad_norm': 148.9589385986328, 'learning_rate': 3.6577352208338015e-06, 'epoch': 2.19}\n",
      "{'loss': 3.4555, 'grad_norm': 175.20645141601562, 'learning_rate': 3.6310817066511106e-06, 'epoch': 2.19}\n",
      "{'loss': 3.5734, 'grad_norm': 150.87619018554688, 'learning_rate': 3.604504099534696e-06, 'epoch': 2.19}\n",
      "{'loss': 3.977, 'grad_norm': 193.1738739013672, 'learning_rate': 3.578002716246074e-06, 'epoch': 2.2}\n",
      "{'loss': 3.5262, 'grad_norm': 182.48597717285156, 'learning_rate': 3.5515778726382967e-06, 'epoch': 2.2}\n",
      "{'loss': 3.4388, 'grad_norm': 159.42596435546875, 'learning_rate': 3.525229883652177e-06, 'epoch': 2.2}\n",
      "{'loss': 3.6092, 'grad_norm': 163.59400939941406, 'learning_rate': 3.4989590633125583e-06, 'epoch': 2.2}\n",
      "{'loss': 3.9703, 'grad_norm': 200.42320251464844, 'learning_rate': 3.4727657247245607e-06, 'epoch': 2.21}\n",
      "{'loss': 3.3816, 'grad_norm': 170.78640747070312, 'learning_rate': 3.446650180069837e-06, 'epoch': 2.21}\n",
      "{'loss': 3.3236, 'grad_norm': 173.16168212890625, 'learning_rate': 3.4206127406028744e-06, 'epoch': 2.21}\n",
      "{'loss': 3.3449, 'grad_norm': 199.6306915283203, 'learning_rate': 3.394653716647277e-06, 'epoch': 2.22}\n",
      "{'loss': 3.5271, 'grad_norm': 216.92727661132812, 'learning_rate': 3.3687734175920505e-06, 'epoch': 2.22}\n",
      "{'loss': 3.7276, 'grad_norm': 154.44821166992188, 'learning_rate': 3.342972151887941e-06, 'epoch': 2.22}\n",
      "{'loss': 3.1326, 'grad_norm': 158.8503875732422, 'learning_rate': 3.317250227043746e-06, 'epoch': 2.23}\n",
      "{'loss': 3.6829, 'grad_norm': 192.60470581054688, 'learning_rate': 3.2916079496226407e-06, 'epoch': 2.23}\n",
      "{'loss': 3.3217, 'grad_norm': 168.75531005859375, 'learning_rate': 3.266045625238539e-06, 'epoch': 2.23}\n",
      "{'loss': 3.2562, 'grad_norm': 179.4359130859375, 'learning_rate': 3.2405635585524566e-06, 'epoch': 2.24}\n",
      "{'loss': 3.5869, 'grad_norm': 174.8531494140625, 'learning_rate': 3.21516205326885e-06, 'epoch': 2.24}\n",
      "{'loss': 3.4875, 'grad_norm': 207.8541717529297, 'learning_rate': 3.1898414121320277e-06, 'epoch': 2.24}\n",
      "{'loss': 3.4433, 'grad_norm': 193.7562255859375, 'learning_rate': 3.1646019369225277e-06, 'epoch': 2.25}\n",
      "{'loss': 3.3045, 'grad_norm': 208.1424102783203, 'learning_rate': 3.1394439284535206e-06, 'epoch': 2.25}\n",
      "{'loss': 3.4028, 'grad_norm': 181.9207305908203, 'learning_rate': 3.114367686567228e-06, 'epoch': 2.25}\n",
      "{'loss': 3.2972, 'grad_norm': 176.0227508544922, 'learning_rate': 3.089373510131354e-06, 'epoch': 2.26}\n",
      "{'loss': 3.2306, 'grad_norm': 162.24691772460938, 'learning_rate': 3.064461697035506e-06, 'epoch': 2.26}\n",
      "{'loss': 3.15, 'grad_norm': 141.5066680908203, 'learning_rate': 3.0396325441876627e-06, 'epoch': 2.26}\n",
      "{'loss': 3.1063, 'grad_norm': 206.45643615722656, 'learning_rate': 3.0148863475106315e-06, 'epoch': 2.27}\n",
      "{'loss': 3.7284, 'grad_norm': 232.1022186279297, 'learning_rate': 2.9902234019385056e-06, 'epoch': 2.27}\n",
      "{'loss': 3.1642, 'grad_norm': 174.40933227539062, 'learning_rate': 2.9656440014131737e-06, 'epoch': 2.27}\n",
      "{'loss': 3.0499, 'grad_norm': 160.45814514160156, 'learning_rate': 2.941148438880803e-06, 'epoch': 2.28}\n",
      "{'loss': 3.1218, 'grad_norm': 165.2965545654297, 'learning_rate': 2.9167370062883403e-06, 'epoch': 2.28}\n",
      "{'loss': 3.266, 'grad_norm': 167.21826171875, 'learning_rate': 2.8924099945800533e-06, 'epoch': 2.28}\n",
      "{'loss': 3.2549, 'grad_norm': 201.20924377441406, 'learning_rate': 2.8681676936940397e-06, 'epoch': 2.28}\n",
      "{'loss': 3.2117, 'grad_norm': 179.62852478027344, 'learning_rate': 2.8440103925587904e-06, 'epoch': 2.29}\n",
      "{'loss': 2.9429, 'grad_norm': 167.54046630859375, 'learning_rate': 2.8199383790897405e-06, 'epoch': 2.29}\n",
      "{'loss': 3.2576, 'grad_norm': 168.68048095703125, 'learning_rate': 2.795951940185827e-06, 'epoch': 2.29}\n",
      "{'loss': 3.1846, 'grad_norm': 165.5995635986328, 'learning_rate': 2.7720513617260857e-06, 'epoch': 2.3}\n",
      "{'loss': 3.0249, 'grad_norm': 160.0858154296875, 'learning_rate': 2.748236928566238e-06, 'epoch': 2.3}\n",
      "{'loss': 3.2935, 'grad_norm': 233.26397705078125, 'learning_rate': 2.7245089245352864e-06, 'epoch': 2.3}\n",
      "{'loss': 2.8074, 'grad_norm': 236.0468292236328, 'learning_rate': 2.700867632432145e-06, 'epoch': 2.31}\n",
      "{'loss': 2.7764, 'grad_norm': 154.14859008789062, 'learning_rate': 2.6773133340222677e-06, 'epoch': 2.31}\n",
      "{'loss': 2.8255, 'grad_norm': 217.98098754882812, 'learning_rate': 2.6538463100342773e-06, 'epoch': 2.31}\n",
      "{'loss': 2.4357, 'grad_norm': 183.581298828125, 'learning_rate': 2.6304668401566334e-06, 'epoch': 2.32}\n",
      "{'loss': 3.1277, 'grad_norm': 293.8442077636719, 'learning_rate': 2.607175203034299e-06, 'epoch': 2.32}\n",
      "{'loss': 3.3843, 'grad_norm': 116.36361694335938, 'learning_rate': 2.5839716762654e-06, 'epoch': 2.32}\n",
      "{'loss': 3.1363, 'grad_norm': 105.07633209228516, 'learning_rate': 2.56085653639795e-06, 'epoch': 2.33}\n",
      "{'loss': 3.1817, 'grad_norm': 117.49729919433594, 'learning_rate': 2.5378300589265258e-06, 'epoch': 2.33}\n",
      "{'loss': 3.1931, 'grad_norm': 113.31333923339844, 'learning_rate': 2.514892518288988e-06, 'epoch': 2.33}\n",
      "{'loss': 3.3473, 'grad_norm': 134.85873413085938, 'learning_rate': 2.4920441878632273e-06, 'epoch': 2.34}\n",
      "{'loss': 3.3296, 'grad_norm': 116.94224548339844, 'learning_rate': 2.469285339963892e-06, 'epoch': 2.34}\n",
      "{'loss': 3.4683, 'grad_norm': 126.49826049804688, 'learning_rate': 2.4466162458391364e-06, 'epoch': 2.34}\n",
      "{'loss': 3.6215, 'grad_norm': 140.31781005859375, 'learning_rate': 2.4240371756674063e-06, 'epoch': 2.35}\n",
      "{'loss': 3.85, 'grad_norm': 137.61363220214844, 'learning_rate': 2.401548398554213e-06, 'epoch': 2.35}\n",
      "{'loss': 3.6708, 'grad_norm': 161.03607177734375, 'learning_rate': 2.379150182528909e-06, 'epoch': 2.35}\n",
      "{'loss': 3.5765, 'grad_norm': 149.43553161621094, 'learning_rate': 2.3568427945415163e-06, 'epoch': 2.36}\n",
      "{'loss': 3.7085, 'grad_norm': 132.87484741210938, 'learning_rate': 2.334626500459539e-06, 'epoch': 2.36}\n",
      "{'loss': 3.751, 'grad_norm': 155.69635009765625, 'learning_rate': 2.3125015650647798e-06, 'epoch': 2.36}\n",
      "{'loss': 3.3384, 'grad_norm': 194.07644653320312, 'learning_rate': 2.290468252050204e-06, 'epoch': 2.36}\n",
      "{'loss': 3.6172, 'grad_norm': 184.99256896972656, 'learning_rate': 2.26852682401679e-06, 'epoch': 2.37}\n",
      "{'loss': 3.9211, 'grad_norm': 173.6001739501953, 'learning_rate': 2.246677542470388e-06, 'epoch': 2.37}\n",
      "{'loss': 3.2253, 'grad_norm': 151.7870635986328, 'learning_rate': 2.224920667818622e-06, 'epoch': 2.37}\n",
      "{'loss': 3.7603, 'grad_norm': 165.38555908203125, 'learning_rate': 2.2032564593677773e-06, 'epoch': 2.38}\n",
      "{'loss': 3.7357, 'grad_norm': 200.7650146484375, 'learning_rate': 2.1816851753197023e-06, 'epoch': 2.38}\n",
      "{'loss': 3.214, 'grad_norm': 168.95140075683594, 'learning_rate': 2.1602070727687463e-06, 'epoch': 2.38}\n",
      "{'loss': 3.4541, 'grad_norm': 162.84158325195312, 'learning_rate': 2.1388224076986872e-06, 'epoch': 2.39}\n",
      "{'loss': 3.3448, 'grad_norm': 188.0185546875, 'learning_rate': 2.117531434979675e-06, 'epoch': 2.39}\n",
      "{'loss': 3.2523, 'grad_norm': 142.59625244140625, 'learning_rate': 2.096334408365207e-06, 'epoch': 2.39}\n",
      "{'loss': 3.5529, 'grad_norm': 170.29469299316406, 'learning_rate': 2.075231580489098e-06, 'epoch': 2.4}\n",
      "{'loss': 3.3214, 'grad_norm': 146.7338104248047, 'learning_rate': 2.0542232028624585e-06, 'epoch': 2.4}\n",
      "{'loss': 3.2148, 'grad_norm': 155.45001220703125, 'learning_rate': 2.033309525870717e-06, 'epoch': 2.4}\n",
      "{'loss': 3.1368, 'grad_norm': 146.64935302734375, 'learning_rate': 2.0124907987706243e-06, 'epoch': 2.41}\n",
      "{'loss': 3.3788, 'grad_norm': 151.6657257080078, 'learning_rate': 1.991767269687278e-06, 'epoch': 2.41}\n",
      "{'loss': 3.4316, 'grad_norm': 182.41322326660156, 'learning_rate': 1.971139185611176e-06, 'epoch': 2.41}\n",
      "{'loss': 3.065, 'grad_norm': 175.66441345214844, 'learning_rate': 1.9506067923952676e-06, 'epoch': 2.42}\n",
      "{'loss': 3.1721, 'grad_norm': 190.6668243408203, 'learning_rate': 1.930170334752025e-06, 'epoch': 2.42}\n",
      "{'loss': 3.61, 'grad_norm': 183.56573486328125, 'learning_rate': 1.9098300562505266e-06, 'epoch': 2.42}\n",
      "{'loss': 3.3583, 'grad_norm': 184.9543914794922, 'learning_rate': 1.8895861993135444e-06, 'epoch': 2.43}\n",
      "{'loss': 3.4348, 'grad_norm': 158.57553100585938, 'learning_rate': 1.8694390052146737e-06, 'epoch': 2.43}\n",
      "{'loss': 3.3023, 'grad_norm': 179.77467346191406, 'learning_rate': 1.8493887140754462e-06, 'epoch': 2.43}\n",
      "{'loss': 3.4036, 'grad_norm': 175.69400024414062, 'learning_rate': 1.8294355648624607e-06, 'epoch': 2.44}\n",
      "{'loss': 3.1708, 'grad_norm': 150.00146484375, 'learning_rate': 1.8095797953845507e-06, 'epoch': 2.44}\n",
      "{'loss': 3.3579, 'grad_norm': 157.2950439453125, 'learning_rate': 1.789821642289945e-06, 'epoch': 2.44}\n",
      "{'loss': 3.5535, 'grad_norm': 158.3314208984375, 'learning_rate': 1.7701613410634367e-06, 'epoch': 2.44}\n",
      "{'loss': 3.3496, 'grad_norm': 144.32896423339844, 'learning_rate': 1.750599126023591e-06, 'epoch': 2.45}\n",
      "{'loss': 3.5064, 'grad_norm': 160.57455444335938, 'learning_rate': 1.731135230319948e-06, 'epoch': 2.45}\n",
      "{'loss': 3.2376, 'grad_norm': 171.5819854736328, 'learning_rate': 1.7117698859302357e-06, 'epoch': 2.45}\n",
      "{'loss': 3.1413, 'grad_norm': 167.36305236816406, 'learning_rate': 1.692503323657617e-06, 'epoch': 2.46}\n",
      "{'loss': 3.345, 'grad_norm': 172.0248565673828, 'learning_rate': 1.6733357731279375e-06, 'epoch': 2.46}\n",
      "{'loss': 2.6949, 'grad_norm': 156.54551696777344, 'learning_rate': 1.6542674627869738e-06, 'epoch': 2.46}\n",
      "{'loss': 2.5906, 'grad_norm': 216.55918884277344, 'learning_rate': 1.6352986198977327e-06, 'epoch': 2.47}\n",
      "{'loss': 3.058, 'grad_norm': 186.5666046142578, 'learning_rate': 1.6164294705377292e-06, 'epoch': 2.47}\n",
      "{'loss': 2.6844, 'grad_norm': 173.0972442626953, 'learning_rate': 1.5976602395962892e-06, 'epoch': 2.47}\n",
      "{'loss': 3.022, 'grad_norm': 203.2823944091797, 'learning_rate': 1.5789911507718824e-06, 'epoch': 2.48}\n",
      "{'loss': 3.2369, 'grad_norm': 190.50613403320312, 'learning_rate': 1.560422426569449e-06, 'epoch': 2.48}\n",
      "{'loss': 2.9706, 'grad_norm': 97.04561614990234, 'learning_rate': 1.5419542882977367e-06, 'epoch': 2.48}\n",
      "{'loss': 3.3749, 'grad_norm': 110.53512573242188, 'learning_rate': 1.523586956066686e-06, 'epoch': 2.49}\n",
      "{'loss': 3.1569, 'grad_norm': 120.66654968261719, 'learning_rate': 1.5053206487847916e-06, 'epoch': 2.49}\n",
      "{'loss': 2.8758, 'grad_norm': 123.24276733398438, 'learning_rate': 1.4871555841564889e-06, 'epoch': 2.49}\n",
      "{'loss': 3.395, 'grad_norm': 113.33482360839844, 'learning_rate': 1.4690919786795766e-06, 'epoch': 2.5}\n",
      "{'loss': 3.4342, 'grad_norm': 105.96446990966797, 'learning_rate': 1.4511300476426227e-06, 'epoch': 2.5}\n",
      "{'loss': 3.6158, 'grad_norm': 152.14431762695312, 'learning_rate': 1.433270005122399e-06, 'epoch': 2.5}\n",
      "{'loss': 3.5227, 'grad_norm': 155.73193359375, 'learning_rate': 1.4155120639813392e-06, 'epoch': 2.51}\n",
      "{'loss': 3.6254, 'grad_norm': 121.5849609375, 'learning_rate': 1.3978564358649926e-06, 'epoch': 2.51}\n",
      "{'loss': 3.5566, 'grad_norm': 166.86656188964844, 'learning_rate': 1.3803033311995072e-06, 'epoch': 2.51}\n",
      "{'loss': 3.6333, 'grad_norm': 159.53436279296875, 'learning_rate': 1.3628529591891181e-06, 'epoch': 2.52}\n",
      "{'loss': 3.4511, 'grad_norm': 147.88922119140625, 'learning_rate': 1.345505527813652e-06, 'epoch': 2.52}\n",
      "{'loss': 3.7027, 'grad_norm': 186.3892822265625, 'learning_rate': 1.3282612438260578e-06, 'epoch': 2.52}\n",
      "{'loss': 3.6681, 'grad_norm': 151.06881713867188, 'learning_rate': 1.311120312749935e-06, 'epoch': 2.52}\n",
      "{'loss': 3.7859, 'grad_norm': 161.1415252685547, 'learning_rate': 1.2940829388770837e-06, 'epoch': 2.53}\n",
      "{'loss': 3.554, 'grad_norm': 134.33993530273438, 'learning_rate': 1.2771493252650723e-06, 'epoch': 2.53}\n",
      "{'loss': 3.606, 'grad_norm': 175.21449279785156, 'learning_rate': 1.2603196737348211e-06, 'epoch': 2.53}\n",
      "{'loss': 3.663, 'grad_norm': 169.64051818847656, 'learning_rate': 1.2435941848681864e-06, 'epoch': 2.54}\n",
      "{'loss': 3.6263, 'grad_norm': 185.65457153320312, 'learning_rate': 1.2269730580055806e-06, 'epoch': 2.54}\n",
      "{'loss': 3.5832, 'grad_norm': 160.08294677734375, 'learning_rate': 1.2104564912435924e-06, 'epoch': 2.54}\n",
      "{'loss': 3.159, 'grad_norm': 186.35614013671875, 'learning_rate': 1.19404468143262e-06, 'epoch': 2.55}\n",
      "{'loss': 3.1728, 'grad_norm': 151.6230010986328, 'learning_rate': 1.1777378241745385e-06, 'epoch': 2.55}\n",
      "{'loss': 3.617, 'grad_norm': 180.9880828857422, 'learning_rate': 1.1615361138203574e-06, 'epoch': 2.55}\n",
      "{'loss': 3.4034, 'grad_norm': 156.1156463623047, 'learning_rate': 1.1454397434679022e-06, 'epoch': 2.56}\n",
      "{'loss': 3.5089, 'grad_norm': 148.11431884765625, 'learning_rate': 1.1294489049595247e-06, 'epoch': 2.56}\n",
      "{'loss': 3.2211, 'grad_norm': 157.55772399902344, 'learning_rate': 1.1135637888798101e-06, 'epoch': 2.56}\n",
      "{'loss': 3.4455, 'grad_norm': 156.17567443847656, 'learning_rate': 1.0977845845533009e-06, 'epoch': 2.57}\n",
      "{'loss': 3.6079, 'grad_norm': 191.2084197998047, 'learning_rate': 1.0821114800422482e-06, 'epoch': 2.57}\n",
      "{'loss': 3.4698, 'grad_norm': 162.01463317871094, 'learning_rate': 1.066544662144371e-06, 'epoch': 2.57}\n",
      "{'loss': 3.503, 'grad_norm': 178.7726593017578, 'learning_rate': 1.0510843163906148e-06, 'epoch': 2.58}\n",
      "{'loss': 3.6305, 'grad_norm': 172.5583038330078, 'learning_rate': 1.0357306270429623e-06, 'epoch': 2.58}\n",
      "{'loss': 3.6004, 'grad_norm': 196.84182739257812, 'learning_rate': 1.020483777092226e-06, 'epoch': 2.58}\n",
      "{'loss': 3.4308, 'grad_norm': 177.5296173095703, 'learning_rate': 1.0053439482558602e-06, 'epoch': 2.59}\n",
      "{'loss': 3.2847, 'grad_norm': 191.3316192626953, 'learning_rate': 9.903113209758098e-07, 'epoch': 2.59}\n",
      "{'loss': 3.1821, 'grad_norm': 140.72897338867188, 'learning_rate': 9.753860744163524e-07, 'epoch': 2.59}\n",
      "{'loss': 3.6034, 'grad_norm': 147.08815002441406, 'learning_rate': 9.605683864619574e-07, 'epoch': 2.6}\n",
      "{'loss': 3.1269, 'grad_norm': 191.79771423339844, 'learning_rate': 9.458584337151811e-07, 'epoch': 2.6}\n",
      "{'loss': 3.4833, 'grad_norm': 174.71055603027344, 'learning_rate': 9.312563914945461e-07, 'epoch': 2.6}\n",
      "{'loss': 3.2741, 'grad_norm': 202.49143981933594, 'learning_rate': 9.167624338324599e-07, 'epoch': 2.6}\n",
      "{'loss': 3.6547, 'grad_norm': 213.37718200683594, 'learning_rate': 9.023767334731426e-07, 'epoch': 2.61}\n",
      "{'loss': 3.0966, 'grad_norm': 135.2401580810547, 'learning_rate': 8.880994618705574e-07, 'epoch': 2.61}\n",
      "{'loss': 3.3413, 'grad_norm': 237.2415008544922, 'learning_rate': 8.739307891863813e-07, 'epoch': 2.61}\n",
      "{'loss': 2.9066, 'grad_norm': 189.54150390625, 'learning_rate': 8.598708842879688e-07, 'epoch': 2.62}\n",
      "{'loss': 2.9362, 'grad_norm': 163.26255798339844, 'learning_rate': 8.459199147463371e-07, 'epoch': 2.62}\n",
      "{'loss': 3.3061, 'grad_norm': 190.08010864257812, 'learning_rate': 8.320780468341761e-07, 'epoch': 2.62}\n",
      "{'loss': 2.5505, 'grad_norm': 161.37803649902344, 'learning_rate': 8.183454455238638e-07, 'epoch': 2.63}\n",
      "{'loss': 3.0163, 'grad_norm': 196.0048828125, 'learning_rate': 8.047222744854943e-07, 'epoch': 2.63}\n",
      "{'loss': 2.7563, 'grad_norm': 193.82730102539062, 'learning_rate': 7.912086960849374e-07, 'epoch': 2.63}\n",
      "{'loss': 3.0909, 'grad_norm': 242.37232971191406, 'learning_rate': 7.778048713818975e-07, 'epoch': 2.64}\n",
      "{'loss': 2.9646, 'grad_norm': 282.48602294921875, 'learning_rate': 7.645109601279921e-07, 'epoch': 2.64}\n",
      "{'loss': 3.0795, 'grad_norm': 107.57283782958984, 'learning_rate': 7.513271207648531e-07, 'epoch': 2.64}\n",
      "{'loss': 3.2516, 'grad_norm': 124.5242919921875, 'learning_rate': 7.382535104222366e-07, 'epoch': 2.65}\n",
      "{'loss': 3.2873, 'grad_norm': 114.07148742675781, 'learning_rate': 7.252902849161436e-07, 'epoch': 2.65}\n",
      "{'loss': 3.2947, 'grad_norm': 127.52951049804688, 'learning_rate': 7.124375987469767e-07, 'epoch': 2.65}\n",
      "{'loss': 3.2002, 'grad_norm': 106.63201141357422, 'learning_rate': 6.996956050976878e-07, 'epoch': 2.66}\n",
      "{'loss': 3.3538, 'grad_norm': 110.71678924560547, 'learning_rate': 6.870644558319528e-07, 'epoch': 2.66}\n",
      "{'loss': 3.4585, 'grad_norm': 117.30211639404297, 'learning_rate': 6.745443014923658e-07, 'epoch': 2.66}\n",
      "{'loss': 3.6848, 'grad_norm': 145.78564453125, 'learning_rate': 6.621352912986468e-07, 'epoch': 2.67}\n",
      "{'loss': 3.6922, 'grad_norm': 176.73497009277344, 'learning_rate': 6.498375731458529e-07, 'epoch': 2.67}\n",
      "{'loss': 3.5959, 'grad_norm': 212.35598754882812, 'learning_rate': 6.37651293602628e-07, 'epoch': 2.67}\n",
      "{'loss': 3.6839, 'grad_norm': 140.2598876953125, 'learning_rate': 6.255765979094519e-07, 'epoch': 2.68}\n",
      "{'loss': 3.6877, 'grad_norm': 145.03494262695312, 'learning_rate': 6.136136299768991e-07, 'epoch': 2.68}\n",
      "{'loss': 3.6426, 'grad_norm': 157.21759033203125, 'learning_rate': 6.017625323839415e-07, 'epoch': 2.68}\n",
      "{'loss': 3.4398, 'grad_norm': 169.4998016357422, 'learning_rate': 5.900234463762367e-07, 'epoch': 2.68}\n",
      "{'loss': 3.7785, 'grad_norm': 154.02139282226562, 'learning_rate': 5.783965118644441e-07, 'epoch': 2.69}\n",
      "{'loss': 3.2964, 'grad_norm': 155.19798278808594, 'learning_rate': 5.668818674225684e-07, 'epoch': 2.69}\n",
      "{'loss': 3.3301, 'grad_norm': 179.57733154296875, 'learning_rate': 5.554796502862958e-07, 'epoch': 2.69}\n",
      "{'loss': 3.8356, 'grad_norm': 168.48602294921875, 'learning_rate': 5.441899963513631e-07, 'epoch': 2.7}\n",
      "{'loss': 3.3952, 'grad_norm': 164.29598999023438, 'learning_rate': 5.330130401719413e-07, 'epoch': 2.7}\n",
      "{'loss': 3.2546, 'grad_norm': 194.31369018554688, 'learning_rate': 5.219489149590251e-07, 'epoch': 2.7}\n",
      "{'loss': 3.2457, 'grad_norm': 155.62889099121094, 'learning_rate': 5.109977525788512e-07, 'epoch': 2.71}\n",
      "{'loss': 3.0778, 'grad_norm': 180.29469299316406, 'learning_rate': 5.001596835513256e-07, 'epoch': 2.71}\n",
      "{'loss': 3.5394, 'grad_norm': 181.73841857910156, 'learning_rate': 4.894348370484648e-07, 'epoch': 2.71}\n",
      "{'loss': 3.2807, 'grad_norm': 183.67974853515625, 'learning_rate': 4.788233408928588e-07, 'epoch': 2.72}\n",
      "{'loss': 3.749, 'grad_norm': 157.52413940429688, 'learning_rate': 4.6832532155614895e-07, 'epoch': 2.72}\n",
      "{'loss': 3.3922, 'grad_norm': 141.0074005126953, 'learning_rate': 4.5794090415751666e-07, 'epoch': 2.72}\n",
      "{'loss': 3.2954, 'grad_norm': 141.5591278076172, 'learning_rate': 4.4767021246219566e-07, 'epoch': 2.73}\n",
      "{'loss': 3.5041, 'grad_norm': 148.4921417236328, 'learning_rate': 4.3751336887999597e-07, 'epoch': 2.73}\n",
      "{'loss': 3.4048, 'grad_norm': 170.1424102783203, 'learning_rate': 4.27470494463843e-07, 'epoch': 2.73}\n",
      "{'loss': 3.7952, 'grad_norm': 220.0626983642578, 'learning_rate': 4.1754170890833777e-07, 'epoch': 2.74}\n",
      "{'loss': 3.5151, 'grad_norm': 154.43165588378906, 'learning_rate': 4.077271305483321e-07, 'epoch': 2.74}\n",
      "{'loss': 3.3799, 'grad_norm': 164.85084533691406, 'learning_rate': 3.980268763575079e-07, 'epoch': 2.74}\n",
      "{'loss': 3.4499, 'grad_norm': 182.76856994628906, 'learning_rate': 3.8844106194699696e-07, 'epoch': 2.75}\n",
      "{'loss': 3.3516, 'grad_norm': 161.34242248535156, 'learning_rate': 3.7896980156399533e-07, 'epoch': 2.75}\n",
      "{'loss': 3.1113, 'grad_norm': 193.349853515625, 'learning_rate': 3.6961320809039914e-07, 'epoch': 2.75}\n",
      "{'loss': 3.4263, 'grad_norm': 178.600341796875, 'learning_rate': 3.603713930414676e-07, 'epoch': 2.76}\n",
      "{'loss': 3.6026, 'grad_norm': 194.48269653320312, 'learning_rate': 3.5124446656448654e-07, 'epoch': 2.76}\n",
      "{'loss': 3.0199, 'grad_norm': 198.34718322753906, 'learning_rate': 3.42232537437458e-07, 'epoch': 2.76}\n",
      "{'loss': 3.1865, 'grad_norm': 160.73681640625, 'learning_rate': 3.33335713067805e-07, 'epoch': 2.76}\n",
      "{'loss': 3.1748, 'grad_norm': 176.5546417236328, 'learning_rate': 3.245540994910934e-07, 'epoch': 2.77}\n",
      "{'loss': 3.1469, 'grad_norm': 139.51373291015625, 'learning_rate': 3.158878013697586e-07, 'epoch': 2.77}\n",
      "{'loss': 3.0119, 'grad_norm': 206.28489685058594, 'learning_rate': 3.073369219918698e-07, 'epoch': 2.77}\n",
      "{'loss': 3.1647, 'grad_norm': 221.73211669921875, 'learning_rate': 2.989015632698944e-07, 'epoch': 2.78}\n",
      "{'loss': 3.4075, 'grad_norm': 156.66856384277344, 'learning_rate': 2.905818257394799e-07, 'epoch': 2.78}\n",
      "{'loss': 2.7619, 'grad_norm': 153.529541015625, 'learning_rate': 2.8237780855825957e-07, 'epoch': 2.78}\n",
      "{'loss': 3.1953, 'grad_norm': 152.48806762695312, 'learning_rate': 2.742896095046732e-07, 'epoch': 2.79}\n",
      "{'loss': 3.1483, 'grad_norm': 225.62071228027344, 'learning_rate': 2.6631732497679363e-07, 'epoch': 2.79}\n",
      "{'loss': 3.1569, 'grad_norm': 181.6493377685547, 'learning_rate': 2.584610499911833e-07, 'epoch': 2.79}\n",
      "{'loss': 2.8866, 'grad_norm': 251.1969757080078, 'learning_rate': 2.507208781817638e-07, 'epoch': 2.8}\n",
      "{'loss': 3.2946, 'grad_norm': 246.34408569335938, 'learning_rate': 2.4309690179869503e-07, 'epoch': 2.8}\n",
      "{'loss': 3.3932, 'grad_norm': 111.19287872314453, 'learning_rate': 2.355892117072789e-07, 'epoch': 2.8}\n",
      "{'loss': 3.1046, 'grad_norm': 123.27452850341797, 'learning_rate': 2.2819789738687482e-07, 'epoch': 2.81}\n",
      "{'loss': 3.0262, 'grad_norm': 94.71764373779297, 'learning_rate': 2.2092304692983402e-07, 'epoch': 2.81}\n",
      "{'loss': 3.0507, 'grad_norm': 113.79676055908203, 'learning_rate': 2.1376474704044693e-07, 'epoch': 2.81}\n",
      "{'loss': 3.2152, 'grad_norm': 123.21540069580078, 'learning_rate': 2.067230830339184e-07, 'epoch': 2.82}\n",
      "{'loss': 3.2375, 'grad_norm': 150.63192749023438, 'learning_rate': 1.9979813883533762e-07, 'epoch': 2.82}\n",
      "{'loss': 3.4462, 'grad_norm': 115.79449462890625, 'learning_rate': 1.929899969786897e-07, 'epoch': 2.82}\n",
      "{'loss': 3.5152, 'grad_norm': 120.9515151977539, 'learning_rate': 1.8629873860586567e-07, 'epoch': 2.83}\n",
      "{'loss': 3.4063, 'grad_norm': 136.05638122558594, 'learning_rate': 1.7972444346569752e-07, 'epoch': 2.83}\n",
      "{'loss': 3.6342, 'grad_norm': 146.0439453125, 'learning_rate': 1.7326718991300563e-07, 'epoch': 2.83}\n",
      "{'loss': 3.6001, 'grad_norm': 156.40548706054688, 'learning_rate': 1.6692705490766958e-07, 'epoch': 2.84}\n",
      "{'loss': 3.7109, 'grad_norm': 158.04042053222656, 'learning_rate': 1.6070411401370335e-07, 'epoch': 2.84}\n",
      "{'loss': 3.6355, 'grad_norm': 178.60296630859375, 'learning_rate': 1.5459844139836476e-07, 'epoch': 2.84}\n",
      "{'loss': 3.3076, 'grad_norm': 153.9482421875, 'learning_rate': 1.4861010983126202e-07, 'epoch': 2.84}\n",
      "{'loss': 3.4372, 'grad_norm': 134.20834350585938, 'learning_rate': 1.4273919068349184e-07, 'epoch': 2.85}\n",
      "{'loss': 3.618, 'grad_norm': 157.82232666015625, 'learning_rate': 1.3698575392678492e-07, 'epoch': 2.85}\n",
      "{'loss': 3.7559, 'grad_norm': 124.70575714111328, 'learning_rate': 1.3134986813267968e-07, 'epoch': 2.85}\n",
      "{'loss': 3.3627, 'grad_norm': 166.40870666503906, 'learning_rate': 1.258316004716953e-07, 'epoch': 2.86}\n",
      "{'loss': 3.4432, 'grad_norm': 149.03555297851562, 'learning_rate': 1.2043101671253553e-07, 'epoch': 2.86}\n",
      "{'loss': 3.3057, 'grad_norm': 134.8327178955078, 'learning_rate': 1.1514818122130844e-07, 'epoch': 2.86}\n",
      "{'loss': 3.4564, 'grad_norm': 148.934814453125, 'learning_rate': 1.0998315696075123e-07, 'epoch': 2.87}\n",
      "{'loss': 3.395, 'grad_norm': 193.41429138183594, 'learning_rate': 1.0493600548948879e-07, 'epoch': 2.87}\n",
      "{'loss': 3.2511, 'grad_norm': 151.963623046875, 'learning_rate': 1.0000678696129307e-07, 'epoch': 2.87}\n",
      "{'loss': 3.1594, 'grad_norm': 174.82603454589844, 'learning_rate': 9.519556012436815e-08, 'epoch': 2.88}\n",
      "{'loss': 3.3023, 'grad_norm': 159.4786376953125, 'learning_rate': 9.0502382320653e-08, 'epoch': 2.88}\n",
      "{'loss': 3.6121, 'grad_norm': 145.20167541503906, 'learning_rate': 8.592730948513205e-08, 'epoch': 2.88}\n",
      "{'loss': 3.1645, 'grad_norm': 139.3633270263672, 'learning_rate': 8.147039614517571e-08, 'epoch': 2.89}\n",
      "{'loss': 3.2119, 'grad_norm': 151.7472381591797, 'learning_rate': 7.71316954198853e-08, 'epoch': 2.89}\n",
      "{'loss': 3.4016, 'grad_norm': 135.70191955566406, 'learning_rate': 7.291125901946027e-08, 'epoch': 2.89}\n",
      "{'loss': 3.4454, 'grad_norm': 159.80062866210938, 'learning_rate': 6.880913724458538e-08, 'epoch': 2.9}\n",
      "{'loss': 3.1237, 'grad_norm': 152.7572784423828, 'learning_rate': 6.482537898582886e-08, 'epoch': 2.9}\n",
      "{'loss': 3.1917, 'grad_norm': 164.93370056152344, 'learning_rate': 6.096003172305742e-08, 'epoch': 2.9}\n",
      "{'loss': 3.1782, 'grad_norm': 148.6466064453125, 'learning_rate': 5.721314152487556e-08, 'epoch': 2.91}\n",
      "{'loss': 3.4427, 'grad_norm': 160.2681121826172, 'learning_rate': 5.3584753048073756e-08, 'epoch': 2.91}\n",
      "{'loss': 3.4299, 'grad_norm': 159.94041442871094, 'learning_rate': 5.007490953709227e-08, 'epoch': 2.91}\n",
      "{'loss': 3.4656, 'grad_norm': 138.4359588623047, 'learning_rate': 4.6683652823513725e-08, 'epoch': 2.92}\n",
      "{'loss': 3.3699, 'grad_norm': 190.84381103515625, 'learning_rate': 4.3411023325560245e-08, 'epoch': 2.92}\n",
      "{'loss': 3.1213, 'grad_norm': 197.93496704101562, 'learning_rate': 4.025706004760932e-08, 'epoch': 2.92}\n",
      "{'loss': 3.3066, 'grad_norm': 202.4763641357422, 'learning_rate': 3.7221800579735346e-08, 'epoch': 2.92}\n",
      "{'loss': 3.2843, 'grad_norm': 179.9412841796875, 'learning_rate': 3.430528109725439e-08, 'epoch': 2.93}\n",
      "{'loss': 3.1561, 'grad_norm': 179.16500854492188, 'learning_rate': 3.150753636029902e-08, 'epoch': 2.93}\n",
      "{'loss': 3.309, 'grad_norm': 199.96902465820312, 'learning_rate': 2.8828599713398575e-08, 'epoch': 2.93}\n",
      "{'loss': 3.0425, 'grad_norm': 199.9078826904297, 'learning_rate': 2.6268503085089547e-08, 'epoch': 2.94}\n",
      "{'loss': 3.2522, 'grad_norm': 170.4645233154297, 'learning_rate': 2.3827276987524738e-08, 'epoch': 2.94}\n",
      "{'loss': 2.7412, 'grad_norm': 163.8351593017578, 'learning_rate': 2.1504950516118007e-08, 'epoch': 2.94}\n",
      "{'loss': 2.8811, 'grad_norm': 196.56536865234375, 'learning_rate': 1.9301551349195648e-08, 'epoch': 2.95}\n",
      "{'loss': 2.7416, 'grad_norm': 174.85775756835938, 'learning_rate': 1.721710574766333e-08, 'epoch': 2.95}\n",
      "{'loss': 2.6885, 'grad_norm': 198.83441162109375, 'learning_rate': 1.5251638554694137e-08, 'epoch': 2.95}\n",
      "{'loss': 3.2719, 'grad_norm': 235.3522491455078, 'learning_rate': 1.340517319543877e-08, 'epoch': 2.96}\n",
      "{'loss': 3.1478, 'grad_norm': 252.21090698242188, 'learning_rate': 1.1677731676733584e-08, 'epoch': 2.96}\n",
      "{'loss': 3.1634, 'grad_norm': 101.0271987915039, 'learning_rate': 1.0069334586854106e-08, 'epoch': 2.96}\n",
      "{'loss': 3.2747, 'grad_norm': 120.38616180419922, 'learning_rate': 8.580001095253032e-09, 'epoch': 2.97}\n",
      "{'loss': 3.837, 'grad_norm': 123.69622039794922, 'learning_rate': 7.209748952347051e-09, 'epoch': 2.97}\n",
      "{'loss': 3.7408, 'grad_norm': 149.98326110839844, 'learning_rate': 5.958594489295921e-09, 'epoch': 2.97}\n",
      "{'loss': 2.9625, 'grad_norm': 150.41212463378906, 'learning_rate': 4.826552617807067e-09, 'epoch': 2.98}\n",
      "{'loss': 3.3235, 'grad_norm': 149.09449768066406, 'learning_rate': 3.8136368299668266e-09, 'epoch': 2.98}\n",
      "{'loss': 2.9646, 'grad_norm': 166.40284729003906, 'learning_rate': 2.9198591980705847e-09, 'epoch': 2.98}\n",
      "{'loss': 3.5449, 'grad_norm': 162.88279724121094, 'learning_rate': 2.145230374481777e-09, 'epoch': 2.99}\n",
      "{'loss': 3.7096, 'grad_norm': 139.9879913330078, 'learning_rate': 1.4897595915053242e-09, 'epoch': 2.99}\n",
      "{'loss': 3.2416, 'grad_norm': 195.13369750976562, 'learning_rate': 9.534546612810502e-10, 'epoch': 2.99}\n",
      "{'loss': 3.5715, 'grad_norm': 177.4425811767578, 'learning_rate': 5.363219756837624e-10, 'epoch': 3.0}\n",
      "{'loss': 2.4321, 'grad_norm': 176.15185546875, 'learning_rate': 2.3836650624997627e-10, 'epoch': 3.0}\n",
      "{'loss': 2.9552, 'grad_norm': 260.69464111328125, 'learning_rate': 5.959180412129506e-11, 'epoch': 3.0}\n",
      "{'train_runtime': 286.313, 'train_samples_per_second': 104.78, 'train_steps_per_second': 3.28, 'train_loss': 3.568755222204775, 'epoch': 3.0}\n",
      "100% 939/939 [04:44<00:00,  3.30it/s]\n",
      "*** Trainer State & Trained Model Saved To --> res/pythia-70m_cirriculum_full/output/ ***\n",
      "*** Trainer State & Trained Model Save-Pretrained To --> res/pythia-70m_cirriculum_full/output//pretrained ***\n",
      "*** Training Done!\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mcelestial-sunset-10\u001b[0m at: \u001b[34m\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251129_014918-1gbsfdew/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train.py --config_file configs/full/pythia-70m_cirriculum_full.yml --wandb_key \"0944191bcf43ea6231189f995e76d66cc523c13d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467886c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls res/pythia-70m_cirriculum_full/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9868441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_distributed_trajectories.py --model_path res/pythia-70m_cirriculum_full/output --config_file configs/full/pythia-70m_cirriculum_full.yml --checkpoints all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac3eb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch train.py --config_file configs/s2l/pythia-410m_cirriculum_s2l.yml --wandb_key \"0944191bcf43ea6231189f995e76d66cc523c13d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d13935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-29 01:59:03.523089: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 01:59:03.540483: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764381543.561521   10762 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764381543.567942   10762 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764381543.584288   10762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764381543.584312   10762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764381543.584315   10762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764381543.584318   10762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-29 01:59:03.589058: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhenryliu999\u001b[0m (\u001b[33mhenryliu999-other\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Configuration loaded!\n",
      "full_data_path: TIGER-Lab/MathInstruct\n",
      "model_name_or_path: EleutherAI/pythia-410m-deduped\n",
      "cache_dir: ./cache\n",
      "model_max_length: 512\n",
      "schedule_name: Full\n",
      "result_dir_name: pythia-410m_cirriculum_full\n",
      "train_args:\n",
      "  optim: adamw_torch\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 4\n",
      "  per_device_eval_batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_strategy: 'no'\n",
      "  save_strategy: steps\n",
      "  save_steps: 50\n",
      "  save_total_limit: 12\n",
      "  learning_rate: 2.0e-05\n",
      "  weight_decay: 0.0\n",
      "  warmup_ratio: 0.03\n",
      "  lr_scheduler_type: cosine\n",
      "  logging_steps: 1\n",
      "  bf16: true\n",
      "  tf32: true\n",
      "  group_by_length: true\n",
      "  full_determinism: true\n",
      "  seed: 42\n",
      "ref_model_path: null\n",
      "n_components: -1\n",
      "num_loss_ckpts: -1\n",
      "distance: euclidean\n",
      "seed: 42\n",
      "\n",
      "config.json: 100% 570/570 [00:00<00:00, 5.22MB/s]\n",
      "model.safetensors: 100% 911M/911M [00:02<00:00, 305MB/s]     \n",
      "*** Model initialized!\n",
      "tokenizer_config.json: 100% 396/396 [00:00<00:00, 4.11MB/s]\n",
      "tokenizer.json: 2.11MB [00:00, 149MB/s]\n",
      "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 959kB/s]\n",
      "*** Tokenizer initialized!\n",
      "*** Smart tokenizer and embedding resize done!\n",
      "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n",
      "*** Schedule built!\n",
      "*** labeled_idx: tensor([   0,    1,    2,  ..., 9997, 9998, 9999])\n",
      "*** jdump(labeled_data_json_format, labeled_data_path) SUCESSFUL to --> res/pythia-410m_cirriculum_full/data/labeled.json\n",
      "*** jdump(unlabeled_data_json_format, unlabeled_data_path) SUCESSFUL to --> res/pythia-410m_cirriculum_full/data/unlabeled.json\n",
      "*** Training-Data-Size = 10000\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "*** SANITY-CHECK: Training-Sample#1. - TEXT.:\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "The distance between two stars is 6.52 √ó 10^5 light years. What is the distance between the two stars in parsecs? (1 parsec = 3.26 light years)\n",
      "Answer Choices: (A) 2 √ó 10^5 (B) 4 √ó 10^6 (C) 5 √ó 10^7 (D) 7 √ó 10^7 (E) 9 √ó 10^8\n",
      "\n",
      "### Response:Let's think about the multi-choice question.\n",
      "6.52 √ó 10^5 ly / (3.26 ly/parsec) = 2 x 10^5 persec\n",
      "The answer is A.</s>\n",
      "\n",
      "\n",
      "/content/S2L_Cirriculum/schedule_base.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=self.model,\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 50278, 'bos_token_id': 50279, 'pad_token_id': 50277}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m setting up run gqyllnen (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run gqyllnen (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m setting up run gqyllnen (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/S2L_Cirriculum/wandb/run-20251129_015943-gqyllnen\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdesert-grass-11\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/henryliu999-other/S2L_Cirriculum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/henryliu999-other/S2L_Cirriculum/runs/gqyllnen\u001b[0m\n",
      "{'loss': 1.2472, 'grad_norm': 27.093629837036133, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'loss': 1.2081, 'grad_norm': 23.974123001098633, 'learning_rate': 6.896551724137931e-07, 'epoch': 0.01}\n",
      "{'loss': 1.22, 'grad_norm': 24.11684226989746, 'learning_rate': 1.3793103448275862e-06, 'epoch': 0.01}\n",
      "{'loss': 1.1363, 'grad_norm': 28.3900089263916, 'learning_rate': 2.0689655172413796e-06, 'epoch': 0.01}\n",
      "{'loss': 1.2741, 'grad_norm': 25.70219612121582, 'learning_rate': 2.7586206896551725e-06, 'epoch': 0.02}\n",
      "{'loss': 1.4431, 'grad_norm': 24.49167823791504, 'learning_rate': 3.448275862068966e-06, 'epoch': 0.02}\n",
      "{'loss': 1.4507, 'grad_norm': 29.916536331176758, 'learning_rate': 4.137931034482759e-06, 'epoch': 0.02}\n",
      "{'loss': 1.6217, 'grad_norm': 35.59943389892578, 'learning_rate': 4.8275862068965525e-06, 'epoch': 0.03}\n",
      "{'loss': 1.5333, 'grad_norm': 36.50762176513672, 'learning_rate': 5.517241379310345e-06, 'epoch': 0.03}\n",
      "{'loss': 1.7814, 'grad_norm': 36.73701477050781, 'learning_rate': 6.206896551724138e-06, 'epoch': 0.03}\n",
      "{'loss': 1.8317, 'grad_norm': 32.750404357910156, 'learning_rate': 6.896551724137932e-06, 'epoch': 0.04}\n",
      "{'loss': 1.8169, 'grad_norm': 44.61578369140625, 'learning_rate': 7.586206896551724e-06, 'epoch': 0.04}\n",
      "{'loss': 1.9803, 'grad_norm': 48.533226013183594, 'learning_rate': 8.275862068965518e-06, 'epoch': 0.04}\n",
      "{'loss': 1.7376, 'grad_norm': 46.38938903808594, 'learning_rate': 8.965517241379312e-06, 'epoch': 0.04}\n",
      "{'loss': 2.0411, 'grad_norm': 44.195213317871094, 'learning_rate': 9.655172413793105e-06, 'epoch': 0.05}\n",
      "{'loss': 1.7772, 'grad_norm': 45.16423416137695, 'learning_rate': 1.0344827586206898e-05, 'epoch': 0.05}\n",
      "{'loss': 1.7579, 'grad_norm': 50.288307189941406, 'learning_rate': 1.103448275862069e-05, 'epoch': 0.05}\n",
      "{'loss': 1.9129, 'grad_norm': 57.00651931762695, 'learning_rate': 1.1724137931034483e-05, 'epoch': 0.06}\n",
      "{'loss': 2.0604, 'grad_norm': 45.57039260864258, 'learning_rate': 1.2413793103448277e-05, 'epoch': 0.06}\n",
      "{'loss': 1.8913, 'grad_norm': 45.67634963989258, 'learning_rate': 1.310344827586207e-05, 'epoch': 0.06}\n",
      "{'loss': 2.0612, 'grad_norm': 45.58883285522461, 'learning_rate': 1.3793103448275863e-05, 'epoch': 0.07}\n",
      "{'loss': 1.8672, 'grad_norm': 59.32292556762695, 'learning_rate': 1.4482758620689657e-05, 'epoch': 0.07}\n",
      "{'loss': 1.7731, 'grad_norm': 49.42411422729492, 'learning_rate': 1.5172413793103448e-05, 'epoch': 0.07}\n",
      "{'loss': 2.0168, 'grad_norm': 43.94362258911133, 'learning_rate': 1.586206896551724e-05, 'epoch': 0.08}\n",
      "{'loss': 1.8793, 'grad_norm': 70.2939682006836, 'learning_rate': 1.6551724137931037e-05, 'epoch': 0.08}\n",
      "{'loss': 1.8965, 'grad_norm': 40.69740676879883, 'learning_rate': 1.7241379310344828e-05, 'epoch': 0.08}\n",
      "{'loss': 1.8837, 'grad_norm': 38.41814422607422, 'learning_rate': 1.7931034482758623e-05, 'epoch': 0.09}\n",
      "{'loss': 1.7947, 'grad_norm': 42.52568817138672, 'learning_rate': 1.8620689655172415e-05, 'epoch': 0.09}\n",
      "{'loss': 1.9409, 'grad_norm': 35.755374908447266, 'learning_rate': 1.931034482758621e-05, 'epoch': 0.09}\n",
      "{'loss': 1.9241, 'grad_norm': 36.008453369140625, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 1.867, 'grad_norm': 36.36589813232422, 'learning_rate': 1.9999940408195878e-05, 'epoch': 0.1}\n",
      "{'loss': 1.9899, 'grad_norm': 43.089115142822266, 'learning_rate': 1.9999761633493754e-05, 'epoch': 0.1}\n",
      "{'loss': 1.9351, 'grad_norm': 40.21225357055664, 'learning_rate': 1.9999463678024317e-05, 'epoch': 0.11}\n",
      "{'loss': 2.0359, 'grad_norm': 37.954627990722656, 'learning_rate': 1.999904654533872e-05, 'epoch': 0.11}\n",
      "{'loss': 2.0459, 'grad_norm': 50.3984375, 'learning_rate': 1.9998510240408495e-05, 'epoch': 0.11}\n",
      "{'loss': 1.8648, 'grad_norm': 33.39844512939453, 'learning_rate': 1.999785476962552e-05, 'epoch': 0.12}\n",
      "{'loss': 2.0091, 'grad_norm': 24.34857177734375, 'learning_rate': 1.9997080140801932e-05, 'epoch': 0.12}\n",
      "{'loss': 1.9127, 'grad_norm': 25.62027359008789, 'learning_rate': 1.9996186363170037e-05, 'epoch': 0.12}\n",
      "{'loss': 1.7761, 'grad_norm': 24.467479705810547, 'learning_rate': 1.9995173447382193e-05, 'epoch': 0.12}\n",
      "{'loss': 1.947, 'grad_norm': 27.450965881347656, 'learning_rate': 1.9994041405510705e-05, 'epoch': 0.13}\n",
      "{'loss': 1.9163, 'grad_norm': 30.006792068481445, 'learning_rate': 1.9992790251047655e-05, 'epoch': 0.13}\n",
      "{'loss': 1.9147, 'grad_norm': 26.8416748046875, 'learning_rate': 1.999141999890475e-05, 'epoch': 0.13}\n",
      "{'loss': 2.0408, 'grad_norm': 21.908161163330078, 'learning_rate': 1.9989930665413148e-05, 'epoch': 0.14}\n",
      "{'loss': 1.7333, 'grad_norm': 20.014816284179688, 'learning_rate': 1.998832226832327e-05, 'epoch': 0.14}\n",
      "{'loss': 1.5443, 'grad_norm': 24.270498275756836, 'learning_rate': 1.9986594826804563e-05, 'epoch': 0.14}\n",
      "{'loss': 1.5451, 'grad_norm': 21.0932559967041, 'learning_rate': 1.9984748361445306e-05, 'epoch': 0.15}\n",
      "{'loss': 1.6946, 'grad_norm': 20.9068660736084, 'learning_rate': 1.998278289425234e-05, 'epoch': 0.15}\n",
      "{'loss': 1.601, 'grad_norm': 21.56127166748047, 'learning_rate': 1.9980698448650805e-05, 'epoch': 0.15}\n",
      "{'loss': 1.689, 'grad_norm': 24.062137603759766, 'learning_rate': 1.9978495049483883e-05, 'epoch': 0.16}\n",
      "{'loss': 1.8061, 'grad_norm': 26.863723754882812, 'learning_rate': 1.997617272301248e-05, 'epoch': 0.16}\n",
      "{'loss': 1.443, 'grad_norm': 16.612043380737305, 'learning_rate': 1.9973731496914914e-05, 'epoch': 0.16}\n",
      "{'loss': 1.4136, 'grad_norm': 20.846939086914062, 'learning_rate': 1.9971171400286602e-05, 'epoch': 0.17}\n",
      "{'loss': 1.4132, 'grad_norm': 20.072912216186523, 'learning_rate': 1.9968492463639704e-05, 'epoch': 0.17}\n",
      "{'loss': 1.4423, 'grad_norm': 16.229339599609375, 'learning_rate': 1.9965694718902745e-05, 'epoch': 0.17}\n",
      "{'loss': 1.3944, 'grad_norm': 20.39926528930664, 'learning_rate': 1.9962778199420265e-05, 'epoch': 0.18}\n",
      "{'loss': 1.4645, 'grad_norm': 21.067167282104492, 'learning_rate': 1.9959742939952393e-05, 'epoch': 0.18}\n",
      "{'loss': 1.3919, 'grad_norm': 26.00871467590332, 'learning_rate': 1.9956588976674442e-05, 'epoch': 0.18}\n",
      "{'loss': 1.6805, 'grad_norm': 24.2702693939209, 'learning_rate': 1.995331634717649e-05, 'epoch': 0.19}\n",
      "{'loss': 1.6751, 'grad_norm': 20.65357780456543, 'learning_rate': 1.994992509046291e-05, 'epoch': 0.19}\n",
      "{'loss': 1.6316, 'grad_norm': 20.87973403930664, 'learning_rate': 1.9946415246951928e-05, 'epoch': 0.19}\n",
      "{'loss': 1.9241, 'grad_norm': 25.716005325317383, 'learning_rate': 1.9942786858475126e-05, 'epoch': 0.2}\n",
      "{'loss': 1.801, 'grad_norm': 20.62413787841797, 'learning_rate': 1.9939039968276942e-05, 'epoch': 0.2}\n",
      "{'loss': 1.765, 'grad_norm': 18.11965560913086, 'learning_rate': 1.9935174621014173e-05, 'epoch': 0.2}\n",
      "{'loss': 1.6294, 'grad_norm': 17.262256622314453, 'learning_rate': 1.9931190862755416e-05, 'epoch': 0.2}\n",
      "{'loss': 1.6383, 'grad_norm': 21.299945831298828, 'learning_rate': 1.992708874098054e-05, 'epoch': 0.21}\n",
      "{'loss': 1.705, 'grad_norm': 15.038808822631836, 'learning_rate': 1.992286830458012e-05, 'epoch': 0.21}\n",
      "{'loss': 1.7374, 'grad_norm': 13.73176383972168, 'learning_rate': 1.9918529603854825e-05, 'epoch': 0.21}\n",
      "{'loss': 1.6257, 'grad_norm': 15.316874504089355, 'learning_rate': 1.991407269051487e-05, 'epoch': 0.22}\n",
      "{'loss': 1.8311, 'grad_norm': 14.757955551147461, 'learning_rate': 1.990949761767935e-05, 'epoch': 0.22}\n",
      "{'loss': 1.804, 'grad_norm': 14.52001667022705, 'learning_rate': 1.9904804439875635e-05, 'epoch': 0.22}\n",
      "{'loss': 1.8053, 'grad_norm': 12.967301368713379, 'learning_rate': 1.989999321303871e-05, 'epoch': 0.23}\n",
      "{'loss': 1.6424, 'grad_norm': 13.093436241149902, 'learning_rate': 1.9895063994510512e-05, 'epoch': 0.23}\n",
      "{'loss': 1.7537, 'grad_norm': 11.529438018798828, 'learning_rate': 1.989001684303925e-05, 'epoch': 0.23}\n",
      "{'loss': 1.3976, 'grad_norm': 11.578262329101562, 'learning_rate': 1.9884851818778695e-05, 'epoch': 0.24}\n",
      "{'loss': 1.5692, 'grad_norm': 12.580682754516602, 'learning_rate': 1.9879568983287468e-05, 'epoch': 0.24}\n",
      "{'loss': 1.8562, 'grad_norm': 16.578916549682617, 'learning_rate': 1.9874168399528307e-05, 'epoch': 0.24}\n",
      "{'loss': 1.4611, 'grad_norm': 12.755864143371582, 'learning_rate': 1.986865013186732e-05, 'epoch': 0.25}\n",
      "{'loss': 1.7044, 'grad_norm': 12.787575721740723, 'learning_rate': 1.9863014246073216e-05, 'epoch': 0.25}\n",
      "{'loss': 1.7242, 'grad_norm': 13.635358810424805, 'learning_rate': 1.985726080931651e-05, 'epoch': 0.25}\n",
      "{'loss': 1.4876, 'grad_norm': 13.482460975646973, 'learning_rate': 1.9851389890168738e-05, 'epoch': 0.26}\n",
      "{'loss': 1.7547, 'grad_norm': 14.414863586425781, 'learning_rate': 1.9845401558601634e-05, 'epoch': 0.26}\n",
      "{'loss': 1.6772, 'grad_norm': 12.274845123291016, 'learning_rate': 1.98392958859863e-05, 'epoch': 0.26}\n",
      "{'loss': 1.8035, 'grad_norm': 13.015154838562012, 'learning_rate': 1.9833072945092334e-05, 'epoch': 0.27}\n",
      "{'loss': 1.6745, 'grad_norm': 14.658114433288574, 'learning_rate': 1.9826732810087e-05, 'epoch': 0.27}\n",
      "{'loss': 1.5794, 'grad_norm': 19.643415451049805, 'learning_rate': 1.9820275556534306e-05, 'epoch': 0.27}\n",
      "{'loss': 1.5746, 'grad_norm': 15.9939546585083, 'learning_rate': 1.9813701261394136e-05, 'epoch': 0.28}\n",
      "{'loss': 1.6763, 'grad_norm': 17.175024032592773, 'learning_rate': 1.980701000302131e-05, 'epoch': 0.28}\n",
      "{'loss': 1.4039, 'grad_norm': 15.698506355285645, 'learning_rate': 1.9800201861164665e-05, 'epoch': 0.28}\n",
      "{'loss': 1.8296, 'grad_norm': 21.75229263305664, 'learning_rate': 1.979327691696608e-05, 'epoch': 0.28}\n",
      "{'loss': 1.8628, 'grad_norm': 18.570068359375, 'learning_rate': 1.9786235252959555e-05, 'epoch': 0.29}\n",
      "{'loss': 1.6897, 'grad_norm': 18.816940307617188, 'learning_rate': 1.977907695307017e-05, 'epoch': 0.29}\n",
      "{'loss': 1.4192, 'grad_norm': 20.119678497314453, 'learning_rate': 1.9771802102613127e-05, 'epoch': 0.29}\n",
      "{'loss': 1.4279, 'grad_norm': 16.95323371887207, 'learning_rate': 1.9764410788292724e-05, 'epoch': 0.3}\n",
      "{'loss': 1.8306, 'grad_norm': 18.39238739013672, 'learning_rate': 1.975690309820131e-05, 'epoch': 0.3}\n",
      "{'loss': 1.4472, 'grad_norm': 17.113460540771484, 'learning_rate': 1.9749279121818235e-05, 'epoch': 0.3}\n",
      "{'loss': 1.5866, 'grad_norm': 20.86079978942871, 'learning_rate': 1.9741538950008817e-05, 'epoch': 0.31}\n",
      "{'loss': 1.2503, 'grad_norm': 21.535938262939453, 'learning_rate': 1.9733682675023207e-05, 'epoch': 0.31}\n",
      "{'loss': 1.3255, 'grad_norm': 22.10923194885254, 'learning_rate': 1.972571039049533e-05, 'epoch': 0.31}\n",
      "{'loss': 1.378, 'grad_norm': 23.876312255859375, 'learning_rate': 1.971762219144174e-05, 'epoch': 0.32}\n",
      "{'loss': 1.3477, 'grad_norm': 25.30373764038086, 'learning_rate': 1.9709418174260523e-05, 'epoch': 0.32}\n",
      "{'loss': 1.3934, 'grad_norm': 17.125038146972656, 'learning_rate': 1.9701098436730108e-05, 'epoch': 0.32}\n",
      "{'loss': 1.4295, 'grad_norm': 13.062219619750977, 'learning_rate': 1.969266307800813e-05, 'epoch': 0.33}\n",
      "{'loss': 1.5503, 'grad_norm': 13.880707740783691, 'learning_rate': 1.9684112198630246e-05, 'epoch': 0.33}\n",
      "{'loss': 1.2913, 'grad_norm': 12.007887840270996, 'learning_rate': 1.967544590050891e-05, 'epoch': 0.33}\n",
      "{'loss': 1.3127, 'grad_norm': 18.465871810913086, 'learning_rate': 1.9666664286932198e-05, 'epoch': 0.34}\n",
      "{'loss': 1.4689, 'grad_norm': 12.096962928771973, 'learning_rate': 1.9657767462562544e-05, 'epoch': 0.34}\n",
      "{'loss': 1.4099, 'grad_norm': 13.357730865478516, 'learning_rate': 1.9648755533435517e-05, 'epoch': 0.34}\n",
      "{'loss': 1.5011, 'grad_norm': 15.162436485290527, 'learning_rate': 1.9639628606958535e-05, 'epoch': 0.35}\n",
      "{'loss': 1.5833, 'grad_norm': 19.13787841796875, 'learning_rate': 1.96303867919096e-05, 'epoch': 0.35}\n",
      "{'loss': 1.5551, 'grad_norm': 17.04464340209961, 'learning_rate': 1.9621030198436007e-05, 'epoch': 0.35}\n",
      "{'loss': 1.6566, 'grad_norm': 18.140151977539062, 'learning_rate': 1.9611558938053003e-05, 'epoch': 0.36}\n",
      "{'loss': 1.5886, 'grad_norm': 21.80881690979004, 'learning_rate': 1.9601973123642493e-05, 'epoch': 0.36}\n",
      "{'loss': 1.5334, 'grad_norm': 19.12769317626953, 'learning_rate': 1.9592272869451672e-05, 'epoch': 0.36}\n",
      "{'loss': 1.5815, 'grad_norm': 18.047870635986328, 'learning_rate': 1.9582458291091664e-05, 'epoch': 0.36}\n",
      "{'loss': 1.6387, 'grad_norm': 18.302736282348633, 'learning_rate': 1.957252950553616e-05, 'epoch': 0.37}\n",
      "{'loss': 1.8374, 'grad_norm': 18.79608917236328, 'learning_rate': 1.9562486631120007e-05, 'epoch': 0.37}\n",
      "{'loss': 1.8008, 'grad_norm': 20.335559844970703, 'learning_rate': 1.9552329787537805e-05, 'epoch': 0.37}\n",
      "{'loss': 1.7614, 'grad_norm': 19.745561599731445, 'learning_rate': 1.9542059095842484e-05, 'epoch': 0.38}\n",
      "{'loss': 1.4225, 'grad_norm': 15.808358192443848, 'learning_rate': 1.9531674678443853e-05, 'epoch': 0.38}\n",
      "{'loss': 1.5417, 'grad_norm': 22.7427921295166, 'learning_rate': 1.952117665910714e-05, 'epoch': 0.38}\n",
      "{'loss': 1.733, 'grad_norm': 13.003190040588379, 'learning_rate': 1.9510565162951538e-05, 'epoch': 0.39}\n",
      "{'loss': 1.6665, 'grad_norm': 12.241786003112793, 'learning_rate': 1.9499840316448675e-05, 'epoch': 0.39}\n",
      "{'loss': 1.6168, 'grad_norm': 20.661705017089844, 'learning_rate': 1.948900224742115e-05, 'epoch': 0.39}\n",
      "{'loss': 1.58, 'grad_norm': 15.433815956115723, 'learning_rate': 1.9478051085040978e-05, 'epoch': 0.4}\n",
      "{'loss': 1.4736, 'grad_norm': 13.177715301513672, 'learning_rate': 1.9466986959828063e-05, 'epoch': 0.4}\n",
      "{'loss': 1.6309, 'grad_norm': 25.670074462890625, 'learning_rate': 1.945581000364864e-05, 'epoch': 0.4}\n",
      "{'loss': 1.2995, 'grad_norm': 12.844149589538574, 'learning_rate': 1.9444520349713705e-05, 'epoch': 0.41}\n",
      "{'loss': 1.5245, 'grad_norm': 14.913558006286621, 'learning_rate': 1.9433118132577432e-05, 'epoch': 0.41}\n",
      "{'loss': 1.5237, 'grad_norm': 15.739096641540527, 'learning_rate': 1.942160348813556e-05, 'epoch': 0.41}\n",
      "{'loss': 1.5904, 'grad_norm': 20.873119354248047, 'learning_rate': 1.9409976553623767e-05, 'epoch': 0.42}\n",
      "{'loss': 1.4056, 'grad_norm': 16.71555519104004, 'learning_rate': 1.9398237467616063e-05, 'epoch': 0.42}\n",
      "{'loss': 1.4371, 'grad_norm': 39.48033142089844, 'learning_rate': 1.9386386370023104e-05, 'epoch': 0.42}\n",
      "{'loss': 1.4593, 'grad_norm': 17.456567764282227, 'learning_rate': 1.9374423402090553e-05, 'epoch': 0.43}\n",
      "{'loss': 1.8715, 'grad_norm': 24.319623947143555, 'learning_rate': 1.9362348706397374e-05, 'epoch': 0.43}\n",
      "{'loss': 1.6453, 'grad_norm': 16.798873901367188, 'learning_rate': 1.9350162426854152e-05, 'epoch': 0.43}\n",
      "{'loss': 1.5518, 'grad_norm': 15.375784873962402, 'learning_rate': 1.933786470870136e-05, 'epoch': 0.44}\n",
      "{'loss': 1.5859, 'grad_norm': 16.91322135925293, 'learning_rate': 1.9325455698507638e-05, 'epoch': 0.44}\n",
      "{'loss': 1.808, 'grad_norm': 16.045696258544922, 'learning_rate': 1.931293554416805e-05, 'epoch': 0.44}\n",
      "{'loss': 1.4652, 'grad_norm': 14.647836685180664, 'learning_rate': 1.9300304394902315e-05, 'epoch': 0.44}\n",
      "{'loss': 1.4546, 'grad_norm': 13.989152908325195, 'learning_rate': 1.9287562401253023e-05, 'epoch': 0.45}\n",
      "{'loss': 1.3995, 'grad_norm': 14.446857452392578, 'learning_rate': 1.927470971508386e-05, 'epoch': 0.45}\n",
      "{'loss': 1.733, 'grad_norm': 16.395973205566406, 'learning_rate': 1.9261746489577767e-05, 'epoch': 0.45}\n",
      "{'loss': 1.5823, 'grad_norm': 19.482967376708984, 'learning_rate': 1.924867287923515e-05, 'epoch': 0.46}\n",
      "{'loss': 1.3561, 'grad_norm': 15.238972663879395, 'learning_rate': 1.923548903987201e-05, 'epoch': 0.46}\n",
      "{'loss': 1.3925, 'grad_norm': 15.660013198852539, 'learning_rate': 1.9222195128618108e-05, 'epoch': 0.46}\n",
      "{'loss': 1.4908, 'grad_norm': 17.950510025024414, 'learning_rate': 1.9208791303915063e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2559, 'grad_norm': 14.640763282775879, 'learning_rate': 1.919527772551451e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3033, 'grad_norm': 15.499478340148926, 'learning_rate': 1.918165455447614e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3515, 'grad_norm': 21.03371238708496, 'learning_rate': 1.9167921953165827e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4448, 'grad_norm': 19.76853370666504, 'learning_rate': 1.9154080085253665e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3855, 'grad_norm': 12.757416725158691, 'learning_rate': 1.9140129115712035e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3154, 'grad_norm': 11.6941499710083, 'learning_rate': 1.912606921081362e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3494, 'grad_norm': 7.861086845397949, 'learning_rate': 1.9111900538129443e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3664, 'grad_norm': 8.90381908416748, 'learning_rate': 1.909762326652686e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3136, 'grad_norm': 9.790143966674805, 'learning_rate': 1.908323756616754e-05, 'epoch': 0.5}\n",
      "{'loss': 1.4056, 'grad_norm': 9.91052532196045, 'learning_rate': 1.9068743608505454e-05, 'epoch': 0.5}\n",
      "{'loss': 1.4383, 'grad_norm': 9.199407577514648, 'learning_rate': 1.9054141566284822e-05, 'epoch': 0.5}\n",
      "{'loss': 1.5289, 'grad_norm': 9.841320991516113, 'learning_rate': 1.9039431613538047e-05, 'epoch': 0.51}\n",
      "{'loss': 1.7608, 'grad_norm': 9.316141128540039, 'learning_rate': 1.9024613925583652e-05, 'epoch': 0.51}\n",
      "{'loss': 1.5188, 'grad_norm': 9.939764976501465, 'learning_rate': 1.900968867902419e-05, 'epoch': 0.51}\n",
      "{'loss': 1.5971, 'grad_norm': 14.839086532592773, 'learning_rate': 1.899465605174414e-05, 'epoch': 0.52}\n",
      "{'loss': 1.5611, 'grad_norm': 11.374881744384766, 'learning_rate': 1.8979516222907776e-05, 'epoch': 0.52}\n",
      "{'loss': 1.4557, 'grad_norm': 11.129461288452148, 'learning_rate': 1.896426937295704e-05, 'epoch': 0.52}\n",
      "{'loss': 1.7324, 'grad_norm': 10.835678100585938, 'learning_rate': 1.8948915683609387e-05, 'epoch': 0.52}\n",
      "{'loss': 1.47, 'grad_norm': 11.885248184204102, 'learning_rate': 1.8933455337855633e-05, 'epoch': 0.53}\n",
      "{'loss': 1.6899, 'grad_norm': 14.869141578674316, 'learning_rate': 1.8917888519957756e-05, 'epoch': 0.53}\n",
      "{'loss': 1.4228, 'grad_norm': 13.889668464660645, 'learning_rate': 1.89022154154467e-05, 'epoch': 0.53}\n",
      "{'loss': 1.4155, 'grad_norm': 13.067256927490234, 'learning_rate': 1.8886436211120195e-05, 'epoch': 0.54}\n",
      "{'loss': 1.6763, 'grad_norm': 24.105684280395508, 'learning_rate': 1.8870551095040476e-05, 'epoch': 0.54}\n",
      "{'loss': 1.4015, 'grad_norm': 12.468908309936523, 'learning_rate': 1.8854560256532098e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3702, 'grad_norm': 13.502534866333008, 'learning_rate': 1.8838463886179647e-05, 'epoch': 0.55}\n",
      "{'loss': 1.5564, 'grad_norm': 12.89148998260498, 'learning_rate': 1.8822262175825463e-05, 'epoch': 0.55}\n",
      "{'loss': 1.6622, 'grad_norm': 14.484284400939941, 'learning_rate': 1.880595531856738e-05, 'epoch': 0.55}\n",
      "{'loss': 1.6374, 'grad_norm': 15.697340965270996, 'learning_rate': 1.878954350875641e-05, 'epoch': 0.56}\n",
      "{'loss': 1.5836, 'grad_norm': 12.995500564575195, 'learning_rate': 1.877302694199442e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4699, 'grad_norm': 13.729212760925293, 'learning_rate': 1.8756405815131815e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3725, 'grad_norm': 12.488597869873047, 'learning_rate': 1.873968032626518e-05, 'epoch': 0.57}\n",
      "{'loss': 1.5874, 'grad_norm': 14.96603012084961, 'learning_rate': 1.872285067473493e-05, 'epoch': 0.57}\n",
      "{'loss': 1.6212, 'grad_norm': 14.572916984558105, 'learning_rate': 1.8705917061122917e-05, 'epoch': 0.57}\n",
      "{'loss': 1.4576, 'grad_norm': 22.28921127319336, 'learning_rate': 1.8688879687250067e-05, 'epoch': 0.58}\n",
      "{'loss': 1.5906, 'grad_norm': 16.159669876098633, 'learning_rate': 1.8671738756173946e-05, 'epoch': 0.58}\n",
      "{'loss': 1.4841, 'grad_norm': 16.19417381286621, 'learning_rate': 1.8654494472186352e-05, 'epoch': 0.58}\n",
      "{'loss': 1.6276, 'grad_norm': 16.44759178161621, 'learning_rate': 1.8637147040810884e-05, 'epoch': 0.59}\n",
      "{'loss': 1.4358, 'grad_norm': 19.77128791809082, 'learning_rate': 1.8619696668800494e-05, 'epoch': 0.59}\n",
      "{'loss': 1.5383, 'grad_norm': 14.448455810546875, 'learning_rate': 1.860214356413501e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3162, 'grad_norm': 23.489238739013672, 'learning_rate': 1.8584487936018663e-05, 'epoch': 0.6}\n",
      "{'loss': 1.448, 'grad_norm': 19.420024871826172, 'learning_rate': 1.8566729994877604e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3784, 'grad_norm': 25.242063522338867, 'learning_rate': 1.854886995235738e-05, 'epoch': 0.6}\n",
      "{'loss': 1.4644, 'grad_norm': 19.619203567504883, 'learning_rate': 1.8530908021320427e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3717, 'grad_norm': 23.427133560180664, 'learning_rate': 1.8512844415843514e-05, 'epoch': 0.61}\n",
      "{'loss': 1.4323, 'grad_norm': 16.14582633972168, 'learning_rate': 1.8494679351215212e-05, 'epoch': 0.61}\n",
      "{'loss': 1.4501, 'grad_norm': 22.47583770751953, 'learning_rate': 1.8476413043933316e-05, 'epoch': 0.61}\n",
      "{'loss': 1.6888, 'grad_norm': 27.363021850585938, 'learning_rate': 1.8458045711702264e-05, 'epoch': 0.62}\n",
      "{'loss': 1.4837, 'grad_norm': 21.020875930786133, 'learning_rate': 1.8439577573430557e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1698, 'grad_norm': 20.638654708862305, 'learning_rate': 1.842100884922812e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3692, 'grad_norm': 34.46812057495117, 'learning_rate': 1.8402339760403715e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1618, 'grad_norm': 15.98904037475586, 'learning_rate': 1.8383570529462273e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3084, 'grad_norm': 15.090605735778809, 'learning_rate': 1.8364701380102267e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2729, 'grad_norm': 17.183277130126953, 'learning_rate': 1.834573253721303e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1129, 'grad_norm': 15.149637222290039, 'learning_rate': 1.8326664226872063e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3039, 'grad_norm': 18.097959518432617, 'learning_rate': 1.8307496676342384e-05, 'epoch': 0.64}\n",
      "{'loss': 1.299, 'grad_norm': 14.153168678283691, 'learning_rate': 1.828823011406977e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3257, 'grad_norm': 14.677268981933594, 'learning_rate': 1.8268864769680054e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2536, 'grad_norm': 15.212383270263672, 'learning_rate': 1.824940087397641e-05, 'epoch': 0.65}\n",
      "{'loss': 1.4408, 'grad_norm': 11.10699462890625, 'learning_rate': 1.8229838658936566e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3487, 'grad_norm': 9.653395652770996, 'learning_rate': 1.8210178357710057e-05, 'epoch': 0.66}\n",
      "{'loss': 1.338, 'grad_norm': 13.581986427307129, 'learning_rate': 1.819042020461545e-05, 'epoch': 0.66}\n",
      "{'loss': 1.5449, 'grad_norm': 12.763666152954102, 'learning_rate': 1.8170564435137542e-05, 'epoch': 0.67}\n",
      "{'loss': 1.4968, 'grad_norm': 10.951178550720215, 'learning_rate': 1.8150611285924556e-05, 'epoch': 0.67}\n",
      "{'loss': 1.6705, 'grad_norm': 13.469205856323242, 'learning_rate': 1.8130560994785325e-05, 'epoch': 0.67}\n",
      "{'loss': 1.5185, 'grad_norm': 13.843772888183594, 'learning_rate': 1.8110413800686456e-05, 'epoch': 0.68}\n",
      "{'loss': 1.6476, 'grad_norm': 11.296405792236328, 'learning_rate': 1.8090169943749477e-05, 'epoch': 0.68}\n",
      "{'loss': 1.6305, 'grad_norm': 11.516517639160156, 'learning_rate': 1.8069829665247975e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3657, 'grad_norm': 14.630508422851562, 'learning_rate': 1.8049393207604734e-05, 'epoch': 0.68}\n",
      "{'loss': 1.6646, 'grad_norm': 12.887083053588867, 'learning_rate': 1.8028860814388826e-05, 'epoch': 0.69}\n",
      "{'loss': 1.4254, 'grad_norm': 11.800593376159668, 'learning_rate': 1.8008232730312724e-05, 'epoch': 0.69}\n",
      "{'loss': 1.6334, 'grad_norm': 15.57284927368164, 'learning_rate': 1.7987509201229378e-05, 'epoch': 0.69}\n",
      "{'loss': 1.5336, 'grad_norm': 10.444375038146973, 'learning_rate': 1.7966690474129285e-05, 'epoch': 0.7}\n",
      "{'loss': 1.5631, 'grad_norm': 11.982242584228516, 'learning_rate': 1.7945776797137544e-05, 'epoch': 0.7}\n",
      "{'loss': 1.4204, 'grad_norm': 14.047536849975586, 'learning_rate': 1.7924768419510906e-05, 'epoch': 0.7}\n",
      "{'loss': 1.664, 'grad_norm': 25.90170669555664, 'learning_rate': 1.7903665591634794e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3708, 'grad_norm': 14.024728775024414, 'learning_rate': 1.7882468565020327e-05, 'epoch': 0.71}\n",
      "{'loss': 1.4303, 'grad_norm': 25.065940856933594, 'learning_rate': 1.786117759230132e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3754, 'grad_norm': 19.078094482421875, 'learning_rate': 1.7839792927231253e-05, 'epoch': 0.72}\n",
      "{'loss': 1.5722, 'grad_norm': 12.507901191711426, 'learning_rate': 1.78183148246803e-05, 'epoch': 0.72}\n",
      "{'loss': 1.4518, 'grad_norm': 15.800950050354004, 'learning_rate': 1.7796743540632226e-05, 'epoch': 0.72}\n",
      "{'loss': 1.5784, 'grad_norm': 14.778787612915039, 'learning_rate': 1.777507933218138e-05, 'epoch': 0.73}\n",
      "{'loss': 1.5843, 'grad_norm': 13.895177841186523, 'learning_rate': 1.7753322457529615e-05, 'epoch': 0.73}\n",
      "{'loss': 1.5641, 'grad_norm': 14.531061172485352, 'learning_rate': 1.7731473175983215e-05, 'epoch': 0.73}\n",
      "{'loss': 1.497, 'grad_norm': 13.433439254760742, 'learning_rate': 1.7709531747949796e-05, 'epoch': 0.74}\n",
      "{'loss': 1.6045, 'grad_norm': 16.840972900390625, 'learning_rate': 1.7687498434935224e-05, 'epoch': 0.74}\n",
      "{'loss': 1.5657, 'grad_norm': 14.033491134643555, 'learning_rate': 1.7665373499540464e-05, 'epoch': 0.74}\n",
      "{'loss': 1.4406, 'grad_norm': 20.074464797973633, 'learning_rate': 1.7643157205458483e-05, 'epoch': 0.75}\n",
      "{'loss': 1.5131, 'grad_norm': 18.809389114379883, 'learning_rate': 1.7620849817471094e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3533, 'grad_norm': 14.023041725158691, 'learning_rate': 1.759845160144579e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3875, 'grad_norm': 15.505875587463379, 'learning_rate': 1.7575962824332595e-05, 'epoch': 0.76}\n",
      "{'loss': 1.5431, 'grad_norm': 14.030781745910645, 'learning_rate': 1.7553383754160864e-05, 'epoch': 0.76}\n",
      "{'loss': 1.5698, 'grad_norm': 15.144686698913574, 'learning_rate': 1.7530714660036112e-05, 'epoch': 0.76}\n",
      "{'loss': 1.515, 'grad_norm': 14.702296257019043, 'learning_rate': 1.7507955812136775e-05, 'epoch': 0.76}\n",
      "{'loss': 1.5432, 'grad_norm': 16.486549377441406, 'learning_rate': 1.7485107481711014e-05, 'epoch': 0.77}\n",
      "{'loss': 1.385, 'grad_norm': 16.026409149169922, 'learning_rate': 1.7462169941073478e-05, 'epoch': 0.77}\n",
      "{'loss': 1.429, 'grad_norm': 18.965906143188477, 'learning_rate': 1.7439143463602052e-05, 'epoch': 0.77}\n",
      "{'loss': 1.5796, 'grad_norm': 14.934606552124023, 'learning_rate': 1.74160283237346e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3945, 'grad_norm': 14.110798835754395, 'learning_rate': 1.7392824796965703e-05, 'epoch': 0.78}\n",
      "{'loss': 1.4096, 'grad_norm': 15.447569847106934, 'learning_rate': 1.7369533159843368e-05, 'epoch': 0.78}\n",
      "{'loss': 1.134, 'grad_norm': 14.921366691589355, 'learning_rate': 1.734615368996573e-05, 'epoch': 0.79}\n",
      "{'loss': 0.922, 'grad_norm': 13.118118286132812, 'learning_rate': 1.7322686665977738e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0899, 'grad_norm': 18.471302032470703, 'learning_rate': 1.7299132367567856e-05, 'epoch': 0.79}\n",
      "{'loss': 1.4151, 'grad_norm': 16.118446350097656, 'learning_rate': 1.7275491075464716e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1171, 'grad_norm': 16.520950317382812, 'learning_rate': 1.7251763071433767e-05, 'epoch': 0.8}\n",
      "{'loss': 1.212, 'grad_norm': 13.021488189697266, 'learning_rate': 1.7227948638273918e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3081, 'grad_norm': 11.8041410446167, 'learning_rate': 1.7204048059814175e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3387, 'grad_norm': 13.15970230102539, 'learning_rate': 1.7180061620910263e-05, 'epoch': 0.81}\n",
      "{'loss': 1.323, 'grad_norm': 10.41515064239502, 'learning_rate': 1.715598960744121e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2234, 'grad_norm': 9.853618621826172, 'learning_rate': 1.7131832306305964e-05, 'epoch': 0.82}\n",
      "{'loss': 1.4514, 'grad_norm': 9.310538291931152, 'learning_rate': 1.710759000541995e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3789, 'grad_norm': 11.344127655029297, 'learning_rate': 1.7083262993711663e-05, 'epoch': 0.82}\n",
      "{'loss': 1.5364, 'grad_norm': 11.245309829711914, 'learning_rate': 1.7058851561119198e-05, 'epoch': 0.83}\n",
      "{'loss': 1.4383, 'grad_norm': 9.39879035949707, 'learning_rate': 1.7034355998586828e-05, 'epoch': 0.83}\n",
      "{'loss': 1.7468, 'grad_norm': 12.51472282409668, 'learning_rate': 1.7009776598061496e-05, 'epoch': 0.83}\n",
      "{'loss': 1.6179, 'grad_norm': 12.114042282104492, 'learning_rate': 1.6985113652489374e-05, 'epoch': 0.84}\n",
      "{'loss': 1.5117, 'grad_norm': 10.515917778015137, 'learning_rate': 1.6960367455812336e-05, 'epoch': 0.84}\n",
      "{'loss': 1.525, 'grad_norm': 11.968033790588379, 'learning_rate': 1.6935538302964496e-05, 'epoch': 0.84}\n",
      "{'loss': 1.7276, 'grad_norm': 12.021330833435059, 'learning_rate': 1.691062648986865e-05, 'epoch': 0.84}\n",
      "{'loss': 1.5519, 'grad_norm': 9.761343002319336, 'learning_rate': 1.6885632313432772e-05, 'epoch': 0.85}\n",
      "{'loss': 1.4311, 'grad_norm': 8.260954856872559, 'learning_rate': 1.686055607154648e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3742, 'grad_norm': 9.706308364868164, 'learning_rate': 1.6835398063077476e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3683, 'grad_norm': 10.924155235290527, 'learning_rate': 1.6810158587867973e-05, 'epoch': 0.86}\n",
      "{'loss': 1.4716, 'grad_norm': 8.711442947387695, 'learning_rate': 1.6784837946731148e-05, 'epoch': 0.86}\n",
      "{'loss': 1.3311, 'grad_norm': 9.054081916809082, 'learning_rate': 1.6759436441447544e-05, 'epoch': 0.86}\n",
      "{'loss': 1.4088, 'grad_norm': 8.115363121032715, 'learning_rate': 1.673395437476146e-05, 'epoch': 0.87}\n",
      "{'loss': 1.4006, 'grad_norm': 7.920323848724365, 'learning_rate': 1.6708392050377365e-05, 'epoch': 0.87}\n",
      "{'loss': 1.4901, 'grad_norm': 9.216085433959961, 'learning_rate': 1.668274977295626e-05, 'epoch': 0.87}\n",
      "{'loss': 1.5261, 'grad_norm': 8.459778785705566, 'learning_rate': 1.6657027848112064e-05, 'epoch': 0.88}\n",
      "{'loss': 1.6878, 'grad_norm': 11.647859573364258, 'learning_rate': 1.6631226582407954e-05, 'epoch': 0.88}\n",
      "{'loss': 1.5199, 'grad_norm': 8.74267292022705, 'learning_rate': 1.660534628335273e-05, 'epoch': 0.88}\n",
      "{'loss': 1.4343, 'grad_norm': 8.777178764343262, 'learning_rate': 1.657938725939713e-05, 'epoch': 0.89}\n",
      "{'loss': 1.4198, 'grad_norm': 9.178218841552734, 'learning_rate': 1.6553349819930167e-05, 'epoch': 0.89}\n",
      "{'loss': 1.4956, 'grad_norm': 12.805329322814941, 'learning_rate': 1.6527234275275445e-05, 'epoch': 0.89}\n",
      "{'loss': 1.5623, 'grad_norm': 10.104896545410156, 'learning_rate': 1.6501040936687444e-05, 'epoch': 0.9}\n",
      "{'loss': 1.1492, 'grad_norm': 12.543598175048828, 'learning_rate': 1.6474770116347824e-05, 'epoch': 0.9}\n",
      "{'loss': 1.5138, 'grad_norm': 14.538607597351074, 'learning_rate': 1.6448422127361707e-05, 'epoch': 0.9}\n",
      "{'loss': 1.3998, 'grad_norm': 12.659088134765625, 'learning_rate': 1.6421997283753928e-05, 'epoch': 0.91}\n",
      "{'loss': 1.4049, 'grad_norm': 12.31688117980957, 'learning_rate': 1.6395495900465306e-05, 'epoch': 0.91}\n",
      "{'loss': 1.3477, 'grad_norm': 13.11269760131836, 'learning_rate': 1.6368918293348893e-05, 'epoch': 0.91}\n",
      "{'loss': 1.4638, 'grad_norm': 12.163769721984863, 'learning_rate': 1.63422647791662e-05, 'epoch': 0.92}\n",
      "{'loss': 1.4309, 'grad_norm': 12.62402057647705, 'learning_rate': 1.6315535675583425e-05, 'epoch': 0.92}\n",
      "{'loss': 1.6254, 'grad_norm': 20.119239807128906, 'learning_rate': 1.6288731301167667e-05, 'epoch': 0.92}\n",
      "{'loss': 1.5361, 'grad_norm': 13.396856307983398, 'learning_rate': 1.626185197538314e-05, 'epoch': 0.92}\n",
      "{'loss': 1.4523, 'grad_norm': 13.494990348815918, 'learning_rate': 1.6234898018587336e-05, 'epoch': 0.93}\n",
      "{'loss': 1.5662, 'grad_norm': 41.90048599243164, 'learning_rate': 1.6207869752027248e-05, 'epoch': 0.93}\n",
      "{'loss': 1.3858, 'grad_norm': 13.188246726989746, 'learning_rate': 1.6180767497835503e-05, 'epoch': 0.93}\n",
      "{'loss': 1.4246, 'grad_norm': 14.31867790222168, 'learning_rate': 1.6153591579026545e-05, 'epoch': 0.94}\n",
      "{'loss': 1.2095, 'grad_norm': 11.867061614990234, 'learning_rate': 1.6126342319492784e-05, 'epoch': 0.94}\n",
      "{'loss': 1.1529, 'grad_norm': 12.799060821533203, 'learning_rate': 1.609902004400073e-05, 'epoch': 0.94}\n",
      "{'loss': 1.1721, 'grad_norm': 12.004386901855469, 'learning_rate': 1.6071625078187113e-05, 'epoch': 0.95}\n",
      "{'loss': 1.1834, 'grad_norm': 12.778221130371094, 'learning_rate': 1.6044157748555024e-05, 'epoch': 0.95}\n",
      "{'loss': 1.1186, 'grad_norm': 12.402630805969238, 'learning_rate': 1.6016618382470014e-05, 'epoch': 0.95}\n",
      "{'loss': 1.1817, 'grad_norm': 20.267826080322266, 'learning_rate': 1.598900730815617e-05, 'epoch': 0.96}\n",
      "{'loss': 1.0746, 'grad_norm': 16.62312126159668, 'learning_rate': 1.5961324854692254e-05, 'epoch': 0.96}\n",
      "{'loss': 1.2841, 'grad_norm': 10.751227378845215, 'learning_rate': 1.593357135200773e-05, 'epoch': 0.96}\n",
      "{'loss': 1.227, 'grad_norm': 12.313261985778809, 'learning_rate': 1.5905747130878853e-05, 'epoch': 0.97}\n",
      "{'loss': 1.5614, 'grad_norm': 19.77381134033203, 'learning_rate': 1.5877852522924733e-05, 'epoch': 0.97}\n",
      "{'loss': 1.5323, 'grad_norm': 25.188514709472656, 'learning_rate': 1.5849887860603374e-05, 'epoch': 0.97}\n",
      "{'loss': 1.5883, 'grad_norm': 22.460222244262695, 'learning_rate': 1.582185347720771e-05, 'epoch': 0.98}\n",
      "{'loss': 1.4128, 'grad_norm': 24.05792808532715, 'learning_rate': 1.5793749706861637e-05, 'epoch': 0.98}\n",
      "{'loss': 1.6695, 'grad_norm': 31.85933494567871, 'learning_rate': 1.576557688451603e-05, 'epoch': 0.98}\n",
      "{'loss': 1.5912, 'grad_norm': 19.34246253967285, 'learning_rate': 1.5737335345944758e-05, 'epoch': 0.99}\n",
      "{'loss': 1.5244, 'grad_norm': 16.579862594604492, 'learning_rate': 1.570902542774066e-05, 'epoch': 0.99}\n",
      "{'loss': 1.3709, 'grad_norm': 11.783015251159668, 'learning_rate': 1.568064746731156e-05, 'epoch': 0.99}\n",
      "{'loss': 1.3702, 'grad_norm': 20.339000701904297, 'learning_rate': 1.5652201802876227e-05, 'epoch': 1.0}\n",
      "{'loss': 1.1047, 'grad_norm': 14.112297058105469, 'learning_rate': 1.5623688773460358e-05, 'epoch': 1.0}\n",
      "{'loss': 1.0643, 'grad_norm': 16.274188995361328, 'learning_rate': 1.559510871889252e-05, 'epoch': 1.0}\n",
      "{'loss': 1.2182, 'grad_norm': 7.61199951171875, 'learning_rate': 1.556646197980012e-05, 'epoch': 1.0}\n",
      "{'loss': 1.0922, 'grad_norm': 7.5894389152526855, 'learning_rate': 1.553774889760533e-05, 'epoch': 1.01}\n",
      "{'loss': 1.1668, 'grad_norm': 9.74318790435791, 'learning_rate': 1.5508969814521026e-05, 'epoch': 1.01}\n",
      "{'loss': 1.0503, 'grad_norm': 9.199241638183594, 'learning_rate': 1.5480125073546705e-05, 'epoch': 1.01}\n",
      "{'loss': 1.2043, 'grad_norm': 7.709866523742676, 'learning_rate': 1.5451215018464386e-05, 'epoch': 1.02}\n",
      "{'loss': 1.2754, 'grad_norm': 8.209304809570312, 'learning_rate': 1.542223999383455e-05, 'epoch': 1.02}\n",
      "{'loss': 1.2577, 'grad_norm': 8.304532051086426, 'learning_rate': 1.5393200344991993e-05, 'epoch': 1.02}\n",
      "{'loss': 1.4476, 'grad_norm': 8.544384002685547, 'learning_rate': 1.5364096418041723e-05, 'epoch': 1.03}\n",
      "{'loss': 1.303, 'grad_norm': 10.356827735900879, 'learning_rate': 1.533492855985485e-05, 'epoch': 1.03}\n",
      "{'loss': 1.3206, 'grad_norm': 14.91120433807373, 'learning_rate': 1.530569711806443e-05, 'epoch': 1.03}\n",
      "{'loss': 1.4495, 'grad_norm': 9.669265747070312, 'learning_rate': 1.527640244106133e-05, 'epoch': 1.04}\n",
      "{'loss': 1.4284, 'grad_norm': 9.952008247375488, 'learning_rate': 1.524704487799008e-05, 'epoch': 1.04}\n",
      "{'loss': 1.4504, 'grad_norm': 10.954413414001465, 'learning_rate': 1.5217624778744718e-05, 'epoch': 1.04}\n",
      "{'loss': 1.5067, 'grad_norm': 10.544814109802246, 'learning_rate': 1.5188142493964595e-05, 'epoch': 1.04}\n",
      "{'loss': 1.3995, 'grad_norm': 12.03367805480957, 'learning_rate': 1.5158598375030218e-05, 'epoch': 1.05}\n",
      "{'loss': 1.4606, 'grad_norm': 12.154812812805176, 'learning_rate': 1.5128992774059063e-05, 'epoch': 1.05}\n",
      "{'loss': 1.2837, 'grad_norm': 12.08406925201416, 'learning_rate': 1.5099326043901361e-05, 'epoch': 1.05}\n",
      "{'loss': 1.3243, 'grad_norm': 10.995247840881348, 'learning_rate': 1.5069598538135905e-05, 'epoch': 1.06}\n",
      "{'loss': 1.3855, 'grad_norm': 16.828201293945312, 'learning_rate': 1.503981061106584e-05, 'epoch': 1.06}\n",
      "{'loss': 1.5399, 'grad_norm': 10.459692001342773, 'learning_rate': 1.5009962617714425e-05, 'epoch': 1.06}\n",
      "{'loss': 1.4649, 'grad_norm': 9.972837448120117, 'learning_rate': 1.4980054913820814e-05, 'epoch': 1.07}\n",
      "{'loss': 1.1688, 'grad_norm': 9.041287422180176, 'learning_rate': 1.4950087855835816e-05, 'epoch': 1.07}\n",
      "{'loss': 1.4444, 'grad_norm': 10.1941499710083, 'learning_rate': 1.4920061800917637e-05, 'epoch': 1.07}\n",
      "{'loss': 1.4065, 'grad_norm': 9.851873397827148, 'learning_rate': 1.4889977106927642e-05, 'epoch': 1.08}\n",
      "{'loss': 1.3024, 'grad_norm': 11.218029022216797, 'learning_rate': 1.485983413242606e-05, 'epoch': 1.08}\n",
      "{'loss': 1.2935, 'grad_norm': 9.48108959197998, 'learning_rate': 1.4829633236667746e-05, 'epoch': 1.08}\n",
      "{'loss': 1.1935, 'grad_norm': 9.5401611328125, 'learning_rate': 1.4799374779597866e-05, 'epoch': 1.09}\n",
      "{'loss': 1.3852, 'grad_norm': 11.432683944702148, 'learning_rate': 1.476905912184763e-05, 'epoch': 1.09}\n",
      "{'loss': 1.278, 'grad_norm': 9.366909980773926, 'learning_rate': 1.4738686624729987e-05, 'epoch': 1.09}\n",
      "{'loss': 1.2469, 'grad_norm': 9.061297416687012, 'learning_rate': 1.470825765023532e-05, 'epoch': 1.1}\n",
      "{'loss': 1.2648, 'grad_norm': 10.712254524230957, 'learning_rate': 1.4677772561027121e-05, 'epoch': 1.1}\n",
      "{'loss': 1.2466, 'grad_norm': 11.310608863830566, 'learning_rate': 1.4647231720437687e-05, 'epoch': 1.1}\n",
      "{'loss': 1.3396, 'grad_norm': 10.77717113494873, 'learning_rate': 1.4616635492463775e-05, 'epoch': 1.11}\n",
      "{'loss': 1.3189, 'grad_norm': 9.162737846374512, 'learning_rate': 1.4585984241762268e-05, 'epoch': 1.11}\n",
      "{'loss': 1.196, 'grad_norm': 9.222952842712402, 'learning_rate': 1.4555278333645833e-05, 'epoch': 1.11}\n",
      "{'loss': 1.2393, 'grad_norm': 11.916872024536133, 'learning_rate': 1.4524518134078565e-05, 'epoch': 1.12}\n",
      "{'loss': 1.3649, 'grad_norm': 9.992544174194336, 'learning_rate': 1.4493704009671614e-05, 'epoch': 1.12}\n",
      "{'loss': 1.1663, 'grad_norm': 10.642590522766113, 'learning_rate': 1.446283632767884e-05, 'epoch': 1.12}\n",
      "{'loss': 1.2321, 'grad_norm': 11.037673950195312, 'learning_rate': 1.4431915455992416e-05, 'epoch': 1.12}\n",
      "{'loss': 1.2411, 'grad_norm': 15.622066497802734, 'learning_rate': 1.440094176313844e-05, 'epoch': 1.13}\n",
      "{'loss': 1.3823, 'grad_norm': 9.930530548095703, 'learning_rate': 1.4369915618272568e-05, 'epoch': 1.13}\n",
      "{'loss': 1.2555, 'grad_norm': 12.002904891967773, 'learning_rate': 1.4338837391175582e-05, 'epoch': 1.13}\n",
      "{'loss': 1.1117, 'grad_norm': 9.689949035644531, 'learning_rate': 1.4307707452249013e-05, 'epoch': 1.14}\n",
      "{'loss': 1.488, 'grad_norm': 13.118701934814453, 'learning_rate': 1.42765261725107e-05, 'epoch': 1.14}\n",
      "{'loss': 1.1103, 'grad_norm': 10.894780158996582, 'learning_rate': 1.424529392359039e-05, 'epoch': 1.14}\n",
      "{'loss': 1.0701, 'grad_norm': 12.785613059997559, 'learning_rate': 1.4214011077725293e-05, 'epoch': 1.15}\n",
      "{'loss': 0.8146, 'grad_norm': 10.852293014526367, 'learning_rate': 1.4182678007755653e-05, 'epoch': 1.15}\n",
      "{'loss': 0.8439, 'grad_norm': 9.195640563964844, 'learning_rate': 1.4151295087120307e-05, 'epoch': 1.15}\n",
      "{'loss': 0.9909, 'grad_norm': 11.420082092285156, 'learning_rate': 1.4119862689852224e-05, 'epoch': 1.16}\n",
      "{'loss': 0.8504, 'grad_norm': 13.379630088806152, 'learning_rate': 1.4088381190574051e-05, 'epoch': 1.16}\n",
      "{'loss': 1.1264, 'grad_norm': 12.556077003479004, 'learning_rate': 1.4056850964493668e-05, 'epoch': 1.16}\n",
      "{'loss': 1.1868, 'grad_norm': 14.0816068649292, 'learning_rate': 1.4025272387399676e-05, 'epoch': 1.17}\n",
      "{'loss': 1.1368, 'grad_norm': 12.4837646484375, 'learning_rate': 1.3993645835656955e-05, 'epoch': 1.17}\n",
      "{'loss': 1.1087, 'grad_norm': 12.343517303466797, 'learning_rate': 1.3961971686202163e-05, 'epoch': 1.17}\n",
      "{'loss': 1.1466, 'grad_norm': 10.545587539672852, 'learning_rate': 1.3930250316539237e-05, 'epoch': 1.18}\n",
      "{'loss': 1.1743, 'grad_norm': 14.160867691040039, 'learning_rate': 1.3898482104734909e-05, 'epoch': 1.18}\n",
      "{'loss': 1.2788, 'grad_norm': 13.634122848510742, 'learning_rate': 1.3866667429414188e-05, 'epoch': 1.18}\n",
      "{'loss': 1.5562, 'grad_norm': 14.445489883422852, 'learning_rate': 1.383480666975586e-05, 'epoch': 1.19}\n",
      "{'loss': 1.4486, 'grad_norm': 14.235284805297852, 'learning_rate': 1.3802900205487948e-05, 'epoch': 1.19}\n",
      "{'loss': 1.4751, 'grad_norm': 16.473262786865234, 'learning_rate': 1.3770948416883205e-05, 'epoch': 1.19}\n",
      "{'loss': 1.4491, 'grad_norm': 17.090364456176758, 'learning_rate': 1.3738951684754585e-05, 'epoch': 1.2}\n",
      "{'loss': 1.3061, 'grad_norm': 13.307912826538086, 'learning_rate': 1.3706910390450679e-05, 'epoch': 1.2}\n",
      "{'loss': 1.3896, 'grad_norm': 16.41256332397461, 'learning_rate': 1.3674824915851193e-05, 'epoch': 1.2}\n",
      "{'loss': 1.3419, 'grad_norm': 15.22188663482666, 'learning_rate': 1.3642695643362398e-05, 'epoch': 1.2}\n",
      "{'loss': 1.3539, 'grad_norm': 17.6226806640625, 'learning_rate': 1.3610522955912551e-05, 'epoch': 1.21}\n",
      "{'loss': 1.2366, 'grad_norm': 16.094545364379883, 'learning_rate': 1.3578307236947348e-05, 'epoch': 1.21}\n",
      "{'loss': 1.3279, 'grad_norm': 15.384180068969727, 'learning_rate': 1.3546048870425356e-05, 'epoch': 1.21}\n",
      "{'loss': 1.3258, 'grad_norm': 16.623462677001953, 'learning_rate': 1.3513748240813429e-05, 'epoch': 1.22}\n",
      "{'loss': 1.2104, 'grad_norm': 14.99543285369873, 'learning_rate': 1.3481405733082118e-05, 'epoch': 1.22}\n",
      "{'loss': 1.3268, 'grad_norm': 13.097556114196777, 'learning_rate': 1.3449021732701106e-05, 'epoch': 1.22}\n",
      "{'loss': 1.406, 'grad_norm': 17.829519271850586, 'learning_rate': 1.3416596625634595e-05, 'epoch': 1.23}\n",
      "{'loss': 1.5185, 'grad_norm': 49.406455993652344, 'learning_rate': 1.3384130798336705e-05, 'epoch': 1.23}\n",
      "{'loss': 1.603, 'grad_norm': 17.0820255279541, 'learning_rate': 1.3351624637746885e-05, 'epoch': 1.23}\n",
      "{'loss': 1.2715, 'grad_norm': 13.957545280456543, 'learning_rate': 1.3319078531285286e-05, 'epoch': 1.24}\n",
      "{'loss': 1.1873, 'grad_norm': 12.46259593963623, 'learning_rate': 1.3286492866848143e-05, 'epoch': 1.24}\n",
      "{'loss': 1.3953, 'grad_norm': 18.454790115356445, 'learning_rate': 1.3253868032803171e-05, 'epoch': 1.24}\n",
      "{'loss': 1.1006, 'grad_norm': 15.181138038635254, 'learning_rate': 1.3221204417984907e-05, 'epoch': 1.25}\n",
      "{'loss': 1.318, 'grad_norm': 13.143539428710938, 'learning_rate': 1.3188502411690101e-05, 'epoch': 1.25}\n",
      "{'loss': 1.3911, 'grad_norm': 14.534926414489746, 'learning_rate': 1.3155762403673065e-05, 'epoch': 1.25}\n",
      "{'loss': 1.3803, 'grad_norm': 14.104043960571289, 'learning_rate': 1.3122984784141021e-05, 'epoch': 1.26}\n",
      "{'loss': 1.1273, 'grad_norm': 12.989500045776367, 'learning_rate': 1.3090169943749475e-05, 'epoch': 1.26}\n",
      "{'loss': 1.316, 'grad_norm': 11.784252166748047, 'learning_rate': 1.3057318273597531e-05, 'epoch': 1.26}\n",
      "{'loss': 1.146, 'grad_norm': 11.8733491897583, 'learning_rate': 1.3024430165223245e-05, 'epoch': 1.27}\n",
      "{'loss': 1.2104, 'grad_norm': 25.954923629760742, 'learning_rate': 1.2991506010598965e-05, 'epoch': 1.27}\n",
      "{'loss': 1.223, 'grad_norm': 15.401305198669434, 'learning_rate': 1.2958546202126638e-05, 'epoch': 1.27}\n",
      "{'loss': 1.1265, 'grad_norm': 13.327609062194824, 'learning_rate': 1.2925551132633164e-05, 'epoch': 1.28}\n",
      "{'loss': 1.2769, 'grad_norm': 14.923540115356445, 'learning_rate': 1.2892521195365679e-05, 'epoch': 1.28}\n",
      "{'loss': 1.5452, 'grad_norm': 13.069478988647461, 'learning_rate': 1.2859456783986892e-05, 'epoch': 1.28}\n",
      "{'loss': 1.2184, 'grad_norm': 15.754485130310059, 'learning_rate': 1.2826358292570398e-05, 'epoch': 1.28}\n",
      "{'loss': 1.2473, 'grad_norm': 12.47900104522705, 'learning_rate': 1.2793226115595951e-05, 'epoch': 1.29}\n",
      "{'loss': 1.2338, 'grad_norm': 13.960577964782715, 'learning_rate': 1.2760060647944794e-05, 'epoch': 1.29}\n",
      "{'loss': 1.1883, 'grad_norm': 12.39403247833252, 'learning_rate': 1.2726862284894939e-05, 'epoch': 1.29}\n",
      "{'loss': 1.4236, 'grad_norm': 12.412757873535156, 'learning_rate': 1.2693631422116455e-05, 'epoch': 1.3}\n",
      "{'loss': 1.037, 'grad_norm': 11.16620922088623, 'learning_rate': 1.2660368455666752e-05, 'epoch': 1.3}\n",
      "{'loss': 1.2123, 'grad_norm': 19.550981521606445, 'learning_rate': 1.262707378198587e-05, 'epoch': 1.3}\n",
      "{'loss': 1.0436, 'grad_norm': 11.73232650756836, 'learning_rate': 1.2593747797891743e-05, 'epoch': 1.31}\n",
      "{'loss': 0.8635, 'grad_norm': 9.977005958557129, 'learning_rate': 1.2560390900575472e-05, 'epoch': 1.31}\n",
      "{'loss': 1.0779, 'grad_norm': 10.409286499023438, 'learning_rate': 1.2527003487596598e-05, 'epoch': 1.31}\n",
      "{'loss': 0.9132, 'grad_norm': 13.053997993469238, 'learning_rate': 1.2493585956878354e-05, 'epoch': 1.32}\n",
      "{'loss': 0.8572, 'grad_norm': 13.548495292663574, 'learning_rate': 1.2460138706702929e-05, 'epoch': 1.32}\n",
      "{'loss': 1.223, 'grad_norm': 9.29475212097168, 'learning_rate': 1.242666213570672e-05, 'epoch': 1.32}\n",
      "{'loss': 1.1485, 'grad_norm': 8.599078178405762, 'learning_rate': 1.2393156642875579e-05, 'epoch': 1.33}\n",
      "{'loss': 1.1236, 'grad_norm': 8.518572807312012, 'learning_rate': 1.2359622627540059e-05, 'epoch': 1.33}\n",
      "{'loss': 1.1388, 'grad_norm': 8.880722999572754, 'learning_rate': 1.2326060489370655e-05, 'epoch': 1.33}\n",
      "{'loss': 1.1373, 'grad_norm': 9.40961742401123, 'learning_rate': 1.229247062837304e-05, 'epoch': 1.34}\n",
      "{'loss': 1.1707, 'grad_norm': 8.474485397338867, 'learning_rate': 1.2258853444883297e-05, 'epoch': 1.34}\n",
      "{'loss': 1.2509, 'grad_norm': 10.57576847076416, 'learning_rate': 1.2225209339563144e-05, 'epoch': 1.34}\n",
      "{'loss': 1.2882, 'grad_norm': 12.600860595703125, 'learning_rate': 1.219153871339518e-05, 'epoch': 1.35}\n",
      "{'loss': 1.4607, 'grad_norm': 12.18893051147461, 'learning_rate': 1.2157841967678064e-05, 'epoch': 1.35}\n",
      "{'loss': 1.2277, 'grad_norm': 14.763238906860352, 'learning_rate': 1.2124119504021776e-05, 'epoch': 1.35}\n",
      "{'loss': 1.3796, 'grad_norm': 13.099287986755371, 'learning_rate': 1.2090371724342804e-05, 'epoch': 1.36}\n",
      "{'loss': 1.5715, 'grad_norm': 15.285503387451172, 'learning_rate': 1.2056599030859367e-05, 'epoch': 1.36}\n",
      "{'loss': 1.244, 'grad_norm': 14.043230056762695, 'learning_rate': 1.2022801826086609e-05, 'epoch': 1.36}\n",
      "{'loss': 1.3012, 'grad_norm': 11.35576343536377, 'learning_rate': 1.1988980512831809e-05, 'epoch': 1.36}\n",
      "{'loss': 1.4989, 'grad_norm': 12.621065139770508, 'learning_rate': 1.195513549418959e-05, 'epoch': 1.37}\n",
      "{'loss': 1.273, 'grad_norm': 12.288043975830078, 'learning_rate': 1.1921267173537085e-05, 'epoch': 1.37}\n",
      "{'loss': 1.1774, 'grad_norm': 11.573018074035645, 'learning_rate': 1.1887375954529167e-05, 'epoch': 1.37}\n",
      "{'loss': 1.5839, 'grad_norm': 16.174631118774414, 'learning_rate': 1.1853462241093614e-05, 'epoch': 1.38}\n",
      "{'loss': 1.2818, 'grad_norm': 15.146202087402344, 'learning_rate': 1.1819526437426298e-05, 'epoch': 1.38}\n",
      "{'loss': 1.2568, 'grad_norm': 19.82988166809082, 'learning_rate': 1.1785568947986368e-05, 'epoch': 1.38}\n",
      "{'loss': 1.3707, 'grad_norm': 13.446076393127441, 'learning_rate': 1.1751590177491441e-05, 'epoch': 1.39}\n",
      "{'loss': 1.3491, 'grad_norm': 11.468859672546387, 'learning_rate': 1.1717590530912764e-05, 'epoch': 1.39}\n",
      "{'loss': 1.2114, 'grad_norm': 12.681286811828613, 'learning_rate': 1.1683570413470384e-05, 'epoch': 1.39}\n",
      "{'loss': 1.1142, 'grad_norm': 10.947165489196777, 'learning_rate': 1.164953023062835e-05, 'epoch': 1.4}\n",
      "{'loss': 1.2448, 'grad_norm': 11.895483016967773, 'learning_rate': 1.1615470388089836e-05, 'epoch': 1.4}\n",
      "{'loss': 1.3299, 'grad_norm': 12.564374923706055, 'learning_rate': 1.1581391291792336e-05, 'epoch': 1.4}\n",
      "{'loss': 1.3339, 'grad_norm': 11.36915111541748, 'learning_rate': 1.1547293347902813e-05, 'epoch': 1.41}\n",
      "{'loss': 1.2896, 'grad_norm': 9.77184772491455, 'learning_rate': 1.151317696281287e-05, 'epoch': 1.41}\n",
      "{'loss': 1.1812, 'grad_norm': 10.19845199584961, 'learning_rate': 1.1479042543133895e-05, 'epoch': 1.41}\n",
      "{'loss': 1.0198, 'grad_norm': 10.84755802154541, 'learning_rate': 1.1444890495692214e-05, 'epoch': 1.42}\n",
      "{'loss': 1.5316, 'grad_norm': 12.288375854492188, 'learning_rate': 1.1410721227524256e-05, 'epoch': 1.42}\n",
      "{'loss': 1.3628, 'grad_norm': 12.093709945678711, 'learning_rate': 1.1376535145871685e-05, 'epoch': 1.42}\n",
      "{'loss': 1.1472, 'grad_norm': 8.779691696166992, 'learning_rate': 1.1342332658176556e-05, 'epoch': 1.43}\n",
      "{'loss': 1.3801, 'grad_norm': 11.820806503295898, 'learning_rate': 1.1308114172076464e-05, 'epoch': 1.43}\n",
      "{'loss': 1.0751, 'grad_norm': 11.215828895568848, 'learning_rate': 1.1273880095399667e-05, 'epoch': 1.43}\n",
      "{'loss': 1.2536, 'grad_norm': 12.569446563720703, 'learning_rate': 1.1239630836160246e-05, 'epoch': 1.44}\n",
      "{'loss': 1.044, 'grad_norm': 11.718851089477539, 'learning_rate': 1.1205366802553231e-05, 'epoch': 1.44}\n",
      "{'loss': 1.2683, 'grad_norm': 12.137988090515137, 'learning_rate': 1.1171088402949739e-05, 'epoch': 1.44}\n",
      "{'loss': 1.2546, 'grad_norm': 11.606438636779785, 'learning_rate': 1.1136796045892102e-05, 'epoch': 1.44}\n",
      "{'loss': 1.2992, 'grad_norm': 10.928889274597168, 'learning_rate': 1.1102490140089009e-05, 'epoch': 1.45}\n",
      "{'loss': 1.3085, 'grad_norm': 10.980263710021973, 'learning_rate': 1.1068171094410618e-05, 'epoch': 1.45}\n",
      "{'loss': 1.1699, 'grad_norm': 12.617344856262207, 'learning_rate': 1.10338393178837e-05, 'epoch': 1.45}\n",
      "{'loss': 1.2908, 'grad_norm': 12.953207015991211, 'learning_rate': 1.0999495219686762e-05, 'epoch': 1.46}\n",
      "{'loss': 1.2591, 'grad_norm': 12.409153938293457, 'learning_rate': 1.0965139209145153e-05, 'epoch': 1.46}\n",
      "{'loss': 1.2499, 'grad_norm': 12.255062103271484, 'learning_rate': 1.0930771695726201e-05, 'epoch': 1.46}\n",
      "{'loss': 1.0251, 'grad_norm': 12.500568389892578, 'learning_rate': 1.0896393089034336e-05, 'epoch': 1.47}\n",
      "{'loss': 0.9517, 'grad_norm': 10.882055282592773, 'learning_rate': 1.0862003798806195e-05, 'epoch': 1.47}\n",
      "{'loss': 0.7289, 'grad_norm': 11.185416221618652, 'learning_rate': 1.0827604234905749e-05, 'epoch': 1.47}\n",
      "{'loss': 0.9362, 'grad_norm': 12.732502937316895, 'learning_rate': 1.079319480731941e-05, 'epoch': 1.48}\n",
      "{'loss': 0.9822, 'grad_norm': 14.405077934265137, 'learning_rate': 1.0758775926151155e-05, 'epoch': 1.48}\n",
      "{'loss': 1.1577, 'grad_norm': 9.376566886901855, 'learning_rate': 1.0724348001617626e-05, 'epoch': 1.48}\n",
      "{'loss': 1.2597, 'grad_norm': 9.016305923461914, 'learning_rate': 1.0689911444043249e-05, 'epoch': 1.49}\n",
      "{'loss': 1.113, 'grad_norm': 7.637360095977783, 'learning_rate': 1.0655466663855349e-05, 'epoch': 1.49}\n",
      "{'loss': 1.1032, 'grad_norm': 7.8353095054626465, 'learning_rate': 1.0621014071579241e-05, 'epoch': 1.49}\n",
      "{'loss': 1.1433, 'grad_norm': 8.10083293914795, 'learning_rate': 1.0586554077833346e-05, 'epoch': 1.5}\n",
      "{'loss': 1.2533, 'grad_norm': 9.012182235717773, 'learning_rate': 1.0552087093324314e-05, 'epoch': 1.5}\n",
      "{'loss': 1.1281, 'grad_norm': 8.551861763000488, 'learning_rate': 1.0517613528842096e-05, 'epoch': 1.5}\n",
      "{'loss': 1.2692, 'grad_norm': 7.48217248916626, 'learning_rate': 1.0483133795255072e-05, 'epoch': 1.51}\n",
      "{'loss': 1.3405, 'grad_norm': 11.505873680114746, 'learning_rate': 1.044864830350515e-05, 'epoch': 1.51}\n",
      "{'loss': 1.232, 'grad_norm': 7.69214391708374, 'learning_rate': 1.0414157464602866e-05, 'epoch': 1.51}\n",
      "{'loss': 1.4087, 'grad_norm': 9.581838607788086, 'learning_rate': 1.0379661689622477e-05, 'epoch': 1.52}\n",
      "{'loss': 1.4832, 'grad_norm': 8.075206756591797, 'learning_rate': 1.0345161389697083e-05, 'epoch': 1.52}\n",
      "{'loss': 1.5156, 'grad_norm': 10.154224395751953, 'learning_rate': 1.0310656976013704e-05, 'epoch': 1.52}\n",
      "{'loss': 1.3188, 'grad_norm': 7.683579444885254, 'learning_rate': 1.027614885980839e-05, 'epoch': 1.52}\n",
      "{'loss': 1.2843, 'grad_norm': 8.176581382751465, 'learning_rate': 1.0241637452361323e-05, 'epoch': 1.53}\n",
      "{'loss': 1.2729, 'grad_norm': 7.1318278312683105, 'learning_rate': 1.0207123164991912e-05, 'epoch': 1.53}\n",
      "{'loss': 1.3942, 'grad_norm': 10.867375373840332, 'learning_rate': 1.0172606409053887e-05, 'epoch': 1.53}\n",
      "{'loss': 1.4767, 'grad_norm': 8.693802833557129, 'learning_rate': 1.0138087595930394e-05, 'epoch': 1.54}\n",
      "{'loss': 1.3594, 'grad_norm': 7.922430038452148, 'learning_rate': 1.0103567137029111e-05, 'epoch': 1.54}\n",
      "{'loss': 1.1736, 'grad_norm': 8.7865571975708, 'learning_rate': 1.0069045443777318e-05, 'epoch': 1.54}\n",
      "{'loss': 1.2287, 'grad_norm': 8.159887313842773, 'learning_rate': 1.0034522927617014e-05, 'epoch': 1.55}\n",
      "{'loss': 1.3807, 'grad_norm': 8.282713890075684, 'learning_rate': 1e-05, 'epoch': 1.55}\n",
      "{'loss': 1.0754, 'grad_norm': 7.653985500335693, 'learning_rate': 9.965477072382989e-06, 'epoch': 1.55}\n",
      "{'loss': 1.1937, 'grad_norm': 8.19426155090332, 'learning_rate': 9.930954556222683e-06, 'epoch': 1.56}\n",
      "{'loss': 1.3317, 'grad_norm': 8.323820114135742, 'learning_rate': 9.896432862970892e-06, 'epoch': 1.56}\n",
      "{'loss': 1.2229, 'grad_norm': 8.50858211517334, 'learning_rate': 9.861912404069608e-06, 'epoch': 1.56}\n",
      "{'loss': 1.4012, 'grad_norm': 8.460016250610352, 'learning_rate': 9.827393590946116e-06, 'epoch': 1.57}\n",
      "{'loss': 1.1533, 'grad_norm': 9.34698486328125, 'learning_rate': 9.79287683500809e-06, 'epoch': 1.57}\n",
      "{'loss': 1.2868, 'grad_norm': 11.304505348205566, 'learning_rate': 9.75836254763868e-06, 'epoch': 1.57}\n",
      "{'loss': 1.2713, 'grad_norm': 9.212244033813477, 'learning_rate': 9.723851140191613e-06, 'epoch': 1.58}\n",
      "{'loss': 1.2737, 'grad_norm': 8.679449081420898, 'learning_rate': 9.689343023986303e-06, 'epoch': 1.58}\n",
      "{'loss': 1.1818, 'grad_norm': 8.672170639038086, 'learning_rate': 9.654838610302922e-06, 'epoch': 1.58}\n",
      "{'loss': 1.3196, 'grad_norm': 9.679801940917969, 'learning_rate': 9.620338310377526e-06, 'epoch': 1.59}\n",
      "{'loss': 1.1597, 'grad_norm': 9.521411895751953, 'learning_rate': 9.58584253539714e-06, 'epoch': 1.59}\n",
      "{'loss': 0.9382, 'grad_norm': 7.134879112243652, 'learning_rate': 9.551351696494854e-06, 'epoch': 1.59}\n",
      "{'loss': 1.3752, 'grad_norm': 10.00033950805664, 'learning_rate': 9.516866204744932e-06, 'epoch': 1.6}\n",
      "{'loss': 1.2705, 'grad_norm': 9.441974639892578, 'learning_rate': 9.482386471157905e-06, 'epoch': 1.6}\n",
      "{'loss': 1.2376, 'grad_norm': 10.583897590637207, 'learning_rate': 9.447912906675687e-06, 'epoch': 1.6}\n",
      "{'loss': 1.1165, 'grad_norm': 9.340893745422363, 'learning_rate': 9.413445922166654e-06, 'epoch': 1.6}\n",
      "{'loss': 0.9609, 'grad_norm': 10.928790092468262, 'learning_rate': 9.378985928420764e-06, 'epoch': 1.61}\n",
      "{'loss': 1.2857, 'grad_norm': 11.480367660522461, 'learning_rate': 9.344533336144653e-06, 'epoch': 1.61}\n",
      "{'loss': 0.9788, 'grad_norm': 10.186474800109863, 'learning_rate': 9.310088555956751e-06, 'epoch': 1.61}\n",
      "{'loss': 0.9063, 'grad_norm': 11.532774925231934, 'learning_rate': 9.275651998382377e-06, 'epoch': 1.62}\n",
      "{'loss': 0.9544, 'grad_norm': 11.324356079101562, 'learning_rate': 9.241224073848848e-06, 'epoch': 1.62}\n",
      "{'loss': 0.9191, 'grad_norm': 15.020618438720703, 'learning_rate': 9.206805192680592e-06, 'epoch': 1.62}\n",
      "{'loss': 1.1094, 'grad_norm': 15.54071044921875, 'learning_rate': 9.172395765094255e-06, 'epoch': 1.63}\n",
      "{'loss': 0.8665, 'grad_norm': 13.814291000366211, 'learning_rate': 9.137996201193807e-06, 'epoch': 1.63}\n",
      "{'loss': 0.9497, 'grad_norm': 12.137360572814941, 'learning_rate': 9.103606910965666e-06, 'epoch': 1.63}\n",
      "{'loss': 0.873, 'grad_norm': 18.46076774597168, 'learning_rate': 9.069228304273802e-06, 'epoch': 1.64}\n",
      "{'loss': 1.0209, 'grad_norm': 15.35365104675293, 'learning_rate': 9.034860790854848e-06, 'epoch': 1.64}\n",
      "{'loss': 1.0481, 'grad_norm': 16.419437408447266, 'learning_rate': 9.00050478031324e-06, 'epoch': 1.64}\n",
      "{'loss': 1.0628, 'grad_norm': 11.144360542297363, 'learning_rate': 8.966160682116301e-06, 'epoch': 1.65}\n",
      "{'loss': 1.1195, 'grad_norm': 10.038145065307617, 'learning_rate': 8.931828905589385e-06, 'epoch': 1.65}\n",
      "{'loss': 1.0799, 'grad_norm': 12.010442733764648, 'learning_rate': 8.897509859910996e-06, 'epoch': 1.65}\n",
      "{'loss': 1.1113, 'grad_norm': 12.675471305847168, 'learning_rate': 8.863203954107902e-06, 'epoch': 1.66}\n",
      "{'loss': 1.1464, 'grad_norm': 10.22140121459961, 'learning_rate': 8.828911597050263e-06, 'epoch': 1.66}\n",
      "{'loss': 1.3199, 'grad_norm': 11.811229705810547, 'learning_rate': 8.79463319744677e-06, 'epoch': 1.66}\n",
      "{'loss': 1.2945, 'grad_norm': 8.870491027832031, 'learning_rate': 8.760369163839759e-06, 'epoch': 1.67}\n",
      "{'loss': 1.4807, 'grad_norm': 12.404671669006348, 'learning_rate': 8.726119904600337e-06, 'epoch': 1.67}\n",
      "{'loss': 1.6355, 'grad_norm': 21.07191276550293, 'learning_rate': 8.691885827923541e-06, 'epoch': 1.67}\n",
      "{'loss': 1.4075, 'grad_norm': 16.662200927734375, 'learning_rate': 8.657667341823449e-06, 'epoch': 1.68}\n",
      "{'loss': 1.3562, 'grad_norm': 11.924481391906738, 'learning_rate': 8.62346485412832e-06, 'epoch': 1.68}\n",
      "{'loss': 1.5037, 'grad_norm': 27.647254943847656, 'learning_rate': 8.58927877247575e-06, 'epoch': 1.68}\n",
      "{'loss': 1.473, 'grad_norm': 14.007772445678711, 'learning_rate': 8.55510950430779e-06, 'epoch': 1.68}\n",
      "{'loss': 1.4613, 'grad_norm': 9.103519439697266, 'learning_rate': 8.520957456866107e-06, 'epoch': 1.69}\n",
      "{'loss': 1.3446, 'grad_norm': 8.46019172668457, 'learning_rate': 8.48682303718713e-06, 'epoch': 1.69}\n",
      "{'loss': 1.3304, 'grad_norm': 17.04161834716797, 'learning_rate': 8.452706652097187e-06, 'epoch': 1.69}\n",
      "{'loss': 1.0564, 'grad_norm': 8.737227439880371, 'learning_rate': 8.418608708207667e-06, 'epoch': 1.7}\n",
      "{'loss': 1.2864, 'grad_norm': 9.248566627502441, 'learning_rate': 8.384529611910164e-06, 'epoch': 1.7}\n",
      "{'loss': 1.1431, 'grad_norm': 9.28956127166748, 'learning_rate': 8.35046976937165e-06, 'epoch': 1.7}\n",
      "{'loss': 1.1424, 'grad_norm': 8.326753616333008, 'learning_rate': 8.316429586529616e-06, 'epoch': 1.71}\n",
      "{'loss': 1.0134, 'grad_norm': 8.102027893066406, 'learning_rate': 8.28240946908724e-06, 'epoch': 1.71}\n",
      "{'loss': 1.232, 'grad_norm': 10.336404800415039, 'learning_rate': 8.24840982250856e-06, 'epoch': 1.71}\n",
      "{'loss': 1.395, 'grad_norm': 10.517169952392578, 'learning_rate': 8.214431052013636e-06, 'epoch': 1.72}\n",
      "{'loss': 1.4685, 'grad_norm': 11.157259941101074, 'learning_rate': 8.180473562573705e-06, 'epoch': 1.72}\n",
      "{'loss': 1.1871, 'grad_norm': 9.809596061706543, 'learning_rate': 8.146537758906388e-06, 'epoch': 1.72}\n",
      "{'loss': 1.1677, 'grad_norm': 8.285682678222656, 'learning_rate': 8.112624045470834e-06, 'epoch': 1.73}\n",
      "{'loss': 1.0703, 'grad_norm': 8.350813865661621, 'learning_rate': 8.078732826462917e-06, 'epoch': 1.73}\n",
      "{'loss': 1.1862, 'grad_norm': 8.20268726348877, 'learning_rate': 8.044864505810415e-06, 'epoch': 1.73}\n",
      "{'loss': 1.1896, 'grad_norm': 8.631599426269531, 'learning_rate': 8.011019487168193e-06, 'epoch': 1.74}\n",
      "{'loss': 1.1475, 'grad_norm': 8.972465515136719, 'learning_rate': 7.977198173913394e-06, 'epoch': 1.74}\n",
      "{'loss': 1.1747, 'grad_norm': 9.158676147460938, 'learning_rate': 7.943400969140635e-06, 'epoch': 1.74}\n",
      "{'loss': 1.1772, 'grad_norm': 8.269600868225098, 'learning_rate': 7.909628275657199e-06, 'epoch': 1.75}\n",
      "{'loss': 1.1791, 'grad_norm': 8.502671241760254, 'learning_rate': 7.875880495978227e-06, 'epoch': 1.75}\n",
      "{'loss': 1.1825, 'grad_norm': 7.876400470733643, 'learning_rate': 7.84215803232194e-06, 'epoch': 1.75}\n",
      "{'loss': 1.2686, 'grad_norm': 14.937850952148438, 'learning_rate': 7.808461286604828e-06, 'epoch': 1.76}\n",
      "{'loss': 1.1288, 'grad_norm': 8.754910469055176, 'learning_rate': 7.774790660436857e-06, 'epoch': 1.76}\n",
      "{'loss': 0.9548, 'grad_norm': 8.698223114013672, 'learning_rate': 7.741146555116708e-06, 'epoch': 1.76}\n",
      "{'loss': 1.0433, 'grad_norm': 9.577380180358887, 'learning_rate': 7.707529371626966e-06, 'epoch': 1.76}\n",
      "{'loss': 1.171, 'grad_norm': 8.71187686920166, 'learning_rate': 7.67393951062935e-06, 'epoch': 1.77}\n",
      "{'loss': 1.2994, 'grad_norm': 10.397917747497559, 'learning_rate': 7.640377372459944e-06, 'epoch': 1.77}\n",
      "{'loss': 1.216, 'grad_norm': 8.506305694580078, 'learning_rate': 7.606843357124426e-06, 'epoch': 1.77}\n",
      "{'loss': 1.3127, 'grad_norm': 10.883910179138184, 'learning_rate': 7.573337864293283e-06, 'epoch': 1.78}\n",
      "{'loss': 1.1389, 'grad_norm': 12.586996078491211, 'learning_rate': 7.539861293297073e-06, 'epoch': 1.78}\n",
      "{'loss': 0.9968, 'grad_norm': 7.906808376312256, 'learning_rate': 7.506414043121647e-06, 'epoch': 1.78}\n",
      "{'loss': 1.0944, 'grad_norm': 9.238296508789062, 'learning_rate': 7.472996512403403e-06, 'epoch': 1.79}\n",
      "{'loss': 0.96, 'grad_norm': 8.32480239868164, 'learning_rate': 7.4396090994245295e-06, 'epoch': 1.79}\n",
      "{'loss': 0.5989, 'grad_norm': 8.581392288208008, 'learning_rate': 7.406252202108258e-06, 'epoch': 1.79}\n",
      "{'loss': 0.9266, 'grad_norm': 11.888463973999023, 'learning_rate': 7.372926218014131e-06, 'epoch': 1.8}\n",
      "{'loss': 0.8507, 'grad_norm': 9.759237289428711, 'learning_rate': 7.33963154433325e-06, 'epoch': 1.8}\n",
      "{'loss': 1.1506, 'grad_norm': 7.300513744354248, 'learning_rate': 7.306368577883547e-06, 'epoch': 1.8}\n",
      "{'loss': 1.1474, 'grad_norm': 7.21986198425293, 'learning_rate': 7.273137715105063e-06, 'epoch': 1.81}\n",
      "{'loss': 1.0494, 'grad_norm': 14.233599662780762, 'learning_rate': 7.239939352055208e-06, 'epoch': 1.81}\n",
      "{'loss': 1.2008, 'grad_norm': 7.819028377532959, 'learning_rate': 7.2067738844040516e-06, 'epoch': 1.81}\n",
      "{'loss': 1.156, 'grad_norm': 9.173150062561035, 'learning_rate': 7.173641707429606e-06, 'epoch': 1.82}\n",
      "{'loss': 1.1182, 'grad_norm': 6.395073413848877, 'learning_rate': 7.140543216013109e-06, 'epoch': 1.82}\n",
      "{'loss': 1.133, 'grad_norm': 9.340950965881348, 'learning_rate': 7.107478804634324e-06, 'epoch': 1.82}\n",
      "{'loss': 1.2833, 'grad_norm': 9.35492992401123, 'learning_rate': 7.07444886736684e-06, 'epoch': 1.83}\n",
      "{'loss': 1.2872, 'grad_norm': 8.26357364654541, 'learning_rate': 7.041453797873363e-06, 'epoch': 1.83}\n",
      "{'loss': 1.3687, 'grad_norm': 7.56323766708374, 'learning_rate': 7.008493989401039e-06, 'epoch': 1.83}\n",
      "{'loss': 1.3177, 'grad_norm': 12.057421684265137, 'learning_rate': 6.975569834776757e-06, 'epoch': 1.84}\n",
      "{'loss': 1.299, 'grad_norm': 8.772522926330566, 'learning_rate': 6.942681726402474e-06, 'epoch': 1.84}\n",
      "{'loss': 1.1792, 'grad_norm': 7.906858444213867, 'learning_rate': 6.909830056250527e-06, 'epoch': 1.84}\n",
      "{'loss': 1.399, 'grad_norm': 7.554241180419922, 'learning_rate': 6.8770152158589806e-06, 'epoch': 1.84}\n",
      "{'loss': 1.3235, 'grad_norm': 10.228592872619629, 'learning_rate': 6.844237596326941e-06, 'epoch': 1.85}\n",
      "{'loss': 1.3706, 'grad_norm': 10.4474515914917, 'learning_rate': 6.811497588309901e-06, 'epoch': 1.85}\n",
      "{'loss': 1.4246, 'grad_norm': 8.3517484664917, 'learning_rate': 6.778795582015096e-06, 'epoch': 1.85}\n",
      "{'loss': 1.4508, 'grad_norm': 8.311480522155762, 'learning_rate': 6.746131967196834e-06, 'epoch': 1.86}\n",
      "{'loss': 1.3497, 'grad_norm': 8.277278900146484, 'learning_rate': 6.7135071331518575e-06, 'epoch': 1.86}\n",
      "{'loss': 1.2716, 'grad_norm': 7.848539352416992, 'learning_rate': 6.680921468714718e-06, 'epoch': 1.86}\n",
      "{'loss': 1.2848, 'grad_norm': 12.787500381469727, 'learning_rate': 6.648375362253119e-06, 'epoch': 1.87}\n",
      "{'loss': 1.2476, 'grad_norm': 7.814301490783691, 'learning_rate': 6.615869201663296e-06, 'epoch': 1.87}\n",
      "{'loss': 1.2492, 'grad_norm': 7.73518705368042, 'learning_rate': 6.583403374365406e-06, 'epoch': 1.87}\n",
      "{'loss': 1.2225, 'grad_norm': 7.878384113311768, 'learning_rate': 6.550978267298893e-06, 'epoch': 1.88}\n",
      "{'loss': 1.4966, 'grad_norm': 9.964056968688965, 'learning_rate': 6.518594266917883e-06, 'epoch': 1.88}\n",
      "{'loss': 1.3193, 'grad_norm': 7.431002616882324, 'learning_rate': 6.486251759186573e-06, 'epoch': 1.88}\n",
      "{'loss': 1.2123, 'grad_norm': 7.275434494018555, 'learning_rate': 6.453951129574644e-06, 'epoch': 1.89}\n",
      "{'loss': 1.1253, 'grad_norm': 10.359984397888184, 'learning_rate': 6.421692763052654e-06, 'epoch': 1.89}\n",
      "{'loss': 1.2249, 'grad_norm': 7.593205451965332, 'learning_rate': 6.3894770440874525e-06, 'epoch': 1.89}\n",
      "{'loss': 1.561, 'grad_norm': 11.667189598083496, 'learning_rate': 6.357304356637606e-06, 'epoch': 1.9}\n",
      "{'loss': 1.255, 'grad_norm': 12.292037963867188, 'learning_rate': 6.325175084148809e-06, 'epoch': 1.9}\n",
      "{'loss': 0.9966, 'grad_norm': 7.687627792358398, 'learning_rate': 6.293089609549325e-06, 'epoch': 1.9}\n",
      "{'loss': 1.2194, 'grad_norm': 7.700528144836426, 'learning_rate': 6.261048315245419e-06, 'epoch': 1.91}\n",
      "{'loss': 1.398, 'grad_norm': 11.076128005981445, 'learning_rate': 6.229051583116796e-06, 'epoch': 1.91}\n",
      "{'loss': 1.3553, 'grad_norm': 11.88124942779541, 'learning_rate': 6.197099794512056e-06, 'epoch': 1.91}\n",
      "{'loss': 1.237, 'grad_norm': 7.710247993469238, 'learning_rate': 6.165193330244144e-06, 'epoch': 1.92}\n",
      "{'loss': 1.1901, 'grad_norm': 12.207176208496094, 'learning_rate': 6.133332570585813e-06, 'epoch': 1.92}\n",
      "{'loss': 1.2343, 'grad_norm': 10.076744079589844, 'learning_rate': 6.101517895265094e-06, 'epoch': 1.92}\n",
      "{'loss': 1.2819, 'grad_norm': 10.389777183532715, 'learning_rate': 6.069749683460765e-06, 'epoch': 1.92}\n",
      "{'loss': 1.2305, 'grad_norm': 9.115442276000977, 'learning_rate': 6.03802831379784e-06, 'epoch': 1.93}\n",
      "{'loss': 1.2833, 'grad_norm': 11.678800582885742, 'learning_rate': 6.006354164343047e-06, 'epoch': 1.93}\n",
      "{'loss': 1.2453, 'grad_norm': 10.674928665161133, 'learning_rate': 5.9747276126003265e-06, 'epoch': 1.93}\n",
      "{'loss': 1.2914, 'grad_norm': 12.290911674499512, 'learning_rate': 5.943149035506337e-06, 'epoch': 1.94}\n",
      "{'loss': 0.9309, 'grad_norm': 7.473154544830322, 'learning_rate': 5.911618809425952e-06, 'epoch': 1.94}\n",
      "{'loss': 0.8642, 'grad_norm': 9.921716690063477, 'learning_rate': 5.880137310147782e-06, 'epoch': 1.94}\n",
      "{'loss': 1.0899, 'grad_norm': 11.355504989624023, 'learning_rate': 5.848704912879699e-06, 'epoch': 1.95}\n",
      "{'loss': 0.7426, 'grad_norm': 10.54151725769043, 'learning_rate': 5.8173219922443516e-06, 'epoch': 1.95}\n",
      "{'loss': 0.9626, 'grad_norm': 9.547293663024902, 'learning_rate': 5.785988922274711e-06, 'epoch': 1.95}\n",
      "{'loss': 1.0452, 'grad_norm': 11.873251914978027, 'learning_rate': 5.754706076409613e-06, 'epoch': 1.96}\n",
      "{'loss': 0.8819, 'grad_norm': 14.011845588684082, 'learning_rate': 5.723473827489301e-06, 'epoch': 1.96}\n",
      "{'loss': 1.0925, 'grad_norm': 9.763715744018555, 'learning_rate': 5.692292547750989e-06, 'epoch': 1.96}\n",
      "{'loss': 1.1582, 'grad_norm': 10.631465911865234, 'learning_rate': 5.66116260882442e-06, 'epoch': 1.97}\n",
      "{'loss': 1.4751, 'grad_norm': 10.212469100952148, 'learning_rate': 5.630084381727434e-06, 'epoch': 1.97}\n",
      "{'loss': 1.5387, 'grad_norm': 10.771257400512695, 'learning_rate': 5.599058236861559e-06, 'epoch': 1.97}\n",
      "{'loss': 1.3264, 'grad_norm': 12.329891204833984, 'learning_rate': 5.5680845440075885e-06, 'epoch': 1.98}\n",
      "{'loss': 1.0582, 'grad_norm': 9.105908393859863, 'learning_rate': 5.537163672321161e-06, 'epoch': 1.98}\n",
      "{'loss': 1.1617, 'grad_norm': 8.528635025024414, 'learning_rate': 5.5062959903283855e-06, 'epoch': 1.98}\n",
      "{'loss': 1.2691, 'grad_norm': 10.00726318359375, 'learning_rate': 5.475481865921441e-06, 'epoch': 1.99}\n",
      "{'loss': 1.2878, 'grad_norm': 12.437085151672363, 'learning_rate': 5.444721666354169e-06, 'epoch': 1.99}\n",
      "{'loss': 1.2689, 'grad_norm': 9.02550220489502, 'learning_rate': 5.414015758237734e-06, 'epoch': 1.99}\n",
      "{'loss': 1.0845, 'grad_norm': 9.598016738891602, 'learning_rate': 5.3833645075362295e-06, 'epoch': 2.0}\n",
      "{'loss': 0.9399, 'grad_norm': 12.952614784240723, 'learning_rate': 5.352768279562315e-06, 'epoch': 2.0}\n",
      "{'loss': 1.1046, 'grad_norm': 19.40254783630371, 'learning_rate': 5.32222743897288e-06, 'epoch': 2.0}\n",
      "{'loss': 0.9925, 'grad_norm': 9.31238842010498, 'learning_rate': 5.2917423497646834e-06, 'epoch': 2.0}\n",
      "{'loss': 1.0531, 'grad_norm': 10.0696439743042, 'learning_rate': 5.2613133752700145e-06, 'epoch': 2.01}\n",
      "{'loss': 1.1095, 'grad_norm': 10.005908966064453, 'learning_rate': 5.230940878152371e-06, 'epoch': 2.01}\n",
      "{'loss': 0.9987, 'grad_norm': 8.558470726013184, 'learning_rate': 5.200625220402139e-06, 'epoch': 2.01}\n",
      "{'loss': 1.0295, 'grad_norm': 8.735945701599121, 'learning_rate': 5.1703667633322575e-06, 'epoch': 2.02}\n",
      "{'loss': 1.0722, 'grad_norm': 8.929398536682129, 'learning_rate': 5.14016586757394e-06, 'epoch': 2.02}\n",
      "{'loss': 1.12, 'grad_norm': 9.480853080749512, 'learning_rate': 5.110022893072361e-06, 'epoch': 2.02}\n",
      "{'loss': 1.1938, 'grad_norm': 9.312661170959473, 'learning_rate': 5.079938199082363e-06, 'epoch': 2.03}\n",
      "{'loss': 1.2419, 'grad_norm': 10.786375045776367, 'learning_rate': 5.049912144164186e-06, 'epoch': 2.03}\n",
      "{'loss': 1.2917, 'grad_norm': 12.095785140991211, 'learning_rate': 5.019945086179192e-06, 'epoch': 2.03}\n",
      "{'loss': 1.2104, 'grad_norm': 11.38571548461914, 'learning_rate': 4.9900373822855805e-06, 'epoch': 2.04}\n",
      "{'loss': 1.1907, 'grad_norm': 13.698524475097656, 'learning_rate': 4.960189388934163e-06, 'epoch': 2.04}\n",
      "{'loss': 1.2675, 'grad_norm': 14.752087593078613, 'learning_rate': 4.930401461864099e-06, 'epoch': 2.04}\n",
      "{'loss': 1.2839, 'grad_norm': 11.668098449707031, 'learning_rate': 4.900673956098644e-06, 'epoch': 2.04}\n",
      "{'loss': 1.1684, 'grad_norm': 8.749882698059082, 'learning_rate': 4.87100722594094e-06, 'epoch': 2.05}\n",
      "{'loss': 1.1841, 'grad_norm': 10.85978889465332, 'learning_rate': 4.841401624969782e-06, 'epoch': 2.05}\n",
      "{'loss': 1.1028, 'grad_norm': 11.039562225341797, 'learning_rate': 4.811857506035407e-06, 'epoch': 2.05}\n",
      "{'loss': 1.0962, 'grad_norm': 9.479867935180664, 'learning_rate': 4.7823752212552855e-06, 'epoch': 2.06}\n",
      "{'loss': 1.3014, 'grad_norm': 11.98455810546875, 'learning_rate': 4.75295512200992e-06, 'epoch': 2.06}\n",
      "{'loss': 1.0584, 'grad_norm': 12.377083778381348, 'learning_rate': 4.7235975589386715e-06, 'epoch': 2.06}\n",
      "{'loss': 1.2081, 'grad_norm': 10.22639274597168, 'learning_rate': 4.694302881935574e-06, 'epoch': 2.07}\n",
      "{'loss': 1.2907, 'grad_norm': 10.16584587097168, 'learning_rate': 4.66507144014515e-06, 'epoch': 2.07}\n",
      "{'loss': 1.1335, 'grad_norm': 8.838982582092285, 'learning_rate': 4.635903581958276e-06, 'epoch': 2.07}\n",
      "{'loss': 1.2493, 'grad_norm': 10.061490058898926, 'learning_rate': 4.606799655008009e-06, 'epoch': 2.08}\n",
      "{'loss': 1.0995, 'grad_norm': 10.535850524902344, 'learning_rate': 4.5777600061654505e-06, 'epoch': 2.08}\n",
      "{'loss': 1.1451, 'grad_norm': 9.053805351257324, 'learning_rate': 4.5487849815356145e-06, 'epoch': 2.08}\n",
      "{'loss': 0.9808, 'grad_norm': 11.06705093383789, 'learning_rate': 4.519874926453303e-06, 'epoch': 2.09}\n",
      "{'loss': 1.0592, 'grad_norm': 8.520135879516602, 'learning_rate': 4.491030185478976e-06, 'epoch': 2.09}\n",
      "{'loss': 1.2225, 'grad_norm': 9.007512092590332, 'learning_rate': 4.462251102394669e-06, 'epoch': 2.09}\n",
      "{'loss': 0.9117, 'grad_norm': 9.583879470825195, 'learning_rate': 4.433538020199882e-06, 'epoch': 2.1}\n",
      "{'loss': 0.962, 'grad_norm': 9.465499877929688, 'learning_rate': 4.404891281107482e-06, 'epoch': 2.1}\n",
      "{'loss': 1.0307, 'grad_norm': 12.826892852783203, 'learning_rate': 4.3763112265396445e-06, 'epoch': 2.1}\n",
      "{'loss': 1.0033, 'grad_norm': 10.05333423614502, 'learning_rate': 4.347798197123777e-06, 'epoch': 2.11}\n",
      "{'loss': 0.9876, 'grad_norm': 7.932883262634277, 'learning_rate': 4.319352532688444e-06, 'epoch': 2.11}\n",
      "{'loss': 1.0711, 'grad_norm': 12.050382614135742, 'learning_rate': 4.290974572259342e-06, 'epoch': 2.11}\n",
      "{'loss': 0.9627, 'grad_norm': 8.358365058898926, 'learning_rate': 4.262664654055247e-06, 'epoch': 2.12}\n",
      "{'loss': 0.9681, 'grad_norm': 8.814803123474121, 'learning_rate': 4.234423115483971e-06, 'epoch': 2.12}\n",
      "{'loss': 1.2023, 'grad_norm': 9.960518836975098, 'learning_rate': 4.206250293138366e-06, 'epoch': 2.12}\n",
      "{'loss': 1.0009, 'grad_norm': 8.831777572631836, 'learning_rate': 4.178146522792296e-06, 'epoch': 2.12}\n",
      "{'loss': 1.0608, 'grad_norm': 11.36906623840332, 'learning_rate': 4.15011213939663e-06, 'epoch': 2.13}\n",
      "{'loss': 1.0516, 'grad_norm': 12.24788761138916, 'learning_rate': 4.12214747707527e-06, 'epoch': 2.13}\n",
      "{'loss': 0.9625, 'grad_norm': 10.550830841064453, 'learning_rate': 4.094252869121153e-06, 'epoch': 2.13}\n",
      "{'loss': 1.1176, 'grad_norm': 10.30018424987793, 'learning_rate': 4.066428647992275e-06, 'epoch': 2.14}\n",
      "{'loss': 1.0342, 'grad_norm': 8.993060111999512, 'learning_rate': 4.038675145307747e-06, 'epoch': 2.14}\n",
      "{'loss': 0.6903, 'grad_norm': 7.975245952606201, 'learning_rate': 4.010992691843829e-06, 'epoch': 2.14}\n",
      "{'loss': 0.9276, 'grad_norm': 10.44146728515625, 'learning_rate': 3.98338161752999e-06, 'epoch': 2.15}\n",
      "{'loss': 0.8189, 'grad_norm': 8.911561965942383, 'learning_rate': 3.955842251444978e-06, 'epoch': 2.15}\n",
      "{'loss': 0.5679, 'grad_norm': 8.66077995300293, 'learning_rate': 3.9283749218128885e-06, 'epoch': 2.15}\n",
      "{'loss': 0.7993, 'grad_norm': 10.426013946533203, 'learning_rate': 3.900979955999271e-06, 'epoch': 2.16}\n",
      "{'loss': 0.7405, 'grad_norm': 11.230121612548828, 'learning_rate': 3.8736576805072165e-06, 'epoch': 2.16}\n",
      "{'loss': 1.1602, 'grad_norm': 12.184996604919434, 'learning_rate': 3.846408420973456e-06, 'epoch': 2.16}\n",
      "{'loss': 0.943, 'grad_norm': 10.260454177856445, 'learning_rate': 3.819232502164499e-06, 'epoch': 2.17}\n",
      "{'loss': 1.0077, 'grad_norm': 10.320455551147461, 'learning_rate': 3.792130247972756e-06, 'epoch': 2.17}\n",
      "{'loss': 1.0058, 'grad_norm': 7.274523735046387, 'learning_rate': 3.7651019814126656e-06, 'epoch': 2.17}\n",
      "{'loss': 1.0207, 'grad_norm': 9.073034286499023, 'learning_rate': 3.738148024616863e-06, 'epoch': 2.18}\n",
      "{'loss': 1.1362, 'grad_norm': 8.30877685546875, 'learning_rate': 3.7112686988323353e-06, 'epoch': 2.18}\n",
      "{'loss': 1.0618, 'grad_norm': 8.419196128845215, 'learning_rate': 3.684464324416578e-06, 'epoch': 2.18}\n",
      "{'loss': 1.1543, 'grad_norm': 8.60595417022705, 'learning_rate': 3.6577352208338015e-06, 'epoch': 2.19}\n",
      "{'loss': 1.1296, 'grad_norm': 7.4001970291137695, 'learning_rate': 3.6310817066511106e-06, 'epoch': 2.19}\n",
      "{'loss': 1.1824, 'grad_norm': 9.545214653015137, 'learning_rate': 3.604504099534696e-06, 'epoch': 2.19}\n",
      "{'loss': 1.5176, 'grad_norm': 9.834617614746094, 'learning_rate': 3.578002716246074e-06, 'epoch': 2.2}\n",
      "{'loss': 1.1879, 'grad_norm': 10.92955207824707, 'learning_rate': 3.5515778726382967e-06, 'epoch': 2.2}\n",
      "{'loss': 1.1276, 'grad_norm': 11.625956535339355, 'learning_rate': 3.525229883652177e-06, 'epoch': 2.2}\n",
      "{'loss': 1.2392, 'grad_norm': 9.181438446044922, 'learning_rate': 3.4989590633125583e-06, 'epoch': 2.2}\n",
      "{'loss': 1.4399, 'grad_norm': 10.990561485290527, 'learning_rate': 3.4727657247245607e-06, 'epoch': 2.21}\n",
      "{'loss': 1.1009, 'grad_norm': 8.660582542419434, 'learning_rate': 3.446650180069837e-06, 'epoch': 2.21}\n",
      "{'loss': 1.0184, 'grad_norm': 7.365581512451172, 'learning_rate': 3.4206127406028744e-06, 'epoch': 2.21}\n",
      "{'loss': 1.0696, 'grad_norm': 7.511904716491699, 'learning_rate': 3.394653716647277e-06, 'epoch': 2.22}\n",
      "{'loss': 1.0935, 'grad_norm': 8.917716979980469, 'learning_rate': 3.3687734175920505e-06, 'epoch': 2.22}\n",
      "{'loss': 1.2478, 'grad_norm': 9.114049911499023, 'learning_rate': 3.342972151887941e-06, 'epoch': 2.22}\n",
      "{'loss': 1.0586, 'grad_norm': 10.553746223449707, 'learning_rate': 3.317250227043746e-06, 'epoch': 2.23}\n",
      "{'loss': 1.2063, 'grad_norm': 10.683267593383789, 'learning_rate': 3.2916079496226407e-06, 'epoch': 2.23}\n",
      "{'loss': 1.0682, 'grad_norm': 7.82099723815918, 'learning_rate': 3.266045625238539e-06, 'epoch': 2.23}\n",
      "{'loss': 1.133, 'grad_norm': 8.731369972229004, 'learning_rate': 3.2405635585524566e-06, 'epoch': 2.24}\n",
      "{'loss': 1.2762, 'grad_norm': 15.09862232208252, 'learning_rate': 3.21516205326885e-06, 'epoch': 2.24}\n",
      "{'loss': 1.0854, 'grad_norm': 7.646235942840576, 'learning_rate': 3.1898414121320277e-06, 'epoch': 2.24}\n",
      "{'loss': 1.0882, 'grad_norm': 7.90578556060791, 'learning_rate': 3.1646019369225277e-06, 'epoch': 2.25}\n",
      "{'loss': 1.1578, 'grad_norm': 7.401391983032227, 'learning_rate': 3.1394439284535206e-06, 'epoch': 2.25}\n",
      "{'loss': 1.1347, 'grad_norm': 7.966800689697266, 'learning_rate': 3.114367686567228e-06, 'epoch': 2.25}\n",
      "{'loss': 1.0686, 'grad_norm': 10.457164764404297, 'learning_rate': 3.089373510131354e-06, 'epoch': 2.26}\n",
      "{'loss': 1.0477, 'grad_norm': 7.873352527618408, 'learning_rate': 3.064461697035506e-06, 'epoch': 2.26}\n",
      "{'loss': 0.9581, 'grad_norm': 7.998051643371582, 'learning_rate': 3.0396325441876627e-06, 'epoch': 2.26}\n",
      "{'loss': 0.9134, 'grad_norm': 7.798986911773682, 'learning_rate': 3.0148863475106315e-06, 'epoch': 2.27}\n",
      "{'loss': 1.2566, 'grad_norm': 8.074917793273926, 'learning_rate': 2.9902234019385056e-06, 'epoch': 2.27}\n",
      "{'loss': 0.9226, 'grad_norm': 10.722527503967285, 'learning_rate': 2.9656440014131737e-06, 'epoch': 2.27}\n",
      "{'loss': 0.9427, 'grad_norm': 12.083124160766602, 'learning_rate': 2.941148438880803e-06, 'epoch': 2.28}\n",
      "{'loss': 0.8939, 'grad_norm': 8.042149543762207, 'learning_rate': 2.9167370062883403e-06, 'epoch': 2.28}\n",
      "{'loss': 1.0812, 'grad_norm': 8.69479751586914, 'learning_rate': 2.8924099945800533e-06, 'epoch': 2.28}\n",
      "{'loss': 0.9456, 'grad_norm': 9.80831241607666, 'learning_rate': 2.8681676936940397e-06, 'epoch': 2.28}\n",
      "{'loss': 1.0379, 'grad_norm': 8.051023483276367, 'learning_rate': 2.8440103925587904e-06, 'epoch': 2.29}\n",
      "{'loss': 0.9719, 'grad_norm': 9.413684844970703, 'learning_rate': 2.8199383790897405e-06, 'epoch': 2.29}\n",
      "{'loss': 1.0981, 'grad_norm': 10.060179710388184, 'learning_rate': 2.795951940185827e-06, 'epoch': 2.29}\n",
      "{'loss': 0.9763, 'grad_norm': 10.1113920211792, 'learning_rate': 2.7720513617260857e-06, 'epoch': 2.3}\n",
      "{'loss': 0.9655, 'grad_norm': 10.4473876953125, 'learning_rate': 2.748236928566238e-06, 'epoch': 2.3}\n",
      "{'loss': 0.9496, 'grad_norm': 12.850913047790527, 'learning_rate': 2.7245089245352864e-06, 'epoch': 2.3}\n",
      "{'loss': 0.7574, 'grad_norm': 10.063583374023438, 'learning_rate': 2.700867632432145e-06, 'epoch': 2.31}\n",
      "{'loss': 0.7814, 'grad_norm': 10.494169235229492, 'learning_rate': 2.6773133340222677e-06, 'epoch': 2.31}\n",
      "{'loss': 0.6831, 'grad_norm': 13.830262184143066, 'learning_rate': 2.6538463100342773e-06, 'epoch': 2.31}\n",
      "{'loss': 0.5775, 'grad_norm': 11.440420150756836, 'learning_rate': 2.6304668401566334e-06, 'epoch': 2.32}\n",
      "{'loss': 0.6661, 'grad_norm': 12.880931854248047, 'learning_rate': 2.607175203034299e-06, 'epoch': 2.32}\n",
      "{'loss': 1.1216, 'grad_norm': 9.455260276794434, 'learning_rate': 2.5839716762654e-06, 'epoch': 2.32}\n",
      "{'loss': 0.9856, 'grad_norm': 7.911375999450684, 'learning_rate': 2.56085653639795e-06, 'epoch': 2.33}\n",
      "{'loss': 0.9509, 'grad_norm': 9.114357948303223, 'learning_rate': 2.5378300589265258e-06, 'epoch': 2.33}\n",
      "{'loss': 0.9654, 'grad_norm': 11.126029014587402, 'learning_rate': 2.514892518288988e-06, 'epoch': 2.33}\n",
      "{'loss': 1.0442, 'grad_norm': 10.003908157348633, 'learning_rate': 2.4920441878632273e-06, 'epoch': 2.34}\n",
      "{'loss': 0.9923, 'grad_norm': 8.652706146240234, 'learning_rate': 2.469285339963892e-06, 'epoch': 2.34}\n",
      "{'loss': 1.085, 'grad_norm': 11.994494438171387, 'learning_rate': 2.4466162458391364e-06, 'epoch': 2.34}\n",
      "{'loss': 1.1904, 'grad_norm': 13.14014720916748, 'learning_rate': 2.4240371756674063e-06, 'epoch': 2.35}\n",
      "{'loss': 1.3826, 'grad_norm': 10.44692611694336, 'learning_rate': 2.401548398554213e-06, 'epoch': 2.35}\n",
      "{'loss': 1.2776, 'grad_norm': 11.278865814208984, 'learning_rate': 2.379150182528909e-06, 'epoch': 2.35}\n",
      "{'loss': 1.1554, 'grad_norm': 10.533470153808594, 'learning_rate': 2.3568427945415163e-06, 'epoch': 2.36}\n",
      "{'loss': 1.2947, 'grad_norm': 9.836421012878418, 'learning_rate': 2.334626500459539e-06, 'epoch': 2.36}\n",
      "{'loss': 1.3781, 'grad_norm': 11.232010841369629, 'learning_rate': 2.3125015650647798e-06, 'epoch': 2.36}\n",
      "{'loss': 1.0177, 'grad_norm': 10.154953956604004, 'learning_rate': 2.290468252050204e-06, 'epoch': 2.36}\n",
      "{'loss': 1.3796, 'grad_norm': 16.770029067993164, 'learning_rate': 2.26852682401679e-06, 'epoch': 2.37}\n",
      "{'loss': 1.4475, 'grad_norm': 12.046056747436523, 'learning_rate': 2.246677542470388e-06, 'epoch': 2.37}\n",
      "{'loss': 1.1143, 'grad_norm': 16.914533615112305, 'learning_rate': 2.224920667818622e-06, 'epoch': 2.37}\n",
      "{'loss': 1.2432, 'grad_norm': 12.661273002624512, 'learning_rate': 2.2032564593677773e-06, 'epoch': 2.38}\n",
      "{'loss': 1.3541, 'grad_norm': 11.138178825378418, 'learning_rate': 2.1816851753197023e-06, 'epoch': 2.38}\n",
      "{'loss': 1.1582, 'grad_norm': 9.724348068237305, 'learning_rate': 2.1602070727687463e-06, 'epoch': 2.38}\n",
      "{'loss': 1.2002, 'grad_norm': 9.721034049987793, 'learning_rate': 2.1388224076986872e-06, 'epoch': 2.39}\n",
      "{'loss': 1.0612, 'grad_norm': 13.810600280761719, 'learning_rate': 2.117531434979675e-06, 'epoch': 2.39}\n",
      "{'loss': 1.1013, 'grad_norm': 10.274882316589355, 'learning_rate': 2.096334408365207e-06, 'epoch': 2.39}\n",
      "{'loss': 1.076, 'grad_norm': 9.950191497802734, 'learning_rate': 2.075231580489098e-06, 'epoch': 2.4}\n",
      "{'loss': 1.0347, 'grad_norm': 9.439678192138672, 'learning_rate': 2.0542232028624585e-06, 'epoch': 2.4}\n",
      "{'loss': 1.1127, 'grad_norm': 10.45068645477295, 'learning_rate': 2.033309525870717e-06, 'epoch': 2.4}\n",
      "{'loss': 0.958, 'grad_norm': 14.401384353637695, 'learning_rate': 2.0124907987706243e-06, 'epoch': 2.41}\n",
      "{'loss': 1.1104, 'grad_norm': 9.415379524230957, 'learning_rate': 1.991767269687278e-06, 'epoch': 2.41}\n",
      "{'loss': 1.0655, 'grad_norm': 13.533660888671875, 'learning_rate': 1.971139185611176e-06, 'epoch': 2.41}\n",
      "{'loss': 0.9132, 'grad_norm': 15.619601249694824, 'learning_rate': 1.9506067923952676e-06, 'epoch': 2.42}\n",
      "{'loss': 1.0211, 'grad_norm': 10.648078918457031, 'learning_rate': 1.930170334752025e-06, 'epoch': 2.42}\n",
      "{'loss': 1.1715, 'grad_norm': 14.409460067749023, 'learning_rate': 1.9098300562505266e-06, 'epoch': 2.42}\n",
      "{'loss': 1.1417, 'grad_norm': 14.02224063873291, 'learning_rate': 1.8895861993135444e-06, 'epoch': 2.43}\n",
      "{'loss': 1.1025, 'grad_norm': 14.46092700958252, 'learning_rate': 1.8694390052146737e-06, 'epoch': 2.43}\n",
      "{'loss': 1.0424, 'grad_norm': 13.028404235839844, 'learning_rate': 1.8493887140754462e-06, 'epoch': 2.43}\n",
      "{'loss': 1.0847, 'grad_norm': 14.046993255615234, 'learning_rate': 1.8294355648624607e-06, 'epoch': 2.44}\n",
      "{'loss': 0.9709, 'grad_norm': 13.750466346740723, 'learning_rate': 1.8095797953845507e-06, 'epoch': 2.44}\n",
      "{'loss': 1.1404, 'grad_norm': 9.754804611206055, 'learning_rate': 1.789821642289945e-06, 'epoch': 2.44}\n",
      "{'loss': 1.1808, 'grad_norm': 10.252853393554688, 'learning_rate': 1.7701613410634367e-06, 'epoch': 2.44}\n",
      "{'loss': 0.9595, 'grad_norm': 45.715396881103516, 'learning_rate': 1.750599126023591e-06, 'epoch': 2.45}\n",
      "{'loss': 1.1885, 'grad_norm': 15.594269752502441, 'learning_rate': 1.731135230319948e-06, 'epoch': 2.45}\n",
      "{'loss': 0.9495, 'grad_norm': 10.916524887084961, 'learning_rate': 1.7117698859302357e-06, 'epoch': 2.45}\n",
      "{'loss': 0.9858, 'grad_norm': 10.418642044067383, 'learning_rate': 1.692503323657617e-06, 'epoch': 2.46}\n",
      "{'loss': 1.1611, 'grad_norm': 9.676848411560059, 'learning_rate': 1.6733357731279375e-06, 'epoch': 2.46}\n",
      "{'loss': 0.829, 'grad_norm': 11.410253524780273, 'learning_rate': 1.6542674627869738e-06, 'epoch': 2.46}\n",
      "{'loss': 0.858, 'grad_norm': 10.25106143951416, 'learning_rate': 1.6352986198977327e-06, 'epoch': 2.47}\n",
      "{'loss': 0.9086, 'grad_norm': 12.180051803588867, 'learning_rate': 1.6164294705377292e-06, 'epoch': 2.47}\n",
      "{'loss': 0.7447, 'grad_norm': 10.351420402526855, 'learning_rate': 1.5976602395962892e-06, 'epoch': 2.47}\n",
      "{'loss': 0.8855, 'grad_norm': 20.452058792114258, 'learning_rate': 1.5789911507718824e-06, 'epoch': 2.48}\n",
      "{'loss': 0.8259, 'grad_norm': 12.324240684509277, 'learning_rate': 1.560422426569449e-06, 'epoch': 2.48}\n",
      "{'loss': 0.9402, 'grad_norm': 13.175338745117188, 'learning_rate': 1.5419542882977367e-06, 'epoch': 2.48}\n",
      "{'loss': 1.0759, 'grad_norm': 8.076598167419434, 'learning_rate': 1.523586956066686e-06, 'epoch': 2.49}\n",
      "{'loss': 0.9529, 'grad_norm': 9.902657508850098, 'learning_rate': 1.5053206487847916e-06, 'epoch': 2.49}\n",
      "{'loss': 0.8685, 'grad_norm': 9.04281234741211, 'learning_rate': 1.4871555841564889e-06, 'epoch': 2.49}\n",
      "{'loss': 1.0274, 'grad_norm': 11.32139778137207, 'learning_rate': 1.4690919786795766e-06, 'epoch': 2.5}\n",
      "{'loss': 1.1334, 'grad_norm': 18.41847801208496, 'learning_rate': 1.4511300476426227e-06, 'epoch': 2.5}\n",
      "{'loss': 1.2599, 'grad_norm': 9.678349494934082, 'learning_rate': 1.433270005122399e-06, 'epoch': 2.5}\n",
      "{'loss': 1.1444, 'grad_norm': 9.974730491638184, 'learning_rate': 1.4155120639813392e-06, 'epoch': 2.51}\n",
      "{'loss': 1.2375, 'grad_norm': 13.125566482543945, 'learning_rate': 1.3978564358649926e-06, 'epoch': 2.51}\n",
      "{'loss': 1.208, 'grad_norm': 9.207242965698242, 'learning_rate': 1.3803033311995072e-06, 'epoch': 2.51}\n",
      "{'loss': 1.2477, 'grad_norm': 9.962656021118164, 'learning_rate': 1.3628529591891181e-06, 'epoch': 2.52}\n",
      "{'loss': 1.114, 'grad_norm': 10.041435241699219, 'learning_rate': 1.345505527813652e-06, 'epoch': 2.52}\n",
      "{'loss': 1.2789, 'grad_norm': 9.576586723327637, 'learning_rate': 1.3282612438260578e-06, 'epoch': 2.52}\n",
      "{'loss': 1.2862, 'grad_norm': 11.846923828125, 'learning_rate': 1.311120312749935e-06, 'epoch': 2.52}\n",
      "{'loss': 1.3148, 'grad_norm': 12.769562721252441, 'learning_rate': 1.2940829388770837e-06, 'epoch': 2.53}\n",
      "{'loss': 1.2267, 'grad_norm': 10.29100227355957, 'learning_rate': 1.2771493252650723e-06, 'epoch': 2.53}\n",
      "{'loss': 1.2786, 'grad_norm': 12.404952049255371, 'learning_rate': 1.2603196737348211e-06, 'epoch': 2.53}\n",
      "{'loss': 1.2384, 'grad_norm': 11.241174697875977, 'learning_rate': 1.2435941848681864e-06, 'epoch': 2.54}\n",
      "{'loss': 1.1897, 'grad_norm': 12.166672706604004, 'learning_rate': 1.2269730580055806e-06, 'epoch': 2.54}\n",
      "{'loss': 1.3005, 'grad_norm': 9.659017562866211, 'learning_rate': 1.2104564912435924e-06, 'epoch': 2.54}\n",
      "{'loss': 1.0067, 'grad_norm': 8.292366981506348, 'learning_rate': 1.19404468143262e-06, 'epoch': 2.55}\n",
      "{'loss': 1.0542, 'grad_norm': 13.296773910522461, 'learning_rate': 1.1777378241745385e-06, 'epoch': 2.55}\n",
      "{'loss': 1.2331, 'grad_norm': 12.398039817810059, 'learning_rate': 1.1615361138203574e-06, 'epoch': 2.55}\n",
      "{'loss': 1.1187, 'grad_norm': 11.00707721710205, 'learning_rate': 1.1454397434679022e-06, 'epoch': 2.56}\n",
      "{'loss': 1.175, 'grad_norm': 12.509212493896484, 'learning_rate': 1.1294489049595247e-06, 'epoch': 2.56}\n",
      "{'loss': 1.0333, 'grad_norm': 9.451119422912598, 'learning_rate': 1.1135637888798101e-06, 'epoch': 2.56}\n",
      "{'loss': 1.1168, 'grad_norm': 12.599231719970703, 'learning_rate': 1.0977845845533009e-06, 'epoch': 2.57}\n",
      "{'loss': 1.1264, 'grad_norm': 8.780135154724121, 'learning_rate': 1.0821114800422482e-06, 'epoch': 2.57}\n",
      "{'loss': 1.2221, 'grad_norm': 11.955119132995605, 'learning_rate': 1.066544662144371e-06, 'epoch': 2.57}\n",
      "{'loss': 1.0591, 'grad_norm': 12.834741592407227, 'learning_rate': 1.0510843163906148e-06, 'epoch': 2.58}\n",
      "{'loss': 1.2057, 'grad_norm': 11.893834114074707, 'learning_rate': 1.0357306270429623e-06, 'epoch': 2.58}\n",
      "{'loss': 1.0825, 'grad_norm': 10.100226402282715, 'learning_rate': 1.020483777092226e-06, 'epoch': 2.58}\n",
      "{'loss': 1.0534, 'grad_norm': 10.302994728088379, 'learning_rate': 1.0053439482558602e-06, 'epoch': 2.59}\n",
      "{'loss': 1.0168, 'grad_norm': 10.157187461853027, 'learning_rate': 9.903113209758098e-07, 'epoch': 2.59}\n",
      "{'loss': 0.9396, 'grad_norm': 10.557306289672852, 'learning_rate': 9.753860744163524e-07, 'epoch': 2.59}\n",
      "{'loss': 1.1653, 'grad_norm': 10.962141036987305, 'learning_rate': 9.605683864619574e-07, 'epoch': 2.6}\n",
      "{'loss': 0.8722, 'grad_norm': 11.127864837646484, 'learning_rate': 9.458584337151811e-07, 'epoch': 2.6}\n",
      "{'loss': 1.0471, 'grad_norm': 11.223523139953613, 'learning_rate': 9.312563914945461e-07, 'epoch': 2.6}\n",
      "{'loss': 1.0452, 'grad_norm': 9.747879981994629, 'learning_rate': 9.167624338324599e-07, 'epoch': 2.6}\n",
      "{'loss': 1.1547, 'grad_norm': 11.648822784423828, 'learning_rate': 9.023767334731426e-07, 'epoch': 2.61}\n",
      "{'loss': 1.1111, 'grad_norm': 10.780269622802734, 'learning_rate': 8.880994618705574e-07, 'epoch': 2.61}\n",
      "{'loss': 0.9914, 'grad_norm': 11.514026641845703, 'learning_rate': 8.739307891863813e-07, 'epoch': 2.61}\n",
      "{'loss': 0.8802, 'grad_norm': 20.184314727783203, 'learning_rate': 8.598708842879688e-07, 'epoch': 2.62}\n",
      "{'loss': 0.9102, 'grad_norm': 9.364696502685547, 'learning_rate': 8.459199147463371e-07, 'epoch': 2.62}\n",
      "{'loss': 1.0278, 'grad_norm': 10.7337064743042, 'learning_rate': 8.320780468341761e-07, 'epoch': 2.62}\n",
      "{'loss': 0.7582, 'grad_norm': 8.56815242767334, 'learning_rate': 8.183454455238638e-07, 'epoch': 2.63}\n",
      "{'loss': 0.7889, 'grad_norm': 11.106283187866211, 'learning_rate': 8.047222744854943e-07, 'epoch': 2.63}\n",
      "{'loss': 0.7079, 'grad_norm': 11.608138084411621, 'learning_rate': 7.912086960849374e-07, 'epoch': 2.63}\n",
      "{'loss': 0.7483, 'grad_norm': 11.032539367675781, 'learning_rate': 7.778048713818975e-07, 'epoch': 2.64}\n",
      " 88% 825/939 [16:30<01:51,  1.02it/s]"
     ]
    }
   ],
   "source": [
    "!accelerate launch train.py --config_file configs/full/pythia-410m_cirriculum_full.yml --wandb_key \"0944191bcf43ea6231189f995e76d66cc523c13d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad62c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'data/CoT/aqua_rat.json': 3460, 'data/CoT/math50k_camel.json': 1899, 'data/CoT/gsm_rft.json': 1060, 'data/PoT/mathqa.json': 934, 'data/PoT/numglue.json': 513, 'data/PoT/gsm_gpt4.json': 511, 'data/CoT/MATH_train.json': 420, 'data/PoT/MATH_train.json': 404, 'data/PoT/aqua_rat_filtered.json': 377, 'data/CoT/gsm_train.json': 289, 'data/CoT/college_math.json': 72, 'data/PoT/TheoremQA.json': 32, 'data/CoT/TheoremQA.json': 18, 'data/CoT/number_comparison.json': 11})\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# from collections import Counter\n",
    "# dataset = load_dataset(\"TIGER-Lab/MathInstruct\",split=\"train[:10000]\")\n",
    "# sources = [example[\"source\"] for example in dataset]\n",
    "\n",
    "# print(Counter(sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f56caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Pythia 410M Trained On Full Dataset\n",
      "Evaluator: res/pythia-410m_cirriculum_full/output initialized!\n",
      "Evaluator: Tokenizer initialized!\n",
      "Evaluator: tokenizer and embedding resize done!\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "A paper company decides to operate their business more sustainably. They decide that for every tree they chop down, they want to plant three more. If the company chops down 200 trees in the first half of the year and 300 more trees in the second half of the year, how many more trees does the company need to plant? Let's write a program.\n",
      "\n",
      "### Response:n0 = 200.0\n",
      "n1 = 300.0\n",
      "t0 = n0 + n1\n",
      "t1 = n0 - n1\n",
      "t2 = t0 / t1\n",
      "answer = n0 * t2\n",
      "print(answer)\n",
      "actual_answer:  # define the numbers of trees chopped down\n",
      "trees_chopped_first_half = 200\n",
      "trees_chopped_second_half = 300\n",
      "# calculate the total number of trees chopped down\n",
      "total_trees_chopped = trees_chopped_first_half + trees_chopped_second_half\n",
      "# calculate the number of trees to be planted\n",
      "trees_to_plant = total_trees_chopped * 3\n",
      "# print the result\n",
      "print(trees_to_plant)\n",
      "extracts:  3 2\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What profit percent is made by selling an article at a certain price, if by selling at 1/2rd of that price, there would be a loss of 20%?\n",
      "Answer Choices: (A) 20% (B) 29% (C) 60% (D) 27% (E) 28%\n",
      "\n",
      "### Response:Let's think about the multi-choice question.\n",
      "Let the price of the article be x.\n",
      "Then,\n",
      "(1/2)x = 20\n",
      "x = 20/2\n",
      "x = 20/2 * 100 = 27%\n",
      "The answer is C\n",
      "actual_answer:  Let's solve the multi-choice question step by step.\n",
      "SP2 = 1/2 SP1\n",
      "CP = 100\n",
      "SP2 = 80\n",
      "1/2 SP1 = 80\n",
      "SP1 = 160\n",
      "100 --- 160 => 60%\n",
      "The answer is C\n",
      "extracts:  60 27\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Bruce and Bhishma are running on a circular track of length 600m. Speed of Bruce is 30 m/s and that of Bhishma is 20 m/s .They start fro the same point at the same time in the same direction. When will they meet again for the first time?\n",
      "Answer Choices: (A) 40 (B) 45 (C) 50 (D) 60 (E) 70\n",
      "\n",
      "### Response:Let's think about the multi-choice question.\n",
      "Speed of Bruce = 30 * 5/18 = 30/5 m/s\n",
      "Speed of Bhishma = 20 * 5/18 = 20/5 m/s\n",
      "Distance covered by Bruce and Bhishma = 600 * 5/18 = 600/5 m\n",
      "Distance covered by Bruce and Bhishma = 600 * 20/5 = 600/20 m\n",
      "Distance covered by Bruce and Bhishma = 600 * 30/20 = 600/30 m\n",
      "Distance covered by Bruce and Bhishma = 600 * 30/20 = 600/30 m\n",
      "Distance covered by Bruce and Bhishma = 600 * 60/30 = 600/60 m\n",
      "Distance covered by Bruce and Bhishma = 600 * 60/20 = 600/60 m\n",
      "Distance covered by Bruce and Bhishma = 600 * 60/20 = 600/60 m\n",
      "Distance covered by Bruce and Bhishma = 600 * 60/20 = 600/60 m\n",
      "Distance covered by Bruce and Bhishma = 600 * 60/20 = 600/60 m\n",
      "Distance covered by Bruce and Bhishma = 600 * 60/20 = 600/60 m\n",
      "Distance covered by Bruce and Bhishma =\n",
      "actual_answer:  Let's think about the multi-choice question step by step.\n",
      "Actually Arjun has to make a lead of 600 m, because when Arjun will be 600m ahead of Bhishma, they will be together again as a person when completes the total length it starts retracing he same path and thus Arjun and Bhishma can be together again.\n",
      "Since , they make a difference of 10 m in 1 second. so, he will create 600m difference in 60 second.\n",
      "The answe is D\n",
      "extracts:  60 60\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "A car takes measurements of its speed every second for 10 seconds, and the results are shown below. Using Lagrange interpolation, estimate the car's speed at 5.5 seconds.\n",
      "\n",
      "Time (s) | Speed (m/s)\n",
      "--------|------------\n",
      "1 | 10\n",
      "2 | 20\n",
      "3 | 30\n",
      "4 | 40\n",
      "5 | 50\n",
      "6 | 60\n",
      "7 | 70\n",
      "8 | 80\n",
      "9 | 90\n",
      "10 | 100\n",
      "\n",
      "### Response:To estimate the car's speed at 5.5 seconds, we need to find the speed at the beginning of each of the 10 seconds. We can do this by using Lagrange interpolation.\n",
      "\n",
      "Speed at the beginning of each of the 10 seconds = Speed at the beginning of the first 10 seconds + Speed at the beginning of the second 10 seconds + Speed at the beginning of the third 10 seconds + Speed at the beginning of the fourth 10 seconds + Speed at the beginning of the fifth 10 seconds + Speed at the beginning of the sixth 10 seconds + Speed at the beginning of the seventh 10 seconds + Speed at the beginning of the eighth 10 seconds + Speed at the beginning of the ninth 10 seconds + Speed at the beginning of the tenth 10 seconds + Speed at the beginning of the twelfth 10 seconds + Speed at the beginning of the thirteenth 10 seconds + Speed at the beginning of the fourteenth 10 seconds + Speed at the beginning of the fiveteenth 10 seconds + Speed at the beginning of the sixteenth 10 seconds + Speed at the beginning of the seven tenth 10 seconds + Speed at the beginning of the eight tenth 10 seconds + Speed at the beginning of the nine tenth 10 seconds + Speed at the beginning of the ten tenth 10 seconds + Speed at the beginning of the\n",
      "actual_answer:  To estimate the car's speed at 5.5 seconds using Lagrange interpolation, we can use the following formula:\n",
      "\n",
      "L(x) = Œ£ [y_i * l_i(x)]\n",
      "\n",
      "where L(x) is the estimated value at x, y_i are the given values, and l_i(x) are the Lagrange basis polynomials defined as:\n",
      "\n",
      "l_i(x) = Œ† [(x - x_j) / (x_i - x_j)] for j ‚â† i\n",
      "\n",
      "Since we have 10 data points, we will have 10 Lagrange basis polynomials. However, to simplify the calculation, we can choose a few neighboring points around 5.5 seconds. Let's use the data points for 4, 5, 6, and 7 seconds.\n",
      "\n",
      "x_values = [4, 5, 6, 7]\n",
      "y_values = [40, 50, 60, 70]\n",
      "\n",
      "Now, we can calculate the Lagrange basis polynomials and the estimated speed at 5.5 seconds:\n",
      "\n",
      "L(5.5) = Œ£ [y_i * l_i(5.5)]\n",
      "\n",
      "l_0(5.5) = [(5.5 - 5) * (5.5 - 6) * (5.5 - 7)] / [(4 - 5) * (4 - 6) * (4 - 7)] = 0.375\n",
      "l_1(5.5) = [(5.5 - 4) * (5.5 - 6) * (5.5 - 7)] / [(5 - 4) * (5 - 6) * (5 - 7)] = 1.125\n",
      "l_2(5.5) = [(5.5 - 4) * (5.5 - 5) * (5.5 - 7)] / [(6 - 4) * (6 - 5) * (6 - 7)] = 0.75\n",
      "l_3(5.5) = [(5.5 - 4) * (5.5 - 5) * (5.5 - 6)] / [(7 - 4) * (7 - 5) * (7 - 6)] = -0.25\n",
      "\n",
      "L(5.5) = 40 * 0.375 + 50 * 1.125 + 60 * 0.75 + 70 * (-0.25) = 15 + 56.25 + 45 - 17.5 = 98.75\n",
      "\n",
      "So, the estimated speed of the car at 5.5 seconds is 98.75 m/s.\n",
      "extracts:  98.75 10\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "If 1 + 9 + 11 = 1, Then what is the value of\n",
      "12 + 11 + 9 = ?\n",
      "Answer Choices: (A) one (B) Ten (C) six (D) five (E) foue\n",
      "\n",
      "### Response:Let's think about the multi-choice question step by step.\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 + 9 + 11 = 1\n",
      "1 +\n",
      "actual_answer:  Let's reason about the multi-choice question.\n",
      "10\n",
      "Equation 1 + 9 + 11 = 1 can be derived from\n",
      "One (o) + nine (n) + eleven (e) = one => 1\n",
      "Similarly for equation,\n",
      "12 + 11 + 9\n",
      "Twelve (t) + eleven (e) + nine (n) => ten (10)\n",
      "The answer is B\n",
      "extracts:  10 1\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Find the area of a parallelogram with base 24 cm and height 16 cm?\n",
      "Answer Choices: (A) 298 cm2 (B) 384 cm2 (C) 278 cm2 (D) 286 cm2 (E) 276 cm2\n",
      "\n",
      "### Response:Let's think about the multi-choice question step by step.\n",
      "Area of parallelogram = (24 * 16) / 2 = 384 cm2\n",
      "The answer is C\n",
      "actual_answer:  Let's think about the multi-choice question step by step.\n",
      "Area of a parallelogram = base * height = 24 * 16\n",
      "= 384 cm2\n",
      "The answer is B\n",
      "extracts:  2 2\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "A conical tank has radius 4 feet and height 8 feet. How much water in cubic feet can the tank hold? (Use integration to find the volume of the solid).\n",
      "\n",
      "### Response:To solve this problem, we can use the formula for volume of a conical tank:\n",
      "\n",
      "Volume = (4 * œÄ * 2œÄ) * (8 * œÄ)\n",
      "\n",
      "In this case, we have:\n",
      "\n",
      "Volume = (4 * œÄ * 2œÄ) * (8 * œÄ)\n",
      "\n",
      "Volume = (4 * œÄ * 2œÄ) * (8 * œÄ)\n",
      "\n",
      "Volume = (4 * œÄ * 2œÄ) * (8 * œÄ)\n",
      "\n",
      "Volume = (4 * œÄ * 2œÄ) * (8 * œÄ)\n",
      "\n",
      "Volume = (4 * œÄ * 2œÄ) * (8 * œÄ)\n",
      "\n",
      "Volume = (4 * œÄ * 2œÄ) * (8 * œÄ)\n",
      "\n",
      "Volume = (4 * œÄ * 2œÄ) * (8 * œÄ)\n",
      "\n",
      "Volume = (4 * œÄ * 2œÄ) * (8 * œÄ)\n",
      "\n",
      "Volume = (4 * œÄ * 2œÄ) * (8 * œÄ)\n",
      "\n",
      "Volume = (4 * œÄ * 2œÄ) * (8 * œÄ)\n",
      "\n",
      "Volume = (4 * œÄ * 2œÄ) * (8 * œÄ)\n",
      "\n",
      "Volume = (4 * œÄ * 2œÄ) * (\n",
      "actual_answer:  To find the volume of the conical tank using integration, we first need to set up an equation for the radius of the cone as a function of its height. Since the radius is 4 feet and the height is 8 feet, we can use similar triangles to find the equation for the radius r as a function of the height h:\n",
      "\n",
      "r(h) = (1/2)h\n",
      "\n",
      "Now, we can set up an integral to find the volume of the cone. We will integrate the area of each circular cross-section of the cone with respect to height. The area of a circle is given by:\n",
      "\n",
      "A(h) = œÄ[r(h)]^2 = œÄ[(1/2)h]^2 = (œÄ/4)h^2\n",
      "\n",
      "Now, we integrate A(h) with respect to h from 0 to 8:\n",
      "\n",
      "V = ‚à´[A(h) dh] from 0 to 8 = ‚à´[(œÄ/4)h^2 dh] from 0 to 8\n",
      "\n",
      "V = (œÄ/4)‚à´[h^2 dh] from 0 to 8\n",
      "\n",
      "Now, we find the antiderivative of h^2:\n",
      "\n",
      "‚à´[h^2 dh] = (1/3)h^3 + C\n",
      "\n",
      "Now, we evaluate the definite integral:\n",
      "\n",
      "V = (œÄ/4)[(1/3)(8)^3 - (1/3)(0)^3]\n",
      "V = (œÄ/4)(1/3)(512)\n",
      "V = (128œÄ/3) cubic feet\n",
      "\n",
      "So, the conical tank can hold 128œÄ/3 cubic feet of water.\n",
      "extracts:  3 2\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "At a certain resort, each of the 39 food service employees is trained to work in a minimum of 1 restaurant and a maximum of 3 restaurants. The 3 restaurants are the family buffet, the dining room, and the snack bar. Exactly 17 employees are trained to work in the family buffet, 18 are trained to work in the dining room, and 12 are trained to work in the snack bar. If 4 employees are trained to work in exactly 2 restaurants, how many employees are trained to work in all 3 restaurants?\n",
      "Answer Choices: (A) 2 (B) 3 (C) 4 (D) 5 (E) 6\n",
      "\n",
      "### Response:Let's think about the multi-choice question step by step.\n",
      "Total number of employees = 39\n",
      "Required number of employees = (39 - 17) = 18\n",
      "Required number of employees = 18 - (39 - 18) = 6\n",
      "Required number of employees = (6 - 3) = 3\n",
      "Required number of employees = 3\n",
      "The answer is B\n",
      "actual_answer:  Let's think about the multi-choice question.\n",
      "39 = 17+ 18 + 12 - 4 - 2x\n",
      "2x = 17 + 18 + 12 - 4 - 39\n",
      "= 43 - 39\n",
      "= 4\n",
      "x = 2\n",
      "The answe is A\n",
      "extracts:  2 3\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Alyssa and Chelsea jointly bought 40 candles to decorate their room. Alyssa used half of them, and Chelsea used 70% of the remaining candles. How many candles are there in the room now?\n",
      "\n",
      "### Response:Alyssa used 40/2=20 candles\n",
      "Chelsea used 70/2=30 candles\n",
      "The total number of candles in the room is 40+20+30=70\n",
      "The answer is 70\n",
      "actual_answer:  When Alyssa used half of the candles, the number of candles left was 1/2*40 = 20 candles.\n",
      "If Chelsea used 70% of the remaining candles, she used 70/100*20 = 14 candles.\n",
      "The total number of candles in the room now is 20-14 = 6\n",
      "The answer is 6\n",
      "extracts:  6 70\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Compute $\\gcd(83^9+1,83^9+83^2+1)$.\n",
      "\n",
      "### Response:To compute $\\gcd(83^9+1,83^9+83^2+1)$, we can use the formula:\n",
      "\n",
      "$\\gcd(a,b) = \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b) \\cdot \\gcd(a,b\n",
      "actual_answer:  Notice that $83^9+1$ and $83^9+83^2+1$ differ by $83^2$. Therefore, if they have a common divisor, then that divisor must also be a divisor of $83^2$. (To see why this is true, suppose $d$ is a divisor of $83^9+1$, so that $83^9+1 = dm$ for some integer $m$; also suppose that $d$ is a divisor of $83^9+83^2+1$, so that $83^9+83^2+1=dn$ for some integer $n$. Then $83^2=d(n-m)$.)\n",
      "\n",
      "Since $83$ is prime, the only (positive) divisors of $83^2$ are $1$, $83$, and $83^2$ itself. But $83$ cannot be a divisor of $83^9+1$ (which is clearly $1$ more than a multiple of $83$). Therefore, $\\gcd(83^9+1,83^9+83^2+1)=\\boxed{1}$. The answer is 1\n",
      "extracts:  1 +1\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "If X+Y = 2X-2Z, X-2Y = 4Z and X+Y+Z = 21, what is the value of Z/Y?\n",
      "Answer Choices: (A) -4.5. (B) -2. (C) -0.5. (D) 3. (E) 2.5.\n",
      "\n",
      "### Response:Let's solve the multi-choice question step by step.\n",
      "X+Y = 2X-2Z\n",
      "X-2Y = 4Z\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X+Y+Z = 21\n",
      "X\n",
      "actual_answer:  X+Y = 2X-2Z\n",
      "X-Y = 2Z---------- 1\n",
      "X-2Y = 4Z--------- 2\n",
      "Subtracting equation 1 from equation 2\n",
      "2Z = -Y\n",
      "Z/Y= -0.5\n",
      "C is the answer\n",
      "extracts:  -0.5 21\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "An entrepreneurship competition requires registering teams to have 3 team members, at least one of which must be a technology co-founder. If all team members must come from the auditorium during the meet and greet event which has 4 technologists and 4 businessmen, how many possible team submissions are possible?\n",
      "Answer Choices: (A) 52 (B) 100 (C) 162 (D) 198 (E) 202\n",
      "\n",
      "### Response:Let's think about the multi-choice question step by step.\n",
      "Required number of team members = 3 team members + 1 co-founding team member = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members = 4 team members + 4 businessmen\n",
      "Required number of team members =\n",
      "actual_answer:  Let's reason about the multi-choice question.\n",
      "We have 3 scenarios here:\n",
      "1) 1 tech2 businessmen: 4C1 x 4C2 =24\n",
      "2) 2 tech1 businessman: 4C2 x 4C1 = 24\n",
      "3) 3 tech0 businessmen: 4C3 = 4\n",
      "Total:24+24+4 = 52\n",
      "The answer is A\n",
      "extracts:  52 4\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "An artist wishes to paint a circular region on a square poster that is 4 feet on a side. If the area of the circular region is to be 1/2 the area of the poster, what must be the radius of the circular region in feet?\n",
      "Answer Choices: (A) 1/pi (B) sqrt (8/pi)  (C) 1 (D) 2/sqrt (pi)  (E) pi/2\n",
      "\n",
      "### Response:Let's think about the multi-choice question.\n",
      "Area of the circular region = 1/2 * 4 = 4 * 4 = 16 square feet\n",
      "Radius of the circular region = 4 * 4 = 16 * 4 = 64 feet\n",
      "The answer is B\n",
      "actual_answer:  Area of the poster is 4x4 = 16\n",
      "1/2 the area = 8\n",
      "pi*r^2 = 8\n",
      "r^2 = 8/pi\n",
      "r = sqrt(8/pi) answer : C\n",
      "extracts:  8 64\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "A circle is inscribed in a Triangle of side 6cm. And a square is inscribed in the Circle. What is the area of square?\n",
      "Answer Choices: (A) 4 cm^2 (B) 6 cm^2 (C) 5 cm^2 (D) 7 cm^2 (E) 8 cm^2\n",
      "\n",
      "### Response:Let's think about the multi-choice question.\n",
      "Area of square = (6 * 6 * 6) / 2 = 4 * 6 * 6 = 8 cm^2\n",
      "The answer is E\n",
      "actual_answer:  Let's think about the multi-choice question.\n",
      "the radius of the incircle in a equilateral triangle is a/2(sqrt)3 where a=side of triangle=6 cm\n",
      "so the diameter of the circle would be the diagonal of square\n",
      "so diagonal=2*a/2(sqrt)3=a/(sqrt)3\n",
      "so area of square=1/2*(diagonal)^2=6*6/2*3=6 cm^2\n",
      "The answer is B\n",
      "extracts:  2 2\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Suppose the manifold M is a sphere of radius 5 centered at the origin in 3-dimensional Euclidean space. Find all the symmetries of M.\n",
      "\n",
      "### Response:To find all the symmetries of M, we can use the symmetries of the sphere. The symmetries of a sphere are the symmetries of the sphere itself, which are rotations around the origin and reflections about the origin.\n",
      "\n",
      "Rotations around the origin are:\n",
      "\n",
      "1. The reflection about the origin is the same as the reflection about the origin in 3-dimensional Euclidean space.\n",
      "2. The reflection about the origin is the same as the reflection about the origin in 3-dimensional Euclidean space.\n",
      "\n",
      "Reflections about the origin are:\n",
      "\n",
      "1. The origin is at the origin in 3-dimensional Euclidean space.\n",
      "2. The origin is at the origin in 3-dimensional Euclidean space.\n",
      "3. The origin is at the origin in 3-dimensional Euclidean space.\n",
      "\n",
      "Therefore, the symmetries of the sphere are:\n",
      "\n",
      "1. Rotations around the origin:\n",
      "2. Reflections about the origin:\n",
      "3. Reflections about the origin are the same as the reflections about the origin in 3-dimensional Euclidean space.\n",
      "\n",
      "Therefore, the symmetries of the sphere are:\n",
      "\n",
      "1. Rotations around the origin:\n",
      "2. Reflections about the origin:\n",
      "3. Reflections about the origin are the same as the reflections\n",
      "actual_answer:  The symmetries of a sphere in 3-dimensional Euclidean space can be described by the group of orthogonal transformations that preserve the sphere. These transformations can be rotations and reflections. In the case of a sphere of radius 5 centered at the origin, the symmetries are as follows:\n",
      "\n",
      "1. Identity transformation: This is the transformation that leaves every point on the sphere unchanged.\n",
      "\n",
      "2. Rotations: A sphere has rotational symmetry about any axis passing through its center. In this case, the sphere can be rotated by any angle about any axis passing through the origin, and the sphere will remain unchanged. These rotations can be described by 3x3 orthogonal matrices with determinant 1.\n",
      "\n",
      "3. Reflections: A sphere also has reflection symmetry with respect to any plane passing through its center. In this case, the sphere can be reflected across any such plane, and the sphere will remain unchanged. These reflections can be described by 3x3 orthogonal matrices with determinant -1.\n",
      "\n",
      "4. Rotoreflections: These are combined operations of rotation followed by reflection. They can also be described by 3x3 orthogonal matrices with determinant -1.\n",
      "\n",
      "In summary, the symmetries of a sphere of radius 5 centered at the origin in 3-dimensional Euclidean space are given by the orthogonal transformations (rotations, reflections, and rotoreflections) that preserve the sphere. These transformations form the orthogonal group O(3), which is the group of all 3x3 orthogonal matrices with determinant 1 or -1.\n",
      "extracts:  -1 3\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "If e is a positive integer, then 3^e + 3^(e +1) =\n",
      "Answer Choices: (A) 4^a (B) 3^a ‚àí 1 (C) 3^ (2a)  + 1 (D) 3^a* (a ‚àí 1)  (E) 4 (3^e) \n",
      "\n",
      "### Response:Let's think about the multi-choice question step by step.\n",
      "3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) = 3^e + 3^(e +1) =\n",
      "actual_answer:  3^e + 3^(e +1)\n",
      "= 3^e + (3^e * 3^1)\n",
      "= 3^e (1 + 3)\n",
      "= 3^e(4)\n",
      "answer E 4(3^e)\n",
      "extracts:  3 +1\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "The sum of the ages of Jeremy, Sebastian and Sophia in three years is 150. Currently, Sebastian is 4 years older than Jeremy. If Jeremy's age is 40, calculate Sophia's age three years from now?\n",
      "\n",
      "### Response:Sophia's age is 3 years + 4 years = 6 years.\n",
      "Sophia's age is 4 years + 6 years = 8 years.\n",
      "Sophia's age is 8 years + 4 years = 12 years.\n",
      "The answer is 12\n",
      "actual_answer:  The age of Jeremy three years from now will be 40+3 = 43\n",
      "If Sebastian age is four years more than Jeremy's, Sebastian is currently 40+4 = 44 years old.\n",
      "In three years, Sebastian will be 44+3 = 47 years old.\n",
      "The age of Sophia three years from now will be 150-43-47 = 60\n",
      "The answer is 60\n",
      "extracts:  60 12\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Given that  ùëé is a primitive root of the prime number  ùëù=23 , and that  ùëè=12 is a residue modulo  ùëù , find an integer  ùë•  such that  ùëéùë•‚â°ùëè(mod 23) .\n",
      "\n",
      "### Response:To find an integer  ùë• such that  ùëéùë•‚â°ùëè(mod 23) , we can use the formula:\n",
      "\n",
      "ùë• = (ùëé - 1) mod 23\n",
      "\n",
      "In this case, the prime number  ùëé = 23 , and the prime number  ùëè = 12. So, the prime number  ùëéùë• ‚â° 12 mod 23 = 12.\n",
      "\n",
      "Now, we can find the residue modulo 23:\n",
      "\n",
      "ùëé - 1 = 12\n",
      "\n",
      "Now, we can find the remainder modulo 23:\n",
      "\n",
      "ùëéùë• ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â° 12 ‚â°\n",
      "actual_answer:  To find an integer x such that a^x ‚â° b (mod 23), we need to use the discrete logarithm. Since a is a primitive root of the prime number p = 23, we know that a^x will generate all the non-zero residues modulo 23. \n",
      "\n",
      "We can use trial and error to find the primitive root a. Let's try a = 2:\n",
      "\n",
      "2^1 ‚â° 2 (mod 23)\n",
      "2^2 ‚â° 4 (mod 23)\n",
      "2^3 ‚â° 8 (mod 23)\n",
      "2^4 ‚â° 16 (mod 23)\n",
      "2^5 ‚â° 9 (mod 23)\n",
      "2^6 ‚â° 18 (mod 23)\n",
      "2^7 ‚â° 13 (mod 23)\n",
      "2^8 ‚â° 3 (mod 23)\n",
      "2^9 ‚â° 6 (mod 23)\n",
      "2^10 ‚â° 12 (mod 23)\n",
      "...\n",
      "\n",
      "We see that 2^10 ‚â° 12 (mod 23), which means that a = 2 and x = 10 satisfy the given condition. Therefore, the integer x we are looking for is x = 10.\n",
      "extracts:  10 12\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "a grocer has a sale of rs . 6435 , rs . 6927 , rs . 6855 , rs . 7230 and rs . 6562 for 5 consecutive months . how much sale must he have in the sixth month so that he gets an average sale of rs . 7000 ? Let's write a program.\n",
      "\n",
      "### Response:n0 = 6435.0\n",
      "n1 = 6927.0\n",
      "n2 = 6855.0\n",
      "n3 = 7230.0\n",
      "n4 = 6562.0\n",
      "n5 = 7000.0\n",
      "t0 = n0 + n1\n",
      "t1 = n2 + n3\n",
      "t2 = n4 + n5\n",
      "t3 = t0 - t1\n",
      "t4 = t2 / t3\n",
      "answer = t4 * t3\n",
      "print(answer)\n",
      "actual_answer:  n0 = 6435.0\n",
      "n1 = 6927.0\n",
      "n2 = 6855.0\n",
      "n3 = 7230.0\n",
      "n4 = 6562.0\n",
      "n5 = 5.0\n",
      "n6 = 7000.0\n",
      "t0 = n5 + 1.0\n",
      "t1 = n0 + n1\n",
      "t2 = n2 + t1\n",
      "t3 = n6 * t0\n",
      "t4 = n3 + t2\n",
      "t5 = n4 + t4\n",
      "answer = t3 - t5\n",
      "print(answer)\n",
      "extracts:  5 3\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "A park has 8 entrances and 8 exits. In how many ways can a person enter and exit the park if they must exit from a different entrance than they entered from?\n",
      "\n",
      "### Response:There are 8 * 8 = 128 ways to enter the park.\n",
      "There are 8 * 8 = 32 ways to exit the park.\n",
      "The answer is 32\n",
      "actual_answer:  If a person enters from one of the 8 entrances, they have 7 remaining options for exits since they cannot exit from the entrance they entered from. So, for each of the 8 entrances, there are 7 possible exits.\n",
      "\n",
      "Therefore, the total number of ways a person can enter and exit the park is 8 entrances * 7 exits = 56 ways.\n",
      "extracts:  56 32\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "A salesman commission is 10% on all sales upto $5000 and 5% on all sales exceeding this. He remits $15000 to his parent company after deducting his commission. Find the total sales?\n",
      "Answer Choices: (A) 15456 (B) 14758 (C) 15789 (D) 13250 (E) 12450\n",
      "\n",
      "### Response:Let's think about the multi-choice question.\n",
      "Let the sales of the company be Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs. Rs.\n",
      "actual_answer:  Let's think about the multi-choice question.\n",
      "Let his total sales be x\n",
      "Total sales - commission = $15000\n",
      "x - [(10% of 5000)+5% of (x-5000)] = 15000\n",
      "95x/100 = 15000\n",
      "x = 15789 approximately\n",
      "The answer is C\n",
      "extracts:  15789 12450\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "A 4\" cube is painted in all its faces and then it is cut down into 1\" blocks. How many 1\" blocks\n",
      "are there even without a single face being painted?\n",
      "Answer Choices: (A) 7 blocks (B) 8 blocks (C) 9 blocks (D) 10 blocks (E) 11 blocks\n",
      "\n",
      "### Response:Let's think about the multi-choice question.\n",
      "4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4*4\n",
      "actual_answer:  Let's think about the multi-choice question step by step.\n",
      "4\" cube is painted in all its faces and then it is cut down into 1\" blocks.\n",
      "Totally, 64 - 1\" blocks\n",
      "3 sides Painted are - 8 blocks\n",
      "2 sides Painted are - 24 blocks\n",
      "1 sides Painted are - 24 blocks\n",
      "without a single face being Painted are - 8 blocks\n",
      "The answer is B\n",
      "extracts:  8 4\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "If $x + y = 16$ and $x-y = 2$, what is the value of $x^2 - y^2$? Let's write a Python program to solve it.\n",
      "\n",
      "### Response:import sympy as sp\n",
      "\n",
      "# define the variables\n",
      "x, y = sp.symbols('x y')\n",
      "\n",
      "# define the equation\n",
      "eq = sp.Eq(x + y, 16)\n",
      "\n",
      "# solve the equation\n",
      "solutions = sp.solve(eq, x, y)\n",
      "\n",
      "# calculate the value of x^2 - y^2\n",
      "value = solutions[0] - solutions[1]\n",
      "\n",
      "# print the result\n",
      "print(value)\n",
      "actual_answer:  import sympy as sp\n",
      "\n",
      "# define the variables\n",
      "x, y = sp.symbols('x y')\n",
      "\n",
      "# define the equations\n",
      "eq1 = sp.Eq(x + y, 16)\n",
      "eq2 = sp.Eq(x - y, 2)\n",
      "\n",
      "# solve the system of equations\n",
      "solution = sp.solve((eq1, eq2), (x, y))\n",
      "\n",
      "# calculate x^2 - y^2\n",
      "diff = solution[x]**2 - solution[y]**2\n",
      "print(diff)\n",
      "extracts:  2 1\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Let matrix A be \n",
      "\n",
      "    [ 1 3 ]  \n",
      "    [ 3 1 ]\n",
      "\n",
      "Using power iteration method, find the dominant eigenvalue and eigenvector of A given the initial approximation x0 = [1, 1] and stopping criterion ||Ax - Œªx|| < 10^-6.\n",
      "\n",
      "### Response:To find the dominant eigenvalue and eigenvector of A, we first need to find the eigenvalues and eigenvectors of A.\n",
      "\n",
      "First, let's find the eigenvalues of A:\n",
      "\n",
      "1. Eigenvalue of A = |1 - Œª|\n",
      "2. Eigenvalue of A = |1 - Œª| * |A - Œª|\n",
      "3. Eigenvalue of A = |1 - Œª| * |A - Œª| * |A - Œª|\n",
      "\n",
      "Now, let's find the eigenvectors of A:\n",
      "\n",
      "1. Eigenvector of A = |1 - Œª| * |A - Œª|\n",
      "2. Eigenvector of A = |1 - Œª| * |A - Œª| * |A - Œª|\n",
      "3. Eigenvector of A = |1 - Œª| * |A - Œª| * |A - Œª| * |A - Œª|\n",
      "\n",
      "Now, let's find the eigenvectors of A:\n",
      "\n",
      "1. Eigenvector of A = |1 - Œª| * |A - Œª|\n",
      "2. Eigenvector of A = |1 - Œª| * |A - Œª| * |A - Œª|\n",
      "3. Eigenvector of A = |1 - Œª| * |A - Œª\n",
      "actual_answer:  To find the dominant eigenvalue and eigenvector of matrix A using the power iteration method, we will follow these steps:\n",
      "\n",
      "1. Start with an initial approximation x0 = [1, 1].\n",
      "2. Compute the matrix-vector product Ax.\n",
      "3. Normalize the resulting vector.\n",
      "4. Compute the Rayleigh quotient Œª = (x^T * Ax) / (x^T * x).\n",
      "5. Check the stopping criterion ||Ax - Œªx|| < 10^-6. If it is not satisfied, update x0 with the normalized vector and repeat steps 2-5.\n",
      "\n",
      "Let's perform these steps:\n",
      "\n",
      "Matrix A:\n",
      "[ 1 3 ]\n",
      "[ 3 1 ]\n",
      "\n",
      "Initial approximation x0 = [1, 1]\n",
      "\n",
      "Step 1:\n",
      "Ax = [ 1*1 + 3*1, 3*1 + 1*1 ] = [ 4, 4 ]\n",
      "\n",
      "Step 2:\n",
      "Normalize Ax: ||Ax|| = sqrt(4^2 + 4^2) = sqrt(32)\n",
      "Normalized Ax = [ 4/sqrt(32), 4/sqrt(32) ] = [ 2/sqrt(8), 2/sqrt(8) ]\n",
      "\n",
      "Step 3:\n",
      "Rayleigh quotient Œª = (x^T * Ax) / (x^T * x) = ( [1, 1] * [4, 4] ) / ( [1, 1] * [1, 1] ) = (4 + 4) / (1 + 1) = 4\n",
      "\n",
      "Step 4:\n",
      "Check stopping criterion: ||Ax - Œªx|| = ||[ 4, 4 ] - 4 * [1, 1]|| = ||[ 0, 0 ]|| = 0 < 10^-6\n",
      "\n",
      "Since the stopping criterion is satisfied, we have found the dominant eigenvalue and eigenvector:\n",
      "\n",
      "Dominant eigenvalue Œª = 4\n",
      "Dominant eigenvector x = [ 2/sqrt(8), 2/sqrt(8) ]\n",
      "extracts:  8 1\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "A train 350 m long passed a pole in 35 sec. How long will it take to pass a platform 650 m long?\n",
      "Answer Choices: (A) 100 sec (B) 89 sec (C) 85 sec (D) 16 sec (E) 15 sec\n",
      "\n",
      "### Response:Let's think about the multi-choice question.\n",
      "Speed = 350/35 = 10 m/sec.\n",
      "Time taken to pass the platform = 650/10 = 16 sec.\n",
      "The answer is D\n",
      "actual_answer:  Let's think about the multi-choice question step by step.\n",
      "Speed = 350/35 = 10 m/sec.\n",
      "Required time = (350 + 650)/10\n",
      "= 100 sec.\n",
      "The answer is A\n",
      "extracts:  100 16\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the perimeter of the Sierpinski carpet after the 6th iteration if the original square had a side length of 27 cm?\n",
      "\n",
      "### Response:The Sierpinski carpet has a side length of 27 cm.\n",
      "The perimeter of the carpet after the 6th iteration is 27 * 6 = 432 square centimeters.\n",
      "The answer is 432\n",
      "actual_answer:  The Sierpinski carpet is a fractal that is formed by recursively removing the central square from each square in the pattern. After each iteration, the remaining area is divided into 8 smaller squares.\n",
      "\n",
      "Let's find the side length of the squares after the 6th iteration. The side length of the squares is divided by 3 after each iteration. So, after the 6th iteration, the side length will be:\n",
      "\n",
      "Side length after 6th iteration = (Original side length) / (3^6)\n",
      "Side length after 6th iteration = 27 cm / (729)\n",
      "Side length after 6th iteration = 0.037037 cm\n",
      "\n",
      "After the 6th iteration, there will be 8^6 squares remaining:\n",
      "\n",
      "Number of squares = 8^6 = 262144\n",
      "\n",
      "Now, let's find the perimeter of each small square:\n",
      "\n",
      "Perimeter of each small square = 4 √ó (Side length after 6th iteration)\n",
      "Perimeter of each small square = 4 √ó 0.037037 cm\n",
      "Perimeter of each small square ‚âà 0.148148 cm\n",
      "\n",
      "Finally, let's find the total perimeter of the Sierpinski carpet after the 6th iteration:\n",
      "\n",
      "Total perimeter = (Number of squares) √ó (Perimeter of each small square)\n",
      "Total perimeter = 262144 √ó 0.148148 cm\n",
      "Total perimeter ‚âà 38807.407 cm\n",
      "\n",
      "So, the perimeter of the Sierpinski carpet after the 6th iteration is approximately 38807.407 cm.\n",
      "extracts:  38807.407 432\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "The average age of a family of 6 members is 26 years.If the age of the youngest member is 10 years,what was the average age of the family at the birth of the youngest member?\n",
      "Answer Choices: (A) 15 (B) 18 (C) 16 (D) 12 (E) 19\n",
      "\n",
      "### Response:Let's think about the multi-choice question.\n",
      "Let the age of the youngest member be x years.\n",
      "Then,\n",
      "6x = 26\n",
      "x = 10\n",
      "x = 18\n",
      "The answer is B\n",
      "actual_answer:  Let's think about the multi-choice question step by step.\n",
      "Present age of total members = 6 X 26 = 156\n",
      "10 yrs back their ages were = 6 x 10 = 60\n",
      "Ages at the birth of youngest member = 156 - 60 = 96\n",
      "Therefore, avg age at the birth of youngest member = 96/6 = 16.\n",
      "The answer is C\n",
      "extracts:  16 18\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "For what value of $k$ does the equation $x^2+10x+y^2+6y-k=0$ represent a circle of radius 6?\n",
      "\n",
      "### Response:To find the value of $k$ for which the equation $x^2+10x+y^2+6y-k=0$ represents a circle of radius 6, we can use the formula for the radius of a circle:\n",
      "\n",
      "Radius = (1/2) * (x^2 + 10x + y^2 + 6y - k) / 2\n",
      "\n",
      "In this case, $x^2 + 10x + y^2 + 6y - k = 0$, so we can plug in the values of $x$ and $y$ to find the value of $k$:\n",
      "\n",
      "Radius = (1/2) * (10^2 + 10 + y^2 + 6y - k) / 2\n",
      "Radius = (1/2) * (10^2 + 10 + y^2 + 6y - k) / 2\n",
      "Radius = (1/2) * (10^2 + 10 + y^2 + 6y - k) / 2\n",
      "Radius = (1/2) * (10^2 + 10 + y^2 + 6y - k) / 2\n",
      "Radius = (1/2) * (10^2 + 10 + y^2 +\n",
      "actual_answer:  Completing the square, we can rewrite this equation as $(x+5)^2-25+(y+3)^2-9=k$, or $(x+5)^2+(y+3)^2=34+k$. Because this equation must represent a circle of radius 6, we need $34+k=6^2=36$, so $k=\\boxed{2}$. The answer is 2\n",
      "extracts:  2 2\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "S is a set of 64 consecutive multiples of 4. If the smallest number in S is 68, then the greatest number in S is\n",
      "Answer Choices: (A) 198 (B) 297 (C) 320 (D) 402 (E) 405\n",
      "\n",
      "### Response:Let's think about the multi-choice question step by step.\n",
      "S = (64 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S = (68 * 4)\n",
      "S =\n",
      "actual_answer:  Let's think about the multi-choice question.\n",
      "last term =first term +(total no. of terms-1)consecutive difference\n",
      "S is a set of 64 consecutive multiples of 4. If the smallest number in S is 68, then the greatest number in S is\n",
      "first term = 68 ; total terms =64 ; difference = 4\n",
      "68+(63)4 = 320\n",
      "The answer is C\n",
      "extracts:  320 4\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Gerald wants to buy a meat pie that costs 2 pfennigs. Gerald has 54 farthings, and there are 6 farthings to a pfennig. How many pfennigs will Gerald have left after buying the pie? Let's write a program.\n",
      "\n",
      "### Response:penn_cost = 2\n",
      "penn_farthings = 54\n",
      "penn_pies = 6\n",
      "\n",
      "total_pennies = penn_pies * penn_cost\n",
      "\n",
      "print(total_pennies)\n",
      "actual_answer:  # convert farthings to pfennigs\n",
      "total_pfennigs = 54 / 6\n",
      "# subtract the cost of the pie\n",
      "remaining_pfennigs = total_pfennigs - 2\n",
      "# print the result\n",
      "print(remaining_pfennigs)\n",
      "extracts:  2 6\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "$ABCD$ is a square and $M$ and $N$ are the midpoints of $\\overline{BC}$ and $\\overline{CD},$ respectively.  Find $\\sin \\theta.$\n",
      "\n",
      "[asy]\n",
      "unitsize(1.5 cm);\n",
      "\n",
      "draw((0,0)--(2,0)--(2,2)--(0,2)--cycle);\n",
      "draw((0,0)--(2,1));\n",
      "draw((0,0)--(1,2));\n",
      "label(\"$A$\", (0,0), SW);\n",
      "label(\"$B$\", (0,2), NW);\n",
      "label(\"$C$\", (2,2), NE);\n",
      "label(\"$D$\", (2,0), SE);\n",
      "label(\"$M$\", (1,2), N);\n",
      "label(\"$N$\", (2,1), E);\n",
      "label(\"$\\theta$\", (.5,.5), SW);\n",
      "[/asy]\n",
      "\n",
      "### Response:To find the sin theta, we need to find the midpoint of the triangle formed by the two triangles $ABCD$ and $M_A M_B M_C M_D.$  Since the midpoint of a triangle is the midpoint of the two sides, we can use the Pythagorean theorem to find the midpoint of the triangle.  The Pythagorean theorem states that the midpoint of a triangle is the midpoint of the two sides, and it states that the midpoint of a triangle is the midpoint of the two sides if and only if the two sides are parallel.  In this case, the two sides are parallel, so the midpoint of the triangle is the midpoint of the two sides.  The Pythagorean theorem states that the midpoint of a triangle is the midpoint of the two sides if and only if the two sides are parallel.  In this case, the two sides are parallel, so the midpoint of the triangle is the midpoint of the two sides.  The Pythagorean theorem states that the midpoint of a triangle is the midpoint of the two sides if and only if the two sides are parallel.  In this case, the two sides are\n",
      "actual_answer:  We can assume that the side length of the square is 2.  Then by Pythagoras, $AM = AN = \\sqrt{5},$ and $MN = \\sqrt{2},$ so by the Law of Cosines on triangle $AMN,$\n",
      "\\[\\cos \\theta = \\frac{AM^2 + AN^2 - MN^2}{2 \\cdot AM \\cdot AN} = \\frac{5 + 5 - 2}{10} = \\frac{8}{10} = \\frac{4}{5}.\\]Then\n",
      "\\[\\sin^2 \\theta = 1 - \\cos^2 \\theta = \\frac{9}{25}.\\]Since $\\theta$ is acute, $\\sin \\theta = \\boxed{\\frac{3}{5}}.$. The answer is \\frac{3}{5}\n",
      "extracts:  5 .5\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Bill likes long bike rides. On one ride, he goes biking for seven hours. For the first hour, he bikes\n",
      "at a speed of 16 miles per hour. For the next two hours, he bikes at a speed of 12 miles per\n",
      "hour. He remembers biking 112 miles over the seven hours. Compute the average speed, in miles\n",
      "per hour, Bill biked during the last four hours of his trip.\n",
      "Answer Choices: (A) 18 (B) 14 (C) 22 (D) 27 (E) 35\n",
      "\n",
      "### Response:Let's think about the multi-choice question step by step.\n",
      "Speed = (16 x 7) / 7 = 12 miles per hour\n",
      "Average speed = (112 x 12) / 112 = 14 miles per hour\n",
      "The answer is B\n",
      "actual_answer:  Let's think about the multi-choice question.\n",
      "He has to travel 112 - 16 - 12 * 2 = 72 miles in 4 hours\n",
      "72 / 4 = 18 miles per hour\n",
      "The answer is A\n",
      "extracts:  18 14\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "in a certain large company , the ratio of college graduates with a graduate degree to non - college graduates is 1 : 8 , and ratio of college graduates without a graduate degree to non - college graduates is 2 : 3 . if one picks a random college graduate at this large company , what is the probability t this college graduate has a graduate degree ? Please write a program to solve it\n",
      "\n",
      "### Response:n0 = 1.0\n",
      "n1 = 8.0\n",
      "n2 = 2.0\n",
      "n3 = 3.0\n",
      "t0 = n0 * n1\n",
      "t1 = n2 * n3\n",
      "t2 = t0 / t1\n",
      "t3 = t2 / n3\n",
      "t4 = t3 / n0\n",
      "t5 = t4 / t1\n",
      "t6 = t5 / t2\n",
      "t7 = t6 / t5\n",
      "t8 = t7 / t6\n",
      "t9 = t8 / t7\n",
      "t10 = t9 / t8\n",
      "t11 = t10 / t9\n",
      "t12 = t11 / t10\n",
      "t13 = t12 / t11\n",
      "t14 = t13 / t12\n",
      "t15 = t14 / t13\n",
      "t16 = t15 / t14\n",
      "t17 = t16 / t15\n",
      "t18 = t17 / t16\n",
      "t19 = t18 / t17\n",
      "t20 = t19 / t18\n",
      "t21 = t20 / t19\n",
      "t22 = t21 / t20\n",
      "t23 = t22 / t21\n",
      "t24 = t23 / t22\n",
      "t25 =\n",
      "actual_answer:  n0 = 1.0\n",
      "n1 = 8.0\n",
      "n2 = 2.0\n",
      "n3 = 3.0\n",
      "t0 = n0 / n1\n",
      "t1 = n2 / n3\n",
      "t2 = t0 / t1\n",
      "t3 = t2 + n0\n",
      "answer = t2 / t3\n",
      "print(answer)\n",
      "extracts:  3 25\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Mark borrows $10$ dollars from Emily with a simple interest rate of $15\\%$ everyday. What is the least integer number of days after which Mark will have to pay her back at least twice as much as he borrowed?\n",
      "\n",
      "### Response:The interest rate is $15\\%$ so the total interest is $15*10 = $210$.\n",
      "Since Mark has to repay her $10$ dollars, he has to repay $210-10 = $90$ dollars.\n",
      "The answer is 90\n",
      "actual_answer:  Since the interest rate is simple, he has to pay an interest of $10 \\cdot 0.15 =1.5$ dollars every day.\n",
      "\n",
      "Let $x$ be the number of days needed to repay at least double the amount borrowed. Since he has to repay $10 as well as $\\$1.5x$ in interest, we have the inequality $10+1.5x \\ge 10 \\cdot 2$. Solving for $x$, we get $x \\ge 6.\\overline{6}$. The smallest integer greater than $6.\\overline{6}$ is $7$. Therefore, it would take at least $\\boxed{7 \\text{ days}}$. The answer is 7\n",
      "extracts:  7 90\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "A began a business with Rs. 85,000. He was joined afterwards by B with Ks. 42,500. For how much period does B join, if the profits at the end of the year are divided in the ratio of 3 : 1 ?\n",
      "Answer Choices: (A) 4 months (B) 5 months (C) 6 months (D) 8 months (E) 7 months\n",
      "\n",
      "### Response:Let's think about the multi-choice question.\n",
      "Let the number of shares of B be Rs. x.\n",
      "Then,\n",
      "(x + 85000)/3 = 85000\n",
      "=> x = 85000/3\n",
      "=> x = 85000/3 = 8 months\n",
      "The answer is D\n",
      "actual_answer:  Let's solve the multi-choice question step by step.\n",
      "Suppose B joined for x months . Then, ( 85000 * 12 )/(42500 * x) = 3. or x = (85000 * 12) / (42500 * 3) = 8.\n",
      "So, B joined for 8 months.The answer is D\n",
      "extracts:  8 8\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Robert had 25 balls. Tim gave him half of his 40 balls. How many balls does Robert have now?\n",
      "\n",
      "### Response:Robert has 25/2=5 balls\n",
      "Tim gave him a total of 40/2=20 balls\n",
      "So Robert has 5+20=25 balls\n",
      "The answer is 25\n",
      "actual_answer:  Tim gave 40 / 2 = 20 balls to Robert.\n",
      "Robert has 25 + 20 = 45 balls now.\n",
      "The answer is 45\n",
      "extracts:  45 25\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "For integers x and y if x^2+x/y is always an odd integer then which of the following must be true?\n",
      "Answer Choices: (A) x is even ; y is odd (B) x is even ; y is even (C) x is odd ; y is odd (D) x is odd ; y is even (E) cannot be determined\n",
      "\n",
      "### Response:Let's think about the multi-choice question.\n",
      "x^2+x/y is always an odd integer.\n",
      "x is even.\n",
      "y is odd.\n",
      "The answer is A\n",
      "actual_answer:  Let's think about the multi-choice question step by step.\n",
      "Given x and y are integers...NOTE : While taking values, x/y must be properly divisible and the result has to integer itself..\n",
      "x^2+x/y = odd always...\n",
      "lets take x even (2) and y even (2)\n",
      "4 + 2/2 = odd...\n",
      "lets take x odd(3) and y odd(3)\n",
      "9+3/3 = even...\n",
      "So only x even and y even satisfies the equation..\n",
      "If we take one even and other odd then we get x/y as non integer..which shouldn't be the case as per our question stem.\n",
      "The answer is B is correct...\n",
      "extracts:  3 2\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "perimeter of an equilateral and isosceles is 45 and 40 respectively . at least one of the sides of isosceles is equal to the equilateral . what ' s the base of isosceles triangle ? Please write a program to solve it\n",
      "\n",
      "### Response:n0 = 45.0\n",
      "n1 = 40.0\n",
      "n2 = 1.0\n",
      "n3 = 1.0\n",
      "n4 = 45.0\n",
      "n5 = 40.0\n",
      "t0 = n0 * n1\n",
      "t1 = n2 * n3\n",
      "t2 = n4 * n5\n",
      "t3 = t0 + t1\n",
      "t4 = t2 / t3\n",
      "answer = t4 * t3\n",
      "print(answer)\n",
      "actual_answer:  n0 = 45.0\n",
      "n1 = 40.0\n",
      "t0 = n0 / 3.0\n",
      "t1 = t0 * 2.0\n",
      "answer = n1 - t1\n",
      "print(answer)\n",
      "extracts:  1 3\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Barney can perform 45 sit-ups in one minute. Carrie can do twice as many sit-ups per minute as Barney can.  And Jerrie can do 5 more sit-ups per minute than Carrie can do.  If Barney does sit-ups for 1 minute, and Carrie does sit-ups for two minutes, and Jerrie does sit-ups for three minutes, what would be the combined total number of sit-ups performed? Please respond by writing a program in Python.\n",
      "\n",
      "### Response:barney_sitting_ups = 45\n",
      "carrie_sitting_ups = 2\n",
      "jerrie_sitting_ups = 3\n",
      "total_sitting_ups = barney_sitting_ups + carrie_sitting_ups + jerrie_sitting_ups\n",
      "total_sitting_ups = total_sitting_ups * 2\n",
      "total_sitting_ups = total_sitting_ups * 3\n",
      "total_sitting_ups = total_sitting_ups * 2 * 3\n",
      "total_sitting_ups = total_sitting_ups * 3 * 2\n",
      "total_sitting_ups = total_sitting_ups * 2 * 3 * 2\n",
      "total_sitting_ups = total_sitting_ups * 3 * 2 * 3\n",
      "total_sitting_ups = total_sitting_ups * 3 * 2 * 3 * 2\n",
      "total_sitting_ups = total_sitting_ups * 3 * 2 * 3 * 2 * 3\n",
      "total_sitting_ups = total_sitting_ups * 3 * 2 * 3 * 2 * 3 * 2 * 3 *\n",
      "actual_answer:  # define the number of sit-ups each person can do in one minute\n",
      "barney_situps_per_min = 45\n",
      "carrie_situps_per_min = 2 * barney_situps_per_min\n",
      "jerrie_situps_per_min = carrie_situps_per_min + 5\n",
      "\n",
      "# calculate the total number of sit-ups each person does\n",
      "barney_total = barney_situps_per_min * 1\n",
      "carrie_total = carrie_situps_per_min * 2\n",
      "jerrie_total = jerrie_situps_per_min * 3\n",
      "\n",
      "# calculate the combined total\n",
      "combined_total = barney_total + carrie_total + jerrie_total\n",
      "\n",
      "print(combined_total)\n",
      "extracts:  3 3\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "In variational optimization, a common problem is to find the minimum value of a functional. Consider the functional:\n",
      "\n",
      "f(y) = ‚à´[0, 1] (y'^2 - y^2)dx\n",
      "\n",
      "where y(0) = y(1) = 0 and y(x) is a continuously differentiable function of x.\n",
      "\n",
      "What is the value of y(x) that minimizes the functional f(y)?\n",
      "\n",
      "### Response:To find the minimum value of the functional f(y) = ‚à´[0, 1] (y'^2 - y^2)dx, we can use the method of Lagrange multipliers. Let's denote the Lagrangian function as:\n",
      "\n",
      "f(y) = ‚à´[0, 1] (y'^2 - y^2)dx\n",
      "\n",
      "Now, we can use the Lagrange multiplier method to find the value of y(x) that minimizes the Lagrangian function f(y):\n",
      "\n",
      "y(x) = ‚à´[0, 1] (y'^2 - y^2)dx\n",
      "\n",
      "Now, we can use the Lagrange multiplier method to find the value of y(x) that minimizes the Lagrangian function f(y):\n",
      "\n",
      "y(x) = ‚à´[0, 1] (y'^2 - y^2)dx\n",
      "\n",
      "Now, we can use the Lagrange multiplier method to find the value of y(x) that minimizes the Lagrangian function f(y):\n",
      "\n",
      "y(x) = ‚à´[0, 1] (y'^2 - y^2)dx\n",
      "\n",
      "Now, we can use the Lagrange multiplier method to find the value of y(x\n",
      "actual_answer:  To find the function y(x) that minimizes the functional f(y), we can use the Euler-Lagrange equation. The integrand in the functional is:\n",
      "\n",
      "L(y, y') = y'^2 - y^2\n",
      "\n",
      "Now, we compute the partial derivatives of L with respect to y and y':\n",
      "\n",
      "‚àÇL/‚àÇy = -2y\n",
      "‚àÇL/‚àÇy' = 2y'\n",
      "\n",
      "The Euler-Lagrange equation is given by:\n",
      "\n",
      "d(‚àÇL/‚àÇy')/dx - ‚àÇL/‚àÇy = 0\n",
      "\n",
      "Substituting the partial derivatives, we get:\n",
      "\n",
      "d(2y')/dx + 2y = 0\n",
      "\n",
      "This is a second-order linear ordinary differential equation (ODE). To solve this ODE, we can first rewrite it as:\n",
      "\n",
      "y'' + y = 0\n",
      "\n",
      "Now, we can guess a solution of the form y(x) = e^(rx), where r is a constant. Substituting this into the ODE, we get:\n",
      "\n",
      "r^2 e^(rx) + e^(rx) = 0\n",
      "\n",
      "Factoring out e^(rx), we have:\n",
      "\n",
      "e^(rx) (r^2 + 1) = 0\n",
      "\n",
      "Since e^(rx) is never zero, we must have:\n",
      "\n",
      "r^2 + 1 = 0\n",
      "\n",
      "This gives us two complex roots:\n",
      "\n",
      "r = ¬±i\n",
      "\n",
      "Thus, the general solution to the ODE is:\n",
      "\n",
      "y(x) = A cos(x) + B sin(x)\n",
      "\n",
      "Now, we apply the boundary conditions y(0) = y(1) = 0:\n",
      "\n",
      "y(0) = A cos(0) + B sin(0) = A = 0\n",
      "y(1) = B sin(1) = 0\n",
      "\n",
      "Since sin(1) ‚â† 0, we must have B = 0. Therefore, the function y(x) that minimizes the functional f(y) is:\n",
      "\n",
      "y(x) = 0\n",
      "extracts:  0 2\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Melanie had 8 dimes in her bank.  She gave 7 dimes to her dad.  Her mother then gave her 4 dimes. Melanie have _____ dimes now . Please respond by writing a program in Python.\n",
      "\n",
      "### Response:dimes = 8\n",
      "gave = 7\n",
      "mother_gave = 4\n",
      "dimes_given = 8\n",
      "\n",
      "# calculate the total number of dimes given\n",
      "total_given = gave + mother_gave + dimes_given\n",
      "\n",
      "# print the result\n",
      "print(total_given)\n",
      "actual_answer:  melaine_had=8\n",
      "gace_to_dad=7\n",
      "mother_gave=4\n",
      "print(melaine_had-gace_to_dad+mother_gave)\n",
      "extracts:  4 8\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "A man can row his boat with the stream at 26 km/h and against the stream in 4 km/h. The man's rate is?\n",
      "Answer Choices: (A) 1 kmph (B) 7 kmph (C) 98 kmph (D) 11 kmph (E) 4 kmph Please write a program to solve it\n",
      "\n",
      "### Response:speed = 26\n",
      "time = 4\n",
      "rate = speed / time\n",
      "print(rate)\n",
      "actual_answer:  # The man's rate is the average of his speed with the stream and against the stream.\n",
      "man_rate = (26 + 4) / 2\n",
      "print(man_rate)\n",
      "extracts:  2 4\n",
      "pred_answer:  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Statement 1 | The intersection of two subgroups of a group is also a subgroup. Statement 2 | The union of two subgroups of a group is always a subgroup.  Answer Choices: (A) True, True (B) False, False (C) True, False (D) False, True\n",
      "\n",
      "### Response:Statement 1: The intersection of two subgroups of a group is also a subgroup.\n",
      "Statement 2: The union of two subgroups of a group is always a subgroup.\n",
      "The answer is A\n",
      "actual_answer:  Statement 1: The intersection of two subgroups of a group is also a subgroup. This statement is true. Let H and K be subgroups of a group G. By definition, a subgroup must contain the identity, so the intersection of H and K also contains the identity. If a and b are in the intersection, then a, b are in both H and K, and so is ab^-1. So the intersection of H and K is closed under the group operation and inverses, and is therefore a subgroup. Statement 2: The union of two subgroups of a group is always a subgroup. This statement is false. The union of two subgroups is not always a subgroup. For example, consider the group of integers Z under addition, and let H = {2n : n ‚àà Z} and K = {2n+1 : n ‚àà Z} be two subgroups of Z. The union H ‚à™ K = Z, which is a group, but this is just a special case. If we take H = {0} and K = {2} as subgroups of Z, the union H ‚à™ K = {0, 2} is not a subgroup of Z, because it is not closed under the operation of addition. The answer is B.\n",
      "extracts:  2 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-924834864.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#full\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing Pythia 410M Trained On Full Dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mevaluate_model_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"res/pythia-410m_cirriculum_full/output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"TIGER-Lab/MathInstruct\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#s2l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/S2L_Cirriculum/evaluate.py\u001b[0m in \u001b[0;36mevaluate_model_accuracy\u001b[0;34m(model_path, dataset_path, start_idx, end_idx)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             model_outputs = model.generate(\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m         ```\"\"\"\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         outputs: BaseModelOutputWithPast = self.gpt_neox(\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             outputs = layer(\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnpack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFlashAttentionKwargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     ):\n\u001b[0;32m--> 221\u001b[0;31m         attn_output, attn_weights = self.attention(\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, layer_past, output_attentions, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rotary_pos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# Cache QKV values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\u001b[0m in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# Apply rotary embeddings on the first half or full tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mq_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq_rot\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrotate_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_rot\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mk_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk_rot\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrotate_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_rot\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\u001b[0m in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mrotate_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;34m\"\"\"Rotates half the hidden dims of the input.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from evaluate import evaluate_model_accuracy\n",
    "\n",
    "#full\n",
    "print(\"Testing Pythia 410M Trained On Full Dataset On GSM8K\")\n",
    "evaluate_model_accuracy(model_path=\"res/pythia-410m_cirriculum_full/output\",\n",
    "                        dataset_path=\"openai/gsm8k\",\n",
    "                        start_idx=0,\n",
    "                        end_idx=10000)\n",
    "\n",
    "#s2l\n",
    "print(\"Testing Pythia 410M Trained On S2L Dataset On GSM8K \")\n",
    "evaluate_model_accuracy(model_path=\"res/pythia-410m_cirriculum_s2l/output\",\n",
    "                        dataset_path=\"openai/gsm8k\",\n",
    "                        start_idx=0,\n",
    "                        end_idx=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf27c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_model_accuracy\n",
    "\n",
    "#full\n",
    "print(\"Testing Pythia 410M Trained On Full Dataset On MATH\")\n",
    "evaluate_model_accuracy(model_path=\"res/pythia-410m_cirriculum_full/output\",\n",
    "                        dataset_path=\"EleutherAI/hendrycks_math\",\n",
    "                        start_idx=0,\n",
    "                        end_idx=5000)\n",
    "\n",
    "#s2l\n",
    "print(\"Testing Pythia 410M Trained On S2L Dataset On MATH \")\n",
    "evaluate_model_accuracy(model_path=\"res/pythia-410m_cirriculum_s2l/output\",\n",
    "                        dataset_path=\"EleutherAI/hendrycks_math\",\n",
    "                        start_idx=0,\n",
    "                        end_idx=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
